{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVFauInjEr90V+1q8jucuY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akimotolab/Policy_Optimization_Tutorial/blob/main/2_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備\n",
        "\n",
        "まず，必要なパッケージのインストールとインポート，および仮想displayを設定します．"
      ],
      "metadata": {
        "id": "qWe6P9xqH8be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kXoBp0fgHyhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b422d39-7f37-4a34-95ee-bab24bb256f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [631 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,456 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,281 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,013 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,487 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,240 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,512 kB]\n",
            "Fetched 8,856 kB in 3s (2,791 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "15 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1.post0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 7,814 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n",
            "Fetched 7,814 kB in 2s (3,738 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120880 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1.post0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373075 sha256=a16f7e06d813b47035591c31b126ecd808bdab75b6c513aae7a0e13d84b7ec9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: farama-notifications, box2d-py, gymnasium\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "# 必要なパッケージのインストール\n",
        "!apt update\n",
        "!pip install swig\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import torch\n",
        "\n",
        "# 仮想ディスプレイの設定\n",
        "_display = Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "metadata": {
        "id": "iJ1PYM90IHa9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "d5Oklvxh5nAC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "続いて，第１回の資料で定義した`rollout`などの基本的な関数をここでも定義しておきます．"
      ],
      "metadata": {
        "id": "ci2EEsRpD1oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout(envname, policy=None, render=False, seed=None):\n",
        "    if render:\n",
        "        env = gym.make(envname, render_mode=\"rgb_array\")\n",
        "    else:\n",
        "        env = gym.make(envname)\n",
        "    history = []\n",
        "    img = []\n",
        "\n",
        "    # 乱数の設定\n",
        "    if seed is not None:\n",
        "        random.seed(int(seed))\n",
        "    envseed = random.randint(0, 1000)\n",
        "    actseed = random.randint(0, 1000)\n",
        "    observation, info = env.reset(seed=envseed)\n",
        "    env.action_space.seed(actseed)\n",
        "\n",
        "    # 可視化用の設定\n",
        "    if render:\n",
        "        d = Display()\n",
        "        d.start()\n",
        "        img.append(env.render())\n",
        "\n",
        "    # メインループ（環境とのインタラクション）\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # 行動を選択\n",
        "        if policy is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy(observation)\n",
        "\n",
        "        # 行動を実行\n",
        "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "        history.append([observation, action, next_observation, reward, terminated, truncated, info])\n",
        "        observation = next_observation\n",
        "        if render:\n",
        "            display.clear_output(wait=True)\n",
        "            img.append(env.render())\n",
        "    env.close()\n",
        "    return history, img\n",
        "\n",
        "\n",
        "def visualize(img):\n",
        "    dpi = 72\n",
        "    interval = 50\n",
        "    plt.figure(figsize=(img[0].shape[1]/dpi, img[0].shape[0]/dpi), dpi=dpi)\n",
        "    patch = plt.imshow(img[0])\n",
        "    plt.axis=('off')\n",
        "    animate = lambda i: patch.set_data(img[i])\n",
        "    ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(img), interval=interval)\n",
        "    display.display(display.HTML(ani.to_jshtml()))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def cumulative_reward(history):\n",
        "    return sum(hist[3] for hist in history)"
      ],
      "metadata": {
        "id": "9ELUIKjb5vY2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 方策勾配を用いた方策最適化（強化学習）\n",
        "\n",
        "今回は「方策勾配法」を見ていきます．\n",
        "第１回は，汎用的なブラックボックス最適化法を用いた方策最適化の例を見てもらいました．\n",
        "ブラックボックス最適化法を用いていたということは，目的関数$J(\\theta)$が「何らかの方策をパラメータ$\\theta$で用いた場合に，環境とインタラクションした結果得られる累積報酬」を意味しているという情報を用いずに，ただブラックボックスな関数として最適化していることを意味します．\n",
        "ここでは，積極的にこの知識を活用していく方法を検討していきましょう．\n",
        "ブラックボックス最適化としての方策最適化と，強化学習を用いた方策最適化の一番の違いがここにあります．"
      ],
      "metadata": {
        "id": "H_gG0F5uKTYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 状態と行動の価値\n",
        "\n",
        "ブラックボックスな目的関数$J(\\theta)$では，一連のインタラクションを通して得られた報酬の合計を評価しています．\n",
        "これは，最大化したい指標であることに間違いありませんが，一方で，各状態でとった各行動が良かったのかどうか，という情報を与えてくれません．\n",
        "この情報を活用することができれば，ある状態$s$ではある行動$a$を取るとよい，ということがわかり，その確率を高くするように方策を改善することができそうです．"
      ],
      "metadata": {
        "id": "0_V-B-wGJsZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定式化\n",
        "まず，最低限の定式化を行います．\n",
        "今回対象としている方策最適化では，\n",
        "まず初期状態$s_0$を観測します．\n",
        "初期状態は確率分布$p_0$からランダムに生成されます．\n",
        "方策を通して，次にとる行動$a_0 \\sim \\pi(\\cdot \\mid s_0)$を決定します．\n",
        "ここでは，方策として確率的な方策を考えることにします．\n",
        "この行動を実行すると，状態が$s_1$に変わり，これを観測します．\n",
        "それと同時に，$s_0$で行動$a_0$を取ることの良さを表す即時報酬$r_1$が得られます．\n",
        "次状態と即時報酬は，環境が定める条件付き確率$p_T(s_1, r_1 \\mid s_0, a_0)$により定まります．\n",
        "このあとは，$s_1$において方策に従って次の行動$a_1 \\sim \\pi(\\cdot \\mid s_0)$を決定し，次状態と即時報酬を観測する，というステップを繰り返します．\n",
        "この環境との一度のインタラクションをステップと呼びます．\n",
        "\n",
        "注意：「環境が定める」といっても，実際に報酬を設計するのは自分自身（設計者）です．望ましい方策を得るためには，適切な報酬を設計することが極めて重要です．）"
      ],
      "metadata": {
        "id": "e0st6F33vB0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 累積報酬\n",
        "強化学習においては，方策最適化の目的は割引累積報酬の期待値を最大化することと一般に定められます．\n",
        "あるステップ$t$において，その先に得られる割引累積報酬は\n",
        "$$\n",
        "G_t = r_{t+1} + \\gamma r_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+1+k}\n",
        "$$\n",
        "と定義されます．ここで，$\\gamma \\in [0, 1]$は割引率と呼ばれるパラメータです．\n",
        "方策，初期状態分布，状態遷移，即時報酬は確率的ですから，$G_t$自体も確率的に振る舞います．\n",
        "そこで，これの期待値$\\mathrm{E}[G_0]$を考え，これを最大化することを考えます．\n",
        "\n",
        "ここでは，インタラクションが無限に続くことを想定して$G_t$が定義されています．\n",
        "この場合，割引率は$\\gamma < 1$であることが必要です．\n",
        "エピソディックタスクの場合，特別な終了状態（例えば迷路のような問題において，ゴール状態に到達した，落とし穴に落ちて脱落した，など）があり，途中でエピソードが止まることになります．\n",
        "この場合にも，終了状態に達したあとは何をしても終了状態に遷移し，即時報酬はずっと0である，と考えれば，上の定義に当てはまります．\n",
        "このように，インタラクションに一区切りがあるようなタスクはエピソディックタスクと呼ばれ，この一区切りのステップのまとまりをエピソードと呼びます．\n",
        "最適化（学習）の都合上，特定のステップでインタラクションを打ち切り，無理やりエピソディックタスクにするような場合もありますが，この場合にも目的は$\\mathrm{E}[G_0]$の最大化である（有限ステップでの累積報酬ではない）と考えると，以下の議論が成立します．\n",
        "\n",
        "注意：制限時間などのように，特定のステップがすぎると強制的に状態がリセットされるようなケースの場合，注意が必要です．この場合，残り時間などを状態観測に含めることが必要になります．"
      ],
      "metadata": {
        "id": "RXkv9WldvGg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 状態価値と行動価値\n",
        "\n",
        "状態$s$の価値を，「$s_0 = s$からインタラクションを始めて，方策$\\pi$に従って行動選択した際に得られる割引累積報酬の期待値$\\mathrm{E}[G_0 \\mid s_0 = s]$」と定義します．\n",
        "これを$V^{\\pi}(s)$と書きます．\n",
        "定義からわかるように，状態価値は方策$\\pi$に依存しています．\n",
        "割引累積報酬が\n",
        "$$\n",
        "G_{t} = r_{t+1} + \\gamma G_{t+1}\n",
        "$$\n",
        "という再帰的な関係式を満たすことを考えると，状態価値は\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathrm{E}[r_{1} + \\gamma V^{\\pi}(s_{1}) \\mid s_0 = s]\n",
        "$$\n",
        "という関係式を満たすことがわかります．\n",
        "なお，ステップのインデックスに関しては\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathrm{E}[r_{t+1} + \\gamma V^{\\pi}(s_{t+1}) \\mid s_t = s]\n",
        "$$\n",
        "と考えても構いません．\n",
        "状態の価値が高いということは，その方策に従っている場合には，その状態からは得られる割引累積報酬の期待値が高いということを意味しています．\n",
        "ですから，そのような状態を積極的に訪問したいという指針になるでしょう．\n",
        "\n",
        "関連して，行動価値（状態行動価値とも言います）を，「$s_0 = s$からインタラクションを始めて，最初だけ行動$a$を選択し，その後は方策$\\pi$に従って行動選択した際に得られる割引累積報酬の期待値$\\mathrm{E}[G_0 \\mid s_0 = s, a_0 = a]$」と定義します．\n",
        "これを$Q^{\\pi}(s, a)$と書きます．\n",
        "定義からわかるように，もしも最初の行動を$a \\sim \\pi(\\cdot \\mid s)$にしたがって決定していれば行動価値関数の$a \\sim \\pi(\\cdot \\mid s)$についての期待値は，状態$s$の価値\n",
        "$$\n",
        "V^\\pi(s) = \\mathrm{E}[Q^{\\pi}(s, a) \\mid a \\sim \\pi(\\cdot \\mid s)]\n",
        "$$\n",
        "に一致します．\n",
        "\n",
        "価値関数はいずれも解析的に得られるものではありません．\n",
        "しかし，インタラクションを通して，近似していくことが可能です．\n",
        "その代表的な方法の一つに，TD誤差を用いた更新があります．"
      ],
      "metadata": {
        "id": "9PHLZ4LlvJNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 方策勾配\n",
        "\n",
        "目的関数を数値的に最適化する際，まず考えられる方針は勾配法を用いることです．\n",
        "すなわち，$\\theta \\leftarrow \\theta + \\alpha_\\theta \\nabla_\\theta J(\\theta)$と更新する方法です．\n",
        "割引累積報酬をブラックボックスと捉えてしまうと勾配は計算できないのですが，価値関数を明示的に使うことで勾配をうまく近似することが可能です．\n",
        "「方策勾配定理」は，割引累積報酬の期待値の，方策パラメータについての期待値を書き下す方法を提供してくれます．"
      ],
      "metadata": {
        "id": "Lz8VutUocewQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方策勾配定理\n",
        "\n",
        "$J(\\theta) = \\mathrm{E}[V^{\\pi}(s) \\mid s \\sim p_0]$とします．\n",
        "このとき，\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) \\propto \\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "$$\n",
        "と書き下すことができます．\n",
        "ここで，上の期待値は，方策$\\pi_\\theta$のもとでの状態訪問確率と行動選択確率について取られています．\n",
        "実際にインタラクションを通して観測された状態とその状態のもとで方策$\\pi_\\theta$に従って選択された行動を用いれば，右辺は\n",
        "$$\n",
        "Q^{\\pi}(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "で近似することができます．\n",
        "また，行動価値関数をその近似値$q(s_t, a_t)$で置き換えれば，勾配$\\nabla_\\theta J(\\theta)$の推定値として\n",
        "$$\n",
        "q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "を得ることができます．\n",
        "もしくは，1エピソード分の状態遷移履歴を用いて，\n",
        "$$\n",
        "\\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "と近似することができます．\n",
        "これらを用いて，\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + \\alpha_\\theta \\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "などと方策パラメータを更新していきます．ここで，$\\alpha_\\theta$は学習率です．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "awCoxRZr50Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方策勾配の自動計算\n",
        "\n",
        "方策勾配を計算するには，方策の対数の方策パラメータについての勾配 $\\nabla_\\theta \\ln \\pi$ を求める必要があります．\n",
        "方策を変える毎に，これを計算して実行するのは，複雑な方策を採用する際には面倒になります．\n",
        "他方，深層学習で用いられている Pytorch などのライブラリでは，関数の勾配を自動的に計算してくれる機能が備わっています．この機能を活用することで，方策勾配も容易に計算することが可能になります．\n",
        "\n",
        "上に示した1エピソード分のデータを用いた方策勾配\n",
        "$$\n",
        "\\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "を計算したい状況を考えましょう．\n",
        "いま，$q^{\\pi}(s_t, a_t)$は何らかの方法（後述します）で予め計算されているとします．\n",
        "このとき，以下のような関数を考えます．\n",
        "$$\n",
        "L(\\theta; \\{(s_t, a_t)\\}_{t=0}^{T}) = \\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "このような関数$L$を定義すると，上に示した方策勾配は $\\nabla_\\theta L(\\theta; \\{(s_t, a_t)\\}_{t=0}^{T})$ であることが容易にわかります．\n",
        "\n",
        "この事実を用いると，方策勾配を自動計算することが可能になります．\n",
        "行動が離散であれば，各行動の選択確率からなるベクトルを返す関数モデルを用意しておけば，用意に計算できます．"
      ],
      "metadata": {
        "id": "SxNyza3zTfCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# gpuが使用される場合の設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
      ],
      "metadata": {
        "id": "hDnqAPt1jSJh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下では，Pytorchを用いた方策（ここではActorと呼ぶことにします．これは今後紹介するActor-Criticアルゴリズムでの呼び方に従っています．）の実装方法を紹介します．\n",
        "ここでは四層のニューラルネットワークを用いています．中間層のノード数は`dim_hidden`により指定されています．\n",
        "\n",
        "関数`forward`が観測`x`を受け取って，これに対して各行動の選択確率を出力する関数です．\n",
        "行動選択確率は以下のように計算されます．\n",
        "入力を$h_0 = x$とします．\n",
        "$i = 1, 2, 3, 4$について，以下を計算します．\n",
        "$$\n",
        "h_i = g_i\\left( b_i + W_i h_{i=1}\\right)\n",
        "$$\n",
        "ここで，中間層の活性化関数$g_1, g_2, g_3$は$\\mathrm{ReLU}(s) = \\max(0, s)$であり，\n",
        "出力部分の活性化関数$g_4$は出力の和が$1$となるように，$\\mathrm{softmax}$を用いています．"
      ],
      "metadata": {
        "id": "KBZ3NMsNVNRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, dim_state, num_action, dim_hidden=16):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_state, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, dim_hidden)\n",
        "        self.fc3 = nn.Linear(dim_hidden, dim_hidden)\n",
        "        self.fc4 = nn.Linear(dim_hidden, num_action)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.softmax(self.fc4(x), dim=0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3d4NDkjPjSHG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "実行時間の観点から，簡易化した２層のモデルを以下の実験では使うことにします．"
      ],
      "metadata": {
        "id": "obdiPGS_u1AA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, dim_state, num_action, dim_hidden=128):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_state, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, num_action)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.softmax(self.fc2(x), dim=0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MKmWVcecu0dL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REINFORCEアルゴリズム（行動価値関数のMonte Carlo推定を用いた方策勾配法）\n",
        "\n",
        "方策勾配を用いた強化学習の代表的な一つの方法である，REINFORCEアルゴリズムを紹介します．\n",
        "話を簡単にするために，ここでは\n",
        "\n",
        "1. エピソディックタスク，\n",
        "2. 割引率$\\gamma = 1$（割引なし），\n",
        "3. 確率的方策，\n",
        "\n",
        "であることを仮定します．\n",
        "\n",
        "方策勾配法を用いる場合，行動価値関数$Q^{\\pi}(s_t, a_t)$の近似値$q(s_t, a_t)$を得ることが必要になります．\n",
        "この$q(s_t, a_t)$の計算方法の違いにより，様々な方策勾配法のバリエーションが存在します．\n",
        "REINFORCEアルゴリズムでは，Monte-Carlo推定を用いてこれを近似して利用します．"
      ],
      "metadata": {
        "id": "D052tsAPBSKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 行動価値関数のモンテカルロ推定\n",
        "\n",
        "行動価値関数$Q^{\\pi}(s, a)$は「$s_0 = s$からインタラクションを始めて，最初だけ行動$a$を選択し，その後は方策$\\pi$に従って行動選択した際に得られる割引累積報酬の期待値$\\mathrm{E}[G_0 \\mid s_0 = s, a_0 = a]$」と定義されます．\n",
        "すなわち，この期待値は各ステップ$t$での状態の訪問確率と行動の選択確率を知らなければ計算できません．\n",
        "行動の選択確率は方策で決まりますから，これは予め知っている情報ですが，ある状態である行動を取った際の次の状態への遷移確率は未知である（実際に実行して始めて次状態を観測できる）と仮定していますから，これを計算することはできません．\n",
        "期待値を厳密には計算できませんが，実際にインタラクションを通して累積報酬の実現値を観測することは可能です．\n",
        "これを用いて期待値を推定する方法がモンテカルロ推定です．\n",
        "\n",
        "状態遷移はステップ数$t$には依存しないため，行動価値関数$Q^{\\pi}(s, a)$は$\\mathrm{E}[G_t \\mid s_t = s, a_t = a]$と定義しても同じものになります．\n",
        "すなわち，ステップ$t$において観測した状態が$s_t = s$，$a_t = a$であったならば，そこから実際にインタラクションして得られた累積報酬 $G_t = r_{t+1} + \\dots + r_{T}$（$T$は終端ステップ）の期待値がその$(s_t, a_t)$の行動価値であり，観測された$G_t$は確率変数として見た場合の$G_t$の観測値（サンプル）であることがわかります．\n",
        "そのため，観測された$G_t$は行動価値$Q^\\pi(s_t, a_t)$の不偏推定値であることになります．\n",
        "そこで，これを$q^\\pi(s_t, a_t)$として採用することにしましょう．\n"
      ],
      "metadata": {
        "id": "NoMZcO82CzR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "実装上は，1エピソードの結果得られる状態遷移履歴から，各ステップでの将来に得られる累積報酬を計算し，その配列を返します．\n",
        "終了状態から遡ることで計算することでこれを簡単に計算できます．"
      ],
      "metadata": {
        "id": "bFaE6C9oSCjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REINFORCEアルゴリズム\n",
        "\n",
        "REINFORCEアルゴリズムは，価値のモンテカルロ推定を用いた方策勾配法です．\n",
        "REINFORCEにはいくつかのバリエーションがありますが，ここでは，エピソード単位で方策パラメータを更新する最もシンプルな方法を紹介します．\n",
        "（なお，状態遷移毎（ステップ毎）にパラメータを更新するバリエーションもあります．）\n",
        "\n",
        "まず，現在の方策を用いてEエピソード分だけ環境とインタラクションします．\n",
        "この結果から，エピソード内に訪問した各状態についての価値をモンテカルロ推定します．\n",
        "この推定価値を用いて，方策勾配を\n",
        "$$\\begin{aligned}\n",
        "\\nabla_\\theta J(\\theta)\n",
        "&= \\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi(a \\mid s)  \\right] \\\\\n",
        "&\\approx \\frac{1}{E}\\sum_{e=1}^{E}\\frac{1}{T_e}\\sum_{t=1}^{T_e} G_{e,t} \\nabla_{\\theta} \\ln \\pi(a_{e,t} \\mid s_{e,t}) =: \\widehat{\\nabla_\\theta J(\\theta) }\n",
        "\\end{aligned}\n",
        "$$\n",
        "と近似し，方策パラメータを$\\theta \\leftarrow \\theta + \\eta \\widehat{\\nabla_\\theta J(\\theta) }$に従って更新します．\n",
        "ここで，$\\eta > 0$は学習率を表し，問題や方策毎に調整が必要となります．\n",
        "また，状態，行動，累積報酬にはエピソードのインデックスが追加されており，終了ステップもエピソード毎に異なることに注意してください．"
      ],
      "metadata": {
        "id": "VGo279ppMl4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下では，REINFORCEアルゴリズムを実装しています．\n",
        "\n",
        "`select_action`は状態観測を受け取り，方策に従って行動を選択しています．\n",
        "実装上は，観測状態をPytorchのテンソル形式に変換し，これをActorに入力して行動確率確率を計算し，これに従って行動をサンプルしています．\n",
        "パラメータを更新する際に必要になる$\\ln \\pi(a \\mid s)$も出力しています．\n",
        "\n",
        "`rollout`は現在の方策を用いて1エピソード分，環境と状態遷移を繰り返し，その際の即時報酬列と$\\ln \\pi(a \\mid s)$列を返します．\n",
        "\n",
        "`update`はActorのパラメータを更新する関数です．\n",
        "ここでは，エピソード数は$E = 1$を前提としています．\n",
        "確率的傾斜法の理屈から，$E$を大きくして方策勾配の分散を小さくすることと，学習率$\\eta$を小さくしてパラメータ更新の分散を小さくすることには類似の効果があることが知られています．\n",
        "そのため，以下の議論では主に$E = 1$を前提としていきます．\n",
        "\n"
      ],
      "metadata": {
        "id": "XBpIvjyPXvPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReinforceAgent:\n",
        "    def __init__(self, env, model, device, lr):\n",
        "        self.device = device\n",
        "        self.policy = model\n",
        "        self.env = env\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        return self.select_action(observation)[0]\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # 行動選択\n",
        "        observation_ = Variable(torch.Tensor(observation)).to(self.device)\n",
        "        action_probs = self.policy(observation_)\n",
        "        log_probs = action_probs.log()\n",
        "        action = Categorical(action_probs).sample()\n",
        "        return action.data.cpu().numpy(), log_probs[action]\n",
        "\n",
        "    def rollout(self):\n",
        "        # 1 エピソード実行\n",
        "        observation, info = self.env.reset()\n",
        "        steps = 0\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        while not (terminated or truncated):\n",
        "            action, log_prob = self.select_action(observation)\n",
        "            next_observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            observation = next_observation\n",
        "            steps +=1\n",
        "        return rewards, log_probs\n",
        "\n",
        "    def update(self, rewards, log_probs):\n",
        "        # パラメータの更新\n",
        "        g_array = np.cumsum(np.array(rewards)[::-1])[::-1]\n",
        "        loss = - sum([g * lp for g, lp in zip(g_array, log_probs)]) / len(rewards)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "vKagOnIhjSFC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`LunarLander-v2`環境で，REINFORCEアルゴリズムによる学習を実行してみます．\n",
        "比較的時間がかかりますので，注意してください．（30min程度）"
      ],
      "metadata": {
        "id": "ptJVsdesZOiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "env = gym.make(envname)\n",
        "\n",
        "model = Actor(dim_state = 8, num_action = 4).to(device)\n",
        "agent = ReinforceAgent(env, model, device, lr=1e-4)"
      ],
      "metadata": {
        "id": "upMhDE3cjSC_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは，100エピソード毎に学習された方策を用いて得られる累積報酬の平均値と標準偏差を表示しています．"
      ],
      "metadata": {
        "id": "I2PEmHGOZpXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interval = 100\n",
        "returns = np.zeros((100, interval))\n",
        "\n",
        "for i in range(returns.shape[0]):\n",
        "    for j in range(returns.shape[1]):\n",
        "        rewards, log_probs = agent.rollout()\n",
        "        agent.update(rewards, log_probs)\n",
        "        returns[i, j] = np.sum(rewards)\n",
        "    print(interval * (i+1), np.mean(returns[i]), np.std(returns[i]))"
      ],
      "metadata": {
        "id": "V26zgXlbjSAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b9f2b4-e845-4e69-d055-1b78f071f615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 -215.55651266291977 125.18010552537586\n",
            "200 -177.62536831290655 99.22229716939289\n",
            "300 -199.17520398673415 108.76910168503156\n",
            "400 -160.8052318534324 92.27953504077163\n",
            "500 -197.2215528199149 108.22883591168464\n",
            "600 -179.29943967518935 93.37064761185098\n",
            "700 -177.12723637717448 111.9778074426303\n",
            "800 -187.25367595224714 109.64336606831283\n",
            "900 -174.51918943066713 99.83690558575242\n",
            "1000 -146.85939729516952 82.28843298370856\n",
            "1100 -163.17519892089646 98.12431543981401\n",
            "1200 -175.7809211201302 112.07418567628748\n",
            "1300 -176.16724903976817 92.6623429550487\n",
            "1400 -160.11058303164845 95.86116049740542\n",
            "1500 -154.8231618971021 88.1576602043093\n",
            "1600 -191.5976697876359 104.27725837380167\n",
            "1700 -165.89567506219464 93.9511310690935\n",
            "1800 -155.4223498927815 81.39673053820279\n",
            "1900 -158.37590138059204 80.75541286686969\n",
            "2000 -163.18290077456018 85.33790299300009\n",
            "2100 -160.56633851190475 103.18161561901053\n",
            "2200 -154.44542435142077 79.02044495157658\n",
            "2300 -137.09678479763733 88.09374563411565\n",
            "2400 -139.11065055976965 79.96634914010339\n",
            "2500 -148.1373948747376 81.83920851479435\n",
            "2600 -156.05976862797485 85.02828595087556\n",
            "2700 -146.8580521116351 79.0350215640683\n",
            "2800 -137.70328315787864 79.90195704081135\n",
            "2900 -163.6097060814597 90.96121507509464\n",
            "3000 -141.8482847689784 84.89963706106451\n",
            "3100 -126.33107960533444 60.900317973660655\n",
            "3200 -141.42523066969406 76.41244059504349\n",
            "3300 -146.28468137059997 80.19442190518633\n",
            "3400 -148.01161932154548 84.2378782344819\n",
            "3500 -144.44235979054636 77.86656023281684\n",
            "3600 -163.37370641478398 98.67436420729261\n",
            "3700 -141.10672338210006 66.72512337845605\n",
            "3800 -134.8394114310412 72.13486271889889\n",
            "3900 -143.09660434044517 77.54287370818383\n",
            "4000 -147.33069108048178 91.71465241946561\n",
            "4100 -140.28843518294616 72.91948205334207\n",
            "4200 -142.24838722703345 82.88523589731646\n",
            "4300 -131.62341969319706 75.27269247984553\n",
            "4400 -138.58425734286567 69.77380089172716\n",
            "4500 -130.81496339801876 70.80291822242513\n",
            "4600 -147.8809462984248 80.3726015044268\n",
            "4700 -124.87471407556411 65.99875619822815\n",
            "4800 -129.10667372689483 68.96448647846299\n",
            "4900 -132.76670314477812 76.72065100508117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "累積報酬の移り変わりを可視化してみます．累積報酬の平均値は改善していく様子が見られると思います．ただ，比較的標準偏差が大きいことが確認できるでしょう．"
      ],
      "metadata": {
        "id": "f6dwfJlXaHTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = np.arange(1, 1+returns.size, returns.shape[1])\n",
        "avg = np.mean(returns, axis=1)\n",
        "std = np.std(returns, axis=1)\n",
        "plt.errorbar(episodes, avg, std, linestyle=':', marker='^')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "hqSNpXX_-3Ox",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "0d6371ae-c0de-4943-b600-362dcb5b34dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習後の経験分布関数を確認してみましょう．\n",
        "横軸は累積報酬を$-1$倍したものになりますので，小さい値が高い割合で得られるほど，望ましい方策が得られていると言えるでしょう．"
      ],
      "metadata": {
        "id": "a_paerEX87NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=agent, render=False)\n",
        "    return_array[i] = cumulative_reward(history)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "LFnPKD7v8IcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それでは学習結果を確認してみましょう．\n",
        "累積報酬の標準偏差が比較的大きいことが予想されますので，何度か実行し，確認してみましょう．"
      ],
      "metadata": {
        "id": "vOwLxBg_76NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=agent, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "pu_cbDuujR65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "前回のブラックボックス最適化法を用いた方策最適化に対して，方策勾配法は累積報酬だけでなく状態遷移の履歴を活用した方策最適化法になっています．しかし，あまり効率的になっている印象は持てないかもしれません．\n",
        "実際には方策モデルが異なるため今回の実験からは実行時間の比較ができません．\n",
        "また，強化学習では「期待累積報酬」を最大化しており，特定の乱数系列（特定の初期状態）のもとでの累積報酬最適化を行っていた前回の結果と比較するのはフェアではありません．\n",
        "\n",
        "ただ，上記のREINFORCEが効率的でないことも事実です．\n",
        "一つの原因として，「方策勾配は推定分散が大きい」ことが挙げられます．\n",
        "REINFORCEでは，エピソード中に得られた累積報酬を用いて価値関数を近似し，これを用いて方策勾配を近似しています．\n",
        "しかし，実行結果を見てもわかるように，累積報酬の値は大きな分散を持っているため，これを用いて推定される方策勾配も大きな分散を持ってしまいます．\n",
        "方策勾配の分散が大きい場合，これに伴って学習率を小さくしなければならず，学習が遅くなります．"
      ],
      "metadata": {
        "id": "Qdn9vkX0_nMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ベースラインの導入による方策勾配の推定分散削減\n",
        "\n",
        "方策勾配の推定分散を削減するために，方策勾配の式を見直してみましょう．\n",
        "方策勾配は以下の式を近似しています．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_s \\mathrm{E}_a \\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_s \\left[ \\sum_{a} Q^{\\pi}(s, a) \\pi_\\theta(a \\mid s)  \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_s \\left[ \\sum_{a} Q^{\\pi}(s, a) \\nabla_{\\theta} \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "この式を修正してみます．まず，ある関数$b(s)$について，\n",
        "$$\n",
        "\\sum_{a} b(s) \\nabla_{\\theta} \\pi_\\theta(a \\mid s)\n",
        "=\n",
        "b(s) \\sum_{a} \\nabla_{\\theta} \\pi_\\theta(a \\mid s)\n",
        "=\n",
        "b(s) \\nabla_{\\theta} \\sum_{a} \\pi_\\theta(a \\mid s)\n",
        "=\n",
        "b(s) \\nabla_{\\theta} 1\n",
        "= 0\n",
        "$$\n",
        "が成り立つことに注意すれば，上の式は，\n",
        "$$\\begin{aligned}\n",
        "\\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "&= \\mathrm{E}_s \\left[ \\sum_{a} \\left( Q^{\\pi}(s, a) - b(s) \\right) \\nabla_{\\theta} \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_{s,a} \\left[ \\left( Q^{\\pi}(s, a) - b(s) \\right) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "と書き直すことができることがわかると思います．\n",
        "すなわち，行動価値関数$Q^{\\pi}(s, a)$から，状態のみに依存する任意の関数$b(s)$を引いたとしても，この期待値は変化しないことがわかります．\n",
        "しかし，これを推定する場合，\n",
        "$$\n",
        "\\left( q(s, a) - b(s) \\right) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)\n",
        "$$\n",
        "の期待値は上の議論から代わりませんが，その推定分散は$b(s)$に依存することになります．\n",
        "そこで，この$b(s)$を工夫することで，推定量分散を小さくする試みがあります．\n",
        "\n",
        "代表的な$b(s)$の選択肢は，行動価値関数の行動についての期待値，すなわち状態価値関数です．\n",
        "行動価値から状態価値を引いた値はアドバンテージなどと呼ばれ，しばしば強化学習の文脈で現れる量になります．\n",
        "状態価値をベースラインとして利用することは，必ずしも最適ではありませんが，合理的と考えられる理由があります．これについて簡単に考察してみましょう．\n",
        "まず，$\\nabla \\ln \\pi(a \\mid s)$に着目します．\n",
        "これは，$\\ln \\pi(a \\mid s)$の勾配ですから，状態$s$で行動$a$を選択する確率が上昇する方向にパラメータが更新されます．\n",
        "上のREINFORCEでは，エピソード中に実際に観測した状態$s_t$とそこで選択された行動$a_t$について，$\\ln \\pi(a_t \\mid s_t)$に累積報酬を乗じた方向にパラメータが更新されています．\n",
        "ここで，Lunar-Landerの例に着目すると，エピソード中を通してほとんどの場合に累積報酬が負の値になっていることがわかると思います．\n",
        "このとき，どのような行動を取ったとしても，選択された行動の選択確率を下げる方向にパラメータは更新されることになります．\n",
        "これでは効率が悪い更新になっていると想像されます．\n",
        "期待値を累積報酬から減ずることにより，その状態で得られる累積報酬の期待値よりも高い価値を持つ行動については行動選択確率を上げ，逆に価値の低い行動については行動選択確率を下げる，といった更新が可能になります．\n",
        "（なお，常に行動選択確率を下げるにもかかわらず方策が収束していくのは，その下げ幅が報酬の大小関係により異なるため，長い目でみれば報酬の高い行動の選択確率が相対的に上がることになるためである）\n",
        "\n"
      ],
      "metadata": {
        "id": "lcx6bQC0V0s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ベースラインを導入したREINFORCEアルゴリズム\n",
        "\n",
        "それでは状態価値をベースラインとしたREINFORCEアルゴリズムを実装してみましょう．\n",
        "追加で必要なコンポーネントは，状態価値の推定部分です．\n",
        "状態価値は，各状態でその先に得られる累積報酬の期待値で与えられます．\n",
        "そこで，状態価値を近似する関数モデルを$b_\\phi(s)$としたとき，1エピソード中の状態遷移履歴を用いて，\n",
        "$$\n",
        "L(\\phi) = \\frac{1}{T}\\sum_{t=1}^{T} (G_t - b_{\\phi}(s_{t-1}))^2\n",
        "$$\n",
        "を最小化するように$\\phi$を学習することが考えられます．"
      ],
      "metadata": {
        "id": "54PYDEl1Fo8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下では，ベースラインを計算するネットワークを作成しています．\n",
        "ここでもActor-Criticにならい，ベースラインをCriticと呼ぶことにします．\n",
        "\n",
        "CriticのアーキテクチャはActorと同様とします．\n",
        "異なる点は，出力が1次元であり，出力には活性化関数を設けないという点です．"
      ],
      "metadata": {
        "id": "IK-B4miwa4IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, dim_state, dim_hidden=128):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_state, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eQFnydEz8ovf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベースラインを導入したREINFORCEアルゴリズムを実装します．\n",
        "`update`関数において，criticを更新している点が前述のREINFORCEとの差分です．"
      ],
      "metadata": {
        "id": "ni4vEzqDba5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReinforceBaselineAgent:\n",
        "    def __init__(self, env, actor, critic, device, lr_a, lr_c):\n",
        "        self.device = device\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.env = env\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_a)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_c)\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        return self.select_action(observation)[0]\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # 行動選択\n",
        "        observation_ = Variable(torch.Tensor(observation)).to(self.device)\n",
        "        action_probs = self.actor(observation_)\n",
        "        log_probs = action_probs.log()\n",
        "        action = Categorical(action_probs).sample()\n",
        "        return action.data.cpu().numpy(), log_probs[action]\n",
        "\n",
        "    def rollout(self):\n",
        "        # 1 エピソード実行\n",
        "        observation, info = self.env.reset()\n",
        "        steps = 0\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "        states = []\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        while not (terminated or truncated):\n",
        "            action, log_prob = self.select_action(observation)\n",
        "            next_observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            states.append(observation)\n",
        "            observation = next_observation\n",
        "            steps +=1\n",
        "        return rewards, log_probs, states\n",
        "\n",
        "    def update(self, rewards, log_probs, states):\n",
        "        g_array = np.cumsum(np.array(rewards)[::-1])[::-1]\n",
        "        b_list = [self.critic(torch.Tensor(s).to(device)).cpu() for s in states]\n",
        "        # Actor の更新\n",
        "        b_array = torch.cat(b_list, dim=0).detach().numpy()\n",
        "        g_array = g_array - b_array\n",
        "        loss_a = - sum([g * lp for g, lp in zip(g_array, log_probs)]) / len(rewards)\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        loss_a.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        # Critic の更新\n",
        "        loss_c = sum([(b - g)**2 for g, b in zip(g_array, b_list)]) / len(states)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        loss_c.backward()\n",
        "        self.critic_optimizer.step()\n"
      ],
      "metadata": {
        "id": "GeCzR9UQjR0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "env = gym.make(envname)\n",
        "\n",
        "actor2 = Actor(dim_state = 8, num_action = 4).to(device)\n",
        "critic2 = Critic(dim_state = 8).to(device)\n",
        "agent2 = ReinforceBaselineAgent(env, actor2, critic2, device, lr_a=1e-4, lr_c=1e-4)"
      ],
      "metadata": {
        "id": "4cJI3P6pAT9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interval = 100\n",
        "returns2 = np.zeros((100, interval))\n",
        "\n",
        "for i in range(returns2.shape[0]):\n",
        "    for j in range(returns2.shape[1]):\n",
        "        rewards, log_probs, states = agent2.rollout()\n",
        "        agent2.update(rewards, log_probs, states)\n",
        "        returns2[i, j] = np.sum(rewards)\n",
        "    print(interval * (i+1), np.mean(returns2[i]), np.std(returns2[i]))"
      ],
      "metadata": {
        "id": "lHdMuxUQAife"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベースラインの有無による学習曲線の差を可視化してみましょう．\n",
        "比較のため，Actorの学習率をそろえていますが，ベースラインの導入により，方策勾配の分散が削減されるので，少し大きめの学習率を設定することも可能になり，その結果として高速化することも可能になると期待されます．\n",
        "ただ，一方で，ActorとCriticの学習率の両方を調整することが必要となるため，パラメータ調整が実用上は面倒になりえることは述べておきます．\n",
        "\n",
        "[*重要*]\n",
        "ここで比較しているアプローチは，学習結果が乱数に依存します．そのため，本来一試行の結果を比較するというのは適切ではありません．偶然偏った結果が得られている可能性があるからです．例えば平均性能を比較したければ，複数試行学習を実施し，平均や標準偏差，もしくは中央値と四分位範囲，を可視化して比較するなどしましょう．"
      ],
      "metadata": {
        "id": "j5j-ip9ZcDxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = np.arange(1, 1+returns.size, returns.shape[1])\n",
        "avg1 = np.mean(returns, axis=1)\n",
        "std1 = np.std(returns, axis=1)\n",
        "plt.errorbar(episodes, avg1, std1, linestyle=':', marker='^')\n",
        "avg2 = np.mean(returns2, axis=1)\n",
        "std2 = np.std(returns2, axis=1)\n",
        "plt.errorbar(episodes, avg2, std2, linestyle=':', marker='v')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "SOXTLc4oxCXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "経験分布関数についても確認しておきましょう．"
      ],
      "metadata": {
        "id": "pPZYoMrmdNjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=agent2, render=False)\n",
        "    return_array[i] = cumulative_reward(history)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "iVWlj9sJSXlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習結果の確認は以下のコードで行います．"
      ],
      "metadata": {
        "id": "O5QmAygcdr9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=agent2, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "TW0fJ1TswjLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自習課題\n",
        "\n",
        "* 方策を変えてみましょう．特に，中間層のノード数を変更した場合に，学習効率がどの程度変わるのか，グラフを作成するなどして確認しましょう．\n",
        "\n",
        "* 学習率を調整してみましょう．特に，ベースラインを導入したREINFORCEでは，Actorの学習率とCriticの学習率について，効率的なパラメータの関係を確認してみましょう．\n",
        "\n",
        "* タスクを変えてみましょう．タスクが異なれば，適切な方策（ノード数など）や適切な学習率も変化する可能性があります．これを確認してみましょう．"
      ],
      "metadata": {
        "id": "uwEbG_9Bd26g"
      }
    }
  ]
}