{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFwE1h0o7rI2Gx8VsnhOlq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akimotolab/Policy_Optimization_Tutorial/blob/main/ex2_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 課題内容\n",
        "\n",
        "ここでは，`2_policy_graident.ipynb`および`3_actor_critic.ipynb`で学んだ内容を踏まえて，以下の課題について実験的に評価します．\n"
      ],
      "metadata": {
        "id": "7wvixmLudn3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 課題１．バッチ型Actor-Critic法とオンライン型Actor-Critic法の比較\n",
        "\n",
        "`3_actor_critic.ipynb`にて紹介した２つのActor-Critic法（オンライン更新型，バッチ更新型）を，２つ以上の環境（一つは`LunarLander-v2`とする）において比較しましょう．\n",
        "公平な比較となるように，方策は２つの方法で同じモデルとする．\n",
        "また，必ずしも公平とは言えないが，学習率パラメータについてはノートブックに示した値を用いることにする．\n",
        "\n",
        "強化学習では累積報酬の期待値の最大化が目的となるため，方策の評価には平均累積報酬値を用いることとする（`ex1_evolutionary_policy_optimization.ipynb`とは異なることにに注意）．\n",
        "また，`ex1_evolutionary_policy_optimization.ipynb`と同様，各方法で乱数を変えて5試行学習を行うこととし，最大ステップ数を$10^6$とした場合の学習曲線を作図すること．\n",
        "ただし，一度の学習にそれなりの時間を必要とするため，所望の図が作図できるのかの確認をする際には，$10^4$ステップなどと少なめに設定して動作検証をすることが望ましい．\n",
        "また，`Pendulum-v1`と異なり，`LunarLander-v2`の一エピソード毎のステップ数は異なるため，図の横軸は公平な比較となるように，ステップ数とすべきであることに注意しましょう．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3rYuwpMeEL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 課題２．オンライン型Actor-Critic法における方策表現能力の影響\n",
        "\n",
        "オンライン型Actor-Critic法において，ActorおよびCriticの中間ノード数（`dim_hidden`）を変更し，その影響を調査する．\n",
        "中間ノード数は，32, 64, 128, 256 とし，ActorとCriticそれぞれについて変更した場合の影響を調べること．\n",
        "それ以外の設定については課題１と共通とする．"
      ],
      "metadata": {
        "id": "DInZx_38eH_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# レポートについて\n",
        "\n",
        "レポートの執筆方法は`ex1_evolutionary_policy_optimization.ipynb`に従ってください．ただし，実験目的，設定，結果は課題毎に異なるため，課題１，２のそれぞれについて，実験目的から実験結果までを記載してください．共通の設定については，例えば課題２についてまとめている箇所において，「〜については課題１と同様とする．」などと記載すれば十分です．異なる点については必ず明記しましょう．"
      ],
      "metadata": {
        "id": "WkQJmutIfmBe"
      }
    }
  ]
}