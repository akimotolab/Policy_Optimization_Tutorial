{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjWhy2TdcbqD+rGulOHR53",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akimotolab/Policy_Optimization_Tutorial/blob/main/04_deterministic_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備\n",
        "\n",
        "まず，必要なパッケージのインストールとインポート，および仮想displayを設定します．"
      ],
      "metadata": {
        "id": "qWe6P9xqH8be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXoBp0fgHyhe"
      },
      "outputs": [],
      "source": [
        "# 必要なパッケージのインストール\n",
        "!apt update\n",
        "!pip install swig\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import torch\n",
        "\n",
        "# 仮想ディスプレイの設定\n",
        "_display = Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "metadata": {
        "id": "iJ1PYM90IHa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "d5Oklvxh5nAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "続いて，第１回の資料で定義した`rollout`などの基本的な関数をここでも定義しておきます．"
      ],
      "metadata": {
        "id": "ci2EEsRpD1oL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 決定的方策勾配法"
      ],
      "metadata": {
        "id": "3kIwkB82z0Jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回は「Twin-Delayed Deep Deterministic Policy Gradient (TD3) アルゴリズム」について見ていきます．\n",
        "TD3は2018年に発表され，以降，非常に広く利用されているActor-Critic法の一種です．\n",
        "\n",
        "参考文献：Fujimoto et al. Addressing Function Approximation Error in Actor-Critic Methods, ICML 2018."
      ],
      "metadata": {
        "id": "JfrVciBhZ3V0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD3のコード"
      ],
      "metadata": {
        "id": "qnKTAGiK-_4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下に，TD3の著者らによって公開されているTD3のコードを最新のgymnasiumのインターフェースに合わせるためにわずかに修正したコードを示します．まずは実行してみましょう．"
      ],
      "metadata": {
        "id": "t6kR6iVLbDfQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxzYcOY7zewo"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "経験再生バッファー：過去に訪問にした状態，とった行動，遷移した状態，得られた報酬，終了判定，を保存しておくバッファーです．"
      ],
      "metadata": {
        "id": "HIynLYDPbVNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_size):\n",
        "        self.max_size = max_size\n",
        "        self.state = np.zeros((max_size, state_dim))\n",
        "        self.action = np.zeros((max_size, action_dim))\n",
        "        self.next_state = np.zeros((max_size, state_dim))\n",
        "        self.reward = np.zeros((max_size, 1))\n",
        "        self.done = np.zeros((max_size, 1))\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def add(self, state, action, next_state, reward, terminated):\n",
        "        self.state[self.ptr] = state\n",
        "        self.action[self.ptr] = action\n",
        "        self.next_state[self.ptr] = next_state\n",
        "        self.reward[self.ptr] = reward\n",
        "        self.done[self.ptr] = float(terminated)\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def get(self, indeces):\n",
        "        return (\n",
        "            self.state[indeces],\n",
        "            self.action[indeces],\n",
        "            self.next_state[indeces],\n",
        "            self.reward[indeces],\n",
        "            self.done[indeces]\n",
        "        )\n",
        "\n",
        "    def clear(self):\n",
        "        self.state[:, :] = 0.0\n",
        "        self.action[:, :] = 0.0\n",
        "        self.next_state[:, :] = 0.0\n",
        "        self.reward[:] = 0.0\n",
        "        self.done[:] = 0.0\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.choice(self.size, batch_size, replace=False)\n",
        "        return self.get(ind)\n"
      ],
      "metadata": {
        "id": "wXT6-r-L0_qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "アクター：方策そのものです．"
      ],
      "metadata": {
        "id": "E6ASip0zboPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, hidden1_dim, hidden2_dim, min_action, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.nlayer = 2 if hidden2_dim == 0 else 3\n",
        "        self.l1 = nn.Linear(state_dim, hidden1_dim)\n",
        "        if self.nlayer == 2:\n",
        "            self.l3 = nn.Linear(hidden1_dim, len(min_action))\n",
        "        else:\n",
        "            self.l2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
        "            self.l3 = nn.Linear(hidden2_dim, len(min_action))\n",
        "        self.center_action = torch.FloatTensor((max_action + min_action) / 2)\n",
        "        self.radius_action = torch.FloatTensor((max_action - min_action) / 2)\n",
        "\n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.l1(state))\n",
        "        if self.nlayer == 3:\n",
        "            a = F.relu(self.l2(a))\n",
        "        a = torch.tanh(self.l3(a))\n",
        "        return self.center_action + self.radius_action * a\n"
      ],
      "metadata": {
        "id": "FNAxlaRD0_nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "クリティック：状態行動の価値を推定する関数です．"
      ],
      "metadata": {
        "id": "y6P6RSUQbrfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden1_dim, hidden2_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.nlayer = 2 if hidden2_dim == 0 else 3\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, hidden1_dim)\n",
        "        if self.nlayer == 2:\n",
        "            self.l3 = nn.Linear(hidden1_dim, 1)\n",
        "        else:\n",
        "            self.l2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
        "            self.l3 = nn.Linear(hidden2_dim, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "        q = F.relu(self.l1(sa))\n",
        "        if self.nlayer == 3:\n",
        "            q = F.relu(self.l2(q))\n",
        "        q = self.l3(q)\n",
        "        return q"
      ],
      "metadata": {
        "id": "bGLhpvO80_iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TD3：\n",
        "- save_models : 内部のパラメータを保存し，あとから呼び出して評価できるようにする関数\n",
        "- load_models : save_modelsで保存したパラメータを読み込む関数\n",
        "- select_action : 状態をうけとって行動を返す関数．\n",
        "- select_exploratory_action : 探索のための行動を返す関数．\n",
        "- train : １ステップの学習を実行する関数"
      ],
      "metadata": {
        "id": "3G7wO7Hcbxlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3:\n",
        "    # https://github.com/sfujim/TD3\n",
        "    def __init__(self, env, actor, critic1, critic2, state_dim, min_action, max_action, discount, expl_noise, lr, startup_time, tau, policy_noise, noise_clip, policy_freq, buffer_size, batch_size):\n",
        "\n",
        "        self.env = env\n",
        "\n",
        "        self.actor = actor\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.critic1 = critic1\n",
        "        self.critic1_target = copy.deepcopy(self.critic1)\n",
        "        self.critic1_optimizer = torch.optim.Adam(self.critic1.parameters(), lr=lr)\n",
        "        self.critic2 = critic2\n",
        "        self.critic2_target = copy.deepcopy(self.critic2)\n",
        "        self.critic2_optimizer = torch.optim.Adam(self.critic2.parameters(), lr=lr)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.discount = discount\n",
        "        self.expl_noise = expl_noise\n",
        "        self.startup_time = startup_time\n",
        "        self.tau = tau\n",
        "        self.policy_noise = policy_noise\n",
        "        self.noise_clip = noise_clip\n",
        "        self.policy_freq = policy_freq\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer = ReplayBuffer(state_dim, len(self.min_action), buffer_size)\n",
        "\n",
        "        self.total_itr = 0\n",
        "\n",
        "    def save_models(self, path):\n",
        "        torch.save(self.actor.state_dict(), path + '_actor')\n",
        "        torch.save(self.critic1.state_dict(), path + '_critic1')\n",
        "        torch.save(self.critic2.state_dict(), path + '_critic2')\n",
        "        torch.save(self.actor_target.state_dict(), path + '_actor_target')\n",
        "        torch.save(self.critic1_target.state_dict(), path + '_critic1_target')\n",
        "        torch.save(self.critic2_target.state_dict(), path + '_critic2_target')\n",
        "\n",
        "    def load_models(self, path):\n",
        "        self.actor.load_state_dict(torch.load(path + '_actor'))\n",
        "        self.critic1.load_state_dict(torch.load(path + '_critic1'))\n",
        "        self.critic2.load_state_dict(torch.load(path + '_critic2'))\n",
        "        self.actor_target.load_state_dict(torch.load(path + '_actor_target'))\n",
        "        self.critic1_target.load_state_dict(torch.load(path + '_critic1_target'))\n",
        "        self.critic2_target.load_state_dict(torch.load(path + '_critic2_target'))\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1))\n",
        "        return self.actor(state).detach().numpy().flatten()\n",
        "\n",
        "    def select_exploratory_action(self, state):\n",
        "        if self.total_itr < self.startup_time:\n",
        "            action = self.min_action + (self.max_action - self.min_action) * np.random.rand(len(self.min_action))\n",
        "        else:\n",
        "            action = self.select_action(np.array(state))\n",
        "            action += np.random.randn(len(self.min_action)) * (self.expl_noise / 2.0) * (self.max_action - self.min_action)\n",
        "            action = action.clip(self.min_action, self.max_action)\n",
        "        return action\n",
        "\n",
        "    def train(self, state, action, next_state, reward, terminated):\n",
        "        self.total_itr += 1\n",
        "        self.buffer.add(state, action, next_state, reward, terminated)\n",
        "        # Sample replay buffer\n",
        "        if self.buffer.size < self.batch_size:\n",
        "            return\n",
        "        state, action, next_state, reward, terminated = self.buffer.sample(self.batch_size)\n",
        "        state = torch.FloatTensor(state)\n",
        "        action = torch.FloatTensor(action)\n",
        "        next_state = torch.FloatTensor(next_state)\n",
        "        reward = torch.FloatTensor(reward)\n",
        "        done = torch.FloatTensor(terminated)\n",
        "\n",
        "        # train critic\n",
        "        with torch.no_grad():\n",
        "            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
        "            next_action = (self.actor_target(next_state) - self.actor_target.center_action) / self.actor_target.radius_action\n",
        "            next_action = (next_action + noise).clamp(-1.0, 1.0) * self.actor_target.radius_action + self.actor_target.center_action\n",
        "            target_Q1 = reward + self.discount * self.critic1_target(next_state, next_action) * (1 - done)\n",
        "            target_Q2 = reward + self.discount * self.critic2_target(next_state, next_action) * (1 - done)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "        # critic 1\n",
        "        current_Q1 = self.critic1(state, action)\n",
        "        critic1_loss = F.mse_loss(current_Q1, target_Q)\n",
        "        self.critic1_optimizer.zero_grad()\n",
        "        critic1_loss.backward()\n",
        "        self.critic1_optimizer.step()\n",
        "\n",
        "        # critic 2\n",
        "        current_Q2 = self.critic2(state, action)\n",
        "        critic2_loss = F.mse_loss(current_Q2, target_Q)\n",
        "        self.critic2_optimizer.zero_grad()\n",
        "        critic2_loss.backward()\n",
        "        self.critic2_optimizer.step()\n",
        "\n",
        "        if self.total_itr % self.policy_freq == 0:\n",
        "            # actor\n",
        "            actor_loss = -self.critic1(state, self.actor(state)).mean()\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            # target network\n",
        "            for param, target_param in zip(self.critic1.parameters(), self.critic1_target.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "            for param, target_param in zip(self.critic2.parameters(), self.critic2_target.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
      ],
      "metadata": {
        "id": "PLmgbJEs0_fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "実行スクリプト"
      ],
      "metadata": {
        "id": "hyO3V1Dmcc5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "ENV_NAME = 'Pendulum-v1'\n",
        "NEVALS = 10\n",
        "MAX_STEPS = int(1e5)\n",
        "LOG_INTERVAL = int(5e3)\n",
        "\n",
        "seed = 1234\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "random.seed(seed)\n",
        "envseed = random.randint(0, 1000)\n",
        "actseed = random.randint(0, 1000)\n",
        "observation, info = env.reset(seed=envseed)\n",
        "env.action_space.seed(actseed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "directory = Path(\"td3_pendulum_seed{}\".format(seed))\n",
        "directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "dim_action = env.action_space.shape[0]\n",
        "min_action = env.action_space.low * np.ones(dim_action)\n",
        "max_action = env.action_space.high * np.ones(dim_action)\n",
        "dim_state = env.observation_space.shape[0]\n",
        "min_state = env.observation_space.low * np.ones(dim_state)\n",
        "max_state = env.observation_space.high * np.ones(dim_state)\n",
        "hidden1_dim = 256\n",
        "hidden2_dim = 0\n",
        "discount = 0.99\n",
        "expl_noise = 0.1\n",
        "lr = 3e-4\n",
        "startup_time = int(1e4)\n",
        "tau = 0.005\n",
        "policy_noise = 0.2\n",
        "noise_clip = 0.5\n",
        "policy_freq = 2\n",
        "buffer_size = MAX_STEPS\n",
        "batch_size = 256\n",
        "\n",
        "actor = Actor(dim_state, hidden1_dim, hidden2_dim, min_action, max_action)\n",
        "critic1 = Critic(dim_state, dim_action, hidden1_dim, hidden2_dim)\n",
        "critic2 = Critic(dim_state, dim_action, hidden1_dim, hidden2_dim)\n",
        "agent = TD3(env, actor, critic1, critic2, dim_state, min_action, max_action, discount, expl_noise, lr, startup_time, tau, policy_noise, noise_clip, policy_freq, buffer_size, batch_size)\n",
        "\n",
        "state, info = env.reset()\n",
        "cum_reward_train = 0.0\n",
        "for t in range(1, MAX_STEPS+1):\n",
        "    # 探索のための行動選択\n",
        "    action = agent.select_exploratory_action(state)\n",
        "    # 状態遷移\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    # Actor および Critic の学習\n",
        "    agent.train(state, action, next_state, reward, terminated)\n",
        "    #\n",
        "    state = next_state\n",
        "    cum_reward_train += reward\n",
        "    if terminated or truncated:\n",
        "        state, info = env.reset()\n",
        "        print(t, cum_reward_train)\n",
        "        cum_reward_train = 0\n",
        "    if t % LOG_INTERVAL == 0:\n",
        "        agent.save_models(directory.name + '/step{}'.format(t))\n",
        "env.close()"
      ],
      "metadata": {
        "id": "5tmUbADz1UoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最終的に得られた方策について，累積報酬（の-1倍）の経験分布関数を確認してみましょう．"
      ],
      "metadata": {
        "id": "P_dL_hbVfsoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1234\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "random.seed(seed)\n",
        "envseed = random.randint(0, 1000)\n",
        "actseed = random.randint(0, 1000)\n",
        "observation, info = env.reset(seed=envseed)\n",
        "env.action_space.seed(actseed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    cum_reward = 0.0\n",
        "    state, info = env.reset()\n",
        "    for t in range(1, MAX_STEPS+1):\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        state = next_state\n",
        "        cum_reward += reward\n",
        "        if terminated or truncated:\n",
        "            return_array[i] = cum_reward\n",
        "            break\n",
        "env.close()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "VMlgDN7xNnq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "挙動を確認してみましょう．実行するたびに初期値が変わりますので，何度か実行してみましょう．"
      ],
      "metadata": {
        "id": "xWKtF9uN8KAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "img = []\n",
        "\n",
        "d = Display()\n",
        "d.start()\n",
        "\n",
        "\n",
        "# 可視化用エピソードの実行\n",
        "observation, info = env.reset()\n",
        "img.append(env.render())\n",
        "terminated = False\n",
        "truncated = False\n",
        "while not (terminated or truncated):\n",
        "    action = agent.select_action(observation)\n",
        "    next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "    history.append([observation, action, next_observation, reward, terminated, truncated, info])\n",
        "    observation = next_observation\n",
        "    display.clear_output(wait=True)\n",
        "    img.append(env.render())\n",
        "env.close()\n",
        "\n",
        "# 可視化\n",
        "dpi = 72\n",
        "interval = 50\n",
        "plt.figure(figsize=(img[0].shape[1]/dpi, img[0].shape[0]/dpi), dpi=dpi)\n",
        "patch = plt.imshow(img[0])\n",
        "plt.axis=('off')\n",
        "animate = lambda i: patch.set_data(img[i])\n",
        "ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(img), interval=interval)\n",
        "display.display(display.HTML(ani.to_jshtml()))\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "6dhQ_dNPdyHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## アルゴリズムの解説"
      ],
      "metadata": {
        "id": "RyVL8heE_Jw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "得られている方策の振る舞いが，`3_actor_critic.ipynb`で学習した方策よりも圧倒的に良いふるまいになっていることが確認できると思います．また，学習時間も大幅に短縮されていることがわかります．その理由について，オンライン更新型のActor-Critic法との違いに着目し，解説していきます．\n",
        "\n"
      ],
      "metadata": {
        "id": "s3gfcrDu-Y4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 決定的方策"
      ],
      "metadata": {
        "id": "Feqr2MKD_V4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TD3ではその名の通り，「決定的方策」を学習しています．\n",
        "前述のActor-Critic法では，学習過程において様々な行動を選択し，経験できるように，確率的方策（方策が状態で条件付けされた行動選択確率を表現）を用いていました．他方，TD3では，決定的方策（方策が状態から行動への写像を表現）を採用しています．学習対象となる方策は決定的ですが，学習過程では様々な行動を選択することが必要になります．そこで，TD3では，学習対象となる方策と，探索のための方策（行動方策と呼びます）を分けています．\n",
        "\n",
        "プログラム中では，`select_action`が学習対象となる方策による行動選択を表しており，`select_exploratory_action`が行動方策による行動選択を表しています．行動方策は，学習対象となる方策の出力に対して，ノイズをのせることで表現しています．ただし，学習初期の段階では，学習対象となる方策が正しく学習されていないため，取りうる行動からランダムに行動を選択するようにしています．\n",
        "\n",
        "方策が決定的であっても，方策勾配定理と類似の定理が成立し，方策勾配を近似することができます．ここでは本筋から逸れるため，説明は割愛します．\n",
        "\n"
      ],
      "metadata": {
        "id": "MjnlP6dq_tGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 経験再生バッファー"
      ],
      "metadata": {
        "id": "k9W393uhBaTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習効率に最も影響する変更点は，経験再生バッファーを利用している点になります．\n",
        "\n",
        "TD3では前述のオンライン更新型Actor-Critic法と同様，環境とインタラクションするたびに学習（Actor-Criticのパラメータ更新）を実施します．ただし，その際に用いるデータに差があります．\n",
        "- 前述のオンライン更新型Actor-Critic法：各ステップでの状態遷移のデータ（その時の状態，選択した行動，次状態，報酬）を用いて更新します．すなわち，毎回の学習に使うデータは最新の状態遷移のデータ一つ\n",
        "- TD3：各ステップでの状態遷移データを経験再生バッファーに保存しておき，毎回の学習時にはそこからランダムに選択されたバッチサイズ（`batch_size`）を用いて学習します．すなわち，毎回の学習には必ずしも最新のデータを含まない`batch_size`個（上の実行では256個）のデータを用いて更新します．\n",
        "\n",
        "方策勾配の推定に用いるデータ数が多くなるため，推定精度が大幅に向上し，学習率を大きくとることが可能になるため，大幅な効率改善が実現されます．\n",
        "\n",
        "各学習に多くのデータを用いるので，より多くの計算時間が必要になると思われるかもしれませんが，GPU上で並列処理できるため，`batch_size`をある程度まで大きくとっても計算時間はあまり増加しないことに注意してください．\n",
        "\n",
        "なお，経験再生バッファーを使用しない（前述のオンライン更新型Actor-Critic法と同様のデータ活用方法に変更）場合には，実行スクリプトにおいて以下のようにパラメータを設定します．\n",
        "```\n",
        "buffer_size = 1  # 最新のデータだけを保存\n",
        "batch_size = 1  # 一度に使うデータは1つだけ\n",
        "```"
      ],
      "metadata": {
        "id": "tdH5XuuqBePf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Target Actor & Target Critic"
      ],
      "metadata": {
        "id": "XuS9cC19HSw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criticの学習では，TD誤差が小さくなるように学習が進みます．\n",
        "つまり，Criticが表現している方策の行動価値の推定値 $Q^\\omega(s_t, a_t)$ をそのターゲットである $\\Delta = r_{t+1} + \\gamma Q^\\omega(s_{t+1}, \\pi_\\theta(s_{t+1}))$ に近づけるようにCritic を更新します（`3_actor_critic.ipynb`参照．ここでは状態価値でなく，行動価値を推定していますが，ロジックはほとんど同じです．）\n",
        "なお，$\\omega$や$\\theta$はCriticやActorのパラメータを表現しています．\n",
        "しかし，ターゲットである$\\Delta$自体も現在のCritic $Q$やActor $\\pi$に依存しています．これが原因で学習が不安定になることが知られています．\n",
        "\n",
        "これを解消するために，target actor $\\pi_{\\bar{\\theta}}$ と target critic $Q^{\\bar{\\omega}}$ を用意し，$\\Delta$の計算の際に$\\pi_\\theta$および$Q^\\omega$に代わりこれらを用いる方法が提案されています．Target actor と target criticのパラメータは，通常のactorとcriticよりも緩やかに更新されるよう，各ステップにおいて\n",
        "$$\n",
        "\\bar{\\theta} \\leftarrow (1-\\tau) \\bar{\\theta} + \\tau \\theta, \\\\\n",
        "\\bar{\\omega} \\leftarrow (1-\\tau) \\bar{\\omega} + \\tau \\omega$$\n",
        "（$\\tau \\ll 1$）のように更新します．なお，これらのパラメータは$\\bar{\\theta} = \\theta$，$\\bar{\\omega} = \\omega$と初期化されます．\n",
        "\n",
        "なお，$\\tau = 1$ (`tau = 1`) とすれば，Target Actor や Target Critic を用いない方法となります．"
      ],
      "metadata": {
        "id": "Sa3uxVNMIKV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Target Policy Smoothing Regularization"
      ],
      "metadata": {
        "id": "yoUrkealLGZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TD3では深層ネットワークを用いてcriticとactorを表現します．これにより，高い表現能力を持つcriticやactorをモデル化することができます．\n",
        "しかし，深層ネットワークをcriticに用いることから，過学習によって望ましくないピークがcriticに現れることがあります．方策はcriticによって近似される行動価値を最大にするような方策を学習するため，望ましくないピークを出力してしまうような方策が獲得される恐れがあります．\n",
        "\n",
        "これを解消するために，「近い行動の価値は近いはず」という仮定のもと，行動価値関数がなめらかになるよう，$\\Delta$計算の際に target actor の出力$\\pi_{\\bar{\\theta}}$にノイズ $\\epsilon \\sim clip(N(0, \\sigma_{target}), -c, c)$ を加えるといった工夫をしています．ただし，行動が取りうる範囲をはみ出した場合には，境界上に射影しています．\n",
        "\n",
        "なお，上の実行スクリプトにおいて`policy_noise = 0`とすると，この工夫を行わないことになります．"
      ],
      "metadata": {
        "id": "i3n_e0-zLR5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Delayed Policy Update\n"
      ],
      "metadata": {
        "id": "Pble_a8PN0bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actorの学習はcriticの精度に依存します．Criticは現在actorが表現している方策の行動価値を推定しているため，actorの学習よりも早く進むことが望ましいといえます．\n",
        "\n",
        "そこで，actorの更新を`policy_freq`ステップに一度だけ行うようにして，criticの学習が先行するように工夫しています．\n",
        "\n",
        "なお，`policy_freq = 1`とすると，この工夫を使わないことになります．\n"
      ],
      "metadata": {
        "id": "FbQ9ATa7N6xJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Clipped Double Q-Learning"
      ],
      "metadata": {
        "id": "xLHaOQcpOom0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q学習において，推定された行動価値$Q^\\omega(s, \\pi(s))$が真の行動価値$Q^\\pi(s, \\pi(s))$よりも大きく見積もられる問題が，方策勾配法に限らず行動価値を推定する方法において知られています．\n",
        "\n",
        "行動価値の過大評価を防ぎ，価値の推定精度を改善するために，criticを二つ（$Q^{\\omega_1}$，$Q^{\\omega_2}$）用意し，これらをそれぞれ学習させ，二つのcriticの更新におけるターゲット$\\Delta$の計算の際に，$Q^{\\omega_1}(s, a)$と$Q^{\\omega_2}(s, a)$のうち小さい値を採用するといった工夫が取られています．\n",
        "\n",
        "なお，`train`の内部において，\n",
        "```\n",
        "target_Q = torch.min(target_Q1, target_Q2)\n",
        "```\n",
        "を\n",
        "```\n",
        "target_Q = target_Q1\n",
        "```\n",
        "と変更すれば，この工夫を行わない方法となります．ただし，内部的には２つ目のcritic関数を保持・学習しています．"
      ],
      "metadata": {
        "id": "28DM2zjgOumt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自習課題"
      ],
      "metadata": {
        "id": "Tm1WA3FVXPGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TD3で導入されている上述の6つの工夫について，それぞれの効果を検証しましょう．\n",
        "複数の工夫の相乗効果を考慮すると全部で$2^6=64$通りの方法を検証することになり，時間がかかります．そのような場合，各コンポーネントだけを取り除いた方法（6つ）と全ての構成要素を加えた方法（1つ）を比較することで，いずれの要素も重要であることを示すアブレーションテスト（ablation test, ablation study）がしばしば用いられます．アブレーションテストを実施し，各コンポーネントの効果を考察してみましょう．\n",
        "\n",
        "効果は一つの環境（上であれば`Pendulum-v1`）で全て現れるとは限りません．いくつかの環境でテストしてみましょう．TD3は連続状態，連続行動を想定していますので，そのような環境をいくつか選択し，実験結果を比較しましょう．"
      ],
      "metadata": {
        "id": "YnKoloXqXStx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KYP9qBLQXQr0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}