{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqFKoohsa1Ws5KwuVFzVQk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akimotolab/Policy_Optimization_Tutorial/blob/main/ex1_evolutionary_policy_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 課題内容\n",
        "\n",
        "ここでは，`1_policy_optimization_introduction.ipynb`で学んだ内容を発展させ，「より優れた方策を獲得する方法」を検討し，実験的に評価しましょう．\n",
        "\n",
        "そのために，次のようなステップで検討していきます．\n",
        "1. 「より優れた方策を獲得できる方法」という曖昧な目的を明確にしましょう．\n",
        "2. 明確になった目的を評価するための方法を検討しましょう．\n",
        "3. 各々方法を検討し，実験を実施し，比較しましょう．"
      ],
      "metadata": {
        "id": "Hg2fzl5-qtlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. 準備\n",
        "\n",
        "`1_policy_optimization_introduction.ipynb`から必要なコードをコピーしてきます．"
      ],
      "metadata": {
        "id": "A35p2seFsmQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 インストール＆インポート"
      ],
      "metadata": {
        "id": "8ajKBM4TtOab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kD07eD0qoeb"
      },
      "outputs": [],
      "source": [
        "# 必要なパッケージのインストール\n",
        "!apt update\n",
        "!pip install swig\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import torch\n",
        "\n",
        "# 仮想ディスプレイの設定\n",
        "_display = Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "# gpuが使用される場合の設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
      ],
      "metadata": {
        "id": "Y0DaaqMktFvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "4b-fbNQvtI-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 関数やクラスの定義"
      ],
      "metadata": {
        "id": "rv0zezaKtZ-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `rollout`：与えられた方策を用いて環境とインタラクションし，状態行動履歴を返す関数\n",
        "* `visualize`：`rollout`において`render=True`とした場合に返される画像からアニメーションを作成する関数\n",
        "* `cumulative_reward`：`rollout`において返される履歴から累積報酬を計算する関数"
      ],
      "metadata": {
        "id": "07fCxifouQ0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout(envname, policy=None, render=False, seed=None):\n",
        "    if render:\n",
        "        env = gym.make(envname, render_mode=\"rgb_array\")\n",
        "    else:\n",
        "        env = gym.make(envname)\n",
        "    history = []\n",
        "    img = []\n",
        "\n",
        "    # 乱数の設定\n",
        "    if seed is not None:\n",
        "        random.seed(int(seed))\n",
        "    envseed = random.randint(0, 1000)\n",
        "    actseed = random.randint(0, 1000)\n",
        "    observation, info = env.reset(seed=envseed)\n",
        "    env.action_space.seed(actseed)\n",
        "\n",
        "    # 可視化用の設定\n",
        "    if render:\n",
        "        d = Display()\n",
        "        d.start()\n",
        "        img.append(env.render())\n",
        "\n",
        "    # メインループ（環境とのインタラクション）\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # 行動を選択\n",
        "        if policy is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy(observation)\n",
        "\n",
        "        # 行動を実行\n",
        "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "        history.append([observation, action, next_observation, reward, terminated, truncated, info])\n",
        "        observation = next_observation\n",
        "        if render:\n",
        "            display.clear_output(wait=True)\n",
        "            img.append(env.render())\n",
        "    env.close()\n",
        "    return history, img\n",
        "\n",
        "\n",
        "def visualize(img):\n",
        "    dpi = 72\n",
        "    interval = 50\n",
        "    plt.figure(figsize=(img[0].shape[1]/dpi, img[0].shape[0]/dpi), dpi=dpi)\n",
        "    patch = plt.imshow(img[0])\n",
        "    plt.axis=('off')\n",
        "    animate = lambda i: patch.set_data(img[i])\n",
        "    ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(img), interval=interval)\n",
        "    display.display(display.HTML(ani.to_jshtml()))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def cumulative_reward(history):\n",
        "    return sum(hist[3] for hist in history)"
      ],
      "metadata": {
        "id": "z3bJ5t9ttpAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Cmaes`クラス\n",
        "* `ask`：新しい解（方策パラメータ）を一つだけ生成する関数\n",
        "* `tell`：`ask`で生成された解（方策パラメータ）の目的関数値（累積報酬のマイナス1倍）を与えると，次の解を生成するための正規分布の平均ベクトルや共分散行列を更新する関数"
      ],
      "metadata": {
        "id": "-KQg98TbvNEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cmaes:\n",
        "    \"\"\"(1+1)-Active-CMA-ES [Arnold 2010] with Simplified 1/5th Success Rule\"\"\"\n",
        "\n",
        "    def __init__(self, init_fx, init_x, init_s):\n",
        "        self.dim = len(init_x)\n",
        "\n",
        "        self.aup = np.exp(2.0 / (2.0 + self.dim))\n",
        "        self.adown = self.aup ** (-0.25)\n",
        "        self.pthresh = 0.44\n",
        "        self.cp = 1.0 / 12.0\n",
        "        self.cc = 2.0 / (self.dim + 2)\n",
        "        self.ccovp = 2.0 / (self.dim**2 + 6)\n",
        "        self.ccovm = 0.4 / (self.dim**1.6 + 1)\n",
        "\n",
        "        self.x = np.array(init_x, copy=True)\n",
        "        self.xc = np.zeros(self.dim)\n",
        "        self.yc = np.zeros(self.dim)\n",
        "        self.zc = np.zeros(self.dim)\n",
        "        self.s = init_s\n",
        "        self.psucc = 0.5\n",
        "        self.fhist = [init_fx]\n",
        "        self.p = np.zeros(self.dim)\n",
        "        self.chol = np.eye(self.dim)\n",
        "        self.cholinv = np.eye(self.dim)\n",
        "\n",
        "    def ask(self):\n",
        "        self.zc = np.random.randn(self.dim)\n",
        "        self.yc = np.dot(self.chol, self.zc)\n",
        "        self.xc = self.x + self.s * self.yc\n",
        "        return self.xc\n",
        "\n",
        "    def tell(self, fx):\n",
        "        if fx <= self.fhist[0]:\n",
        "            self.fhist = [fx] + self.fhist[:min(4, len(self.fhist))]\n",
        "            self.x = self.xc\n",
        "            self.s *= self.aup\n",
        "            self.psucc = (1.0 - self.cp) * self.psucc + self.cp\n",
        "            if self.psucc > self.pthresh:\n",
        "                self.p = (1.0 - self.cc) * self.p\n",
        "                d = self.ccovp * (1.0 - self.cc * (2.0 - self.cc))\n",
        "            else:\n",
        "                self.p = (1.0 - self.cc) * self.p + np.sqrt(self.cc * (2.0 - self.cc)) * self.yc\n",
        "                d = self.ccovp\n",
        "            w = np.dot(self.cholinv, self.p)\n",
        "            wnorm2 = np.dot(w, w)\n",
        "            if wnorm2 > 1e-6:\n",
        "                a = np.sqrt(1.0 - d)\n",
        "                b = np.sqrt(1.0 - d) * (np.sqrt(1.0 + self.ccovp * wnorm2 / (1.0 - d)) - 1.0) / wnorm2\n",
        "                self.chol = a * self.chol + b * np.outer(self.p, w)\n",
        "                self.cholinv = self.cholinv / a - (b / (a**2 + a * b * wnorm2)) * np.outer(w, np.dot(w, self.cholinv))\n",
        "        else:\n",
        "            self.s *= self.adown\n",
        "            self.psucc = (1.0 - self.cp) * self.psucc\n",
        "            if len(self.fhist) > 4 and fx > self.fhist[4] and self.psucc <= self.pthresh:\n",
        "                znorm2 = np.dot(self.zc, self.zc)\n",
        "                ccov = self.ccovm if 1.0 >= self.ccovm * (2.0 * znorm2 - 1.0) else 1.0 / (2.0 * znorm2 - 1.0)\n",
        "                a = np.sqrt(1.0 + ccov)\n",
        "                b = np.sqrt(1.0 + ccov) * (np.sqrt(1.0 - ccov * znorm2 / (1.0 + ccov)) - 1) / znorm2\n",
        "                self.chol = a * self.chol + b * np.outer(self.yc, self.zc)\n",
        "                self.cholinv = self.cholinv / a - (b / (a**2 + a * b * znorm2)) * np.outer(self.zc, np.dot(self.zc, self.cholinv))"
      ],
      "metadata": {
        "id": "3K53GRLouO1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network方策\n",
        "\n",
        "* 環境を変えた場合，NsやNaも変更することが必要\n",
        "* K はモデルの自由度をコントロールするパラメータ"
      ],
      "metadata": {
        "id": "MVTBOhrQwJ74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NNPolicy:\n",
        "    def __init__(self, weights, Na, Ns, K):\n",
        "        self.B = weights[:K]\n",
        "        self.W = weights[K:K*Ns+K].reshape((K, Ns))\n",
        "        self.V = weights[K*Ns+K:K*Ns+K+K*Na].reshape((Na, K))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        z = np.dot(observation, self.W.T)\n",
        "        z += self.B\n",
        "        z = softmax(z)\n",
        "        z = np.dot(z, self.V.T)\n",
        "        return 2 * np.tanh(z)"
      ],
      "metadata": {
        "id": "bHkhuIJzwFZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "パーセンタイル目的関数\n",
        "\n",
        "*  `NNPolicy`の方策パラメータを引数にとり，10回の独立したエピソードを実行し，得られた累積報酬の90パーセンタイルを出力する関数"
      ],
      "metadata": {
        "id": "10aXw9zkxCl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "Ns = 3\n",
        "Na = 1\n",
        "K = 20\n",
        "N = (Ns + Na + 1) * K\n",
        "\n",
        "def objective(x):\n",
        "    policy = NNPolicy(x, Na, Ns, K)\n",
        "    seed_array = np.arange(100, 1100, 100)\n",
        "    return_array = np.zeros(len(seed_array))\n",
        "    for i, seed in enumerate(seed_array):\n",
        "        history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "        return_array[i] = cumulative_reward(history)\n",
        "    return np.percentile(- return_array, 90) + 1e-10 * np.dot(x, x)"
      ],
      "metadata": {
        "id": "LjmsippCxBwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 実行スクリプト"
      ],
      "metadata": {
        "id": "snBi5CWTy40P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (1+1)-CMA-ESの作成\n",
        "sigma = 10\n",
        "mean = sigma * np.random.randn(N)\n",
        "f_hist = np.empty(1000)  # この長さが最大のイテレーション数\n",
        "f_hist[0] = objective(mean)\n",
        "es = Cmaes(f_hist[0], mean, sigma)\n",
        "\n",
        "# 最適化の実行\n",
        "for t in range(len(f_hist)):\n",
        "    w = es.ask()\n",
        "    f_hist[t] = objective(w)\n",
        "    es.tell(f_hist[t])\n",
        "    if (t+1) % 10 == 0:\n",
        "        print(t+1, np.min(f_hist[:t+1]), es.s)\n",
        "    if t >= 300 and np.min(f_hist[:t+1-300]) == np.min(f_hist[:t+1]):\n",
        "        # 300 イテレーション通して改善が見られない場合には終了\n",
        "        f_hist = f_hist[:t+1]\n",
        "        break\n",
        "\n",
        "# 最適化途中で得られた解の目的関数値を可視化\n",
        "plt.plot(f_hist)"
      ],
      "metadata": {
        "id": "Bm7d6Qy-wgXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 目的を明確にする\n",
        "\n"
      ],
      "metadata": {
        "id": "RGC2lzMQ1J7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "「より優れた方策を獲得できる方法」という表現は曖昧なので，何を持って優れていると判断するかの指標を定量的に定めることが必要です．指標は目的によって変わるものであるため，目的に沿った合理的な指標を定めることが重要です．\n"
      ],
      "metadata": {
        "id": "6pHoo0JB193D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 方策の評価指標\n"
      ],
      "metadata": {
        "id": "HOq0LRHu-6ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは，上述の`objective`関数で評価している指標と同じく，「方策によって得られる累積報酬の90パーセンタイルの値」を方策の良さとして定義します．なお，この評価方法は強化学習の文脈では必ずしも一般的なものではありません．強化学習はあくまで累積報酬の期待値を最大化することを目的とするため，この目的のためにはあまり適さないことになります．進化計算を用いるからこそ可能な指標であることに注意してください．\n",
        "\n",
        "さて，`objective`関数では，固定された乱数10個を用いてそれぞれ1エピソード分のインタラクションを実施し，累積報酬を計算しています．これは最適化の都合上の設計ですが，評価の観点からは適切ではありません．その理由として，以下の２つが挙げられます．\n",
        "\n",
        "1. 10エピソード分の累積報酬（10個のサンプル）で90パーセンタイルを計算しても，その推定精度（理論上の90パーセンタイルに対する10サンプルで計算される推定値の差）は高いとは言えない．\n",
        "\n",
        "2. 関数内で使用されている乱数系列にオーバーフィットした方策が得られており，他の乱数系列で評価した場合にはより悪い90%になる可能性がある．\n",
        "\n",
        "そのため，評価時には最適化時よりも高い精度での90%の推定が求められます．\n",
        "そのための方法としてはいくつか考えられますが，ここでは「最適化時とは異なる100エピソード分の累積報酬値を計算し，それらの90パーセンタイルを計算した結果」を評価指標とします．"
      ],
      "metadata": {
        "id": "ozZxxU158ov0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(policy, envname):\n",
        "    return_array = np.zeros(100)\n",
        "    for i in range(len(return_array)):\n",
        "        history, img = rollout(envname, policy=policy, render=False)\n",
        "        return_array[i] = cumulative_reward(history)\n",
        "    return np.percentile(- return_array, 90), return_array"
      ],
      "metadata": {
        "id": "8OUdZls62uPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の実行例で得られた方策を評価してみます．最適化時に得られた目的関数の値よりも高い値になってしまっていることが確認できます．"
      ],
      "metadata": {
        "id": "yPgTmtWJ-V0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = NNPolicy(es.x, Na, Ns, K)\n",
        "p90, return_array = evaluate(policy, envname)\n",
        "print(p90)"
      ],
      "metadata": {
        "id": "AvEBxIlr1QP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "経験分布関数も確認しておきましょう．"
      ],
      "metadata": {
        "id": "YuNXCQQ9alcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "4guHPMIq-eOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 方法の評価\n"
      ],
      "metadata": {
        "id": "JT0hdACtAMnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "「良い方策」の指標が得られたので，「良い最適化法」の評価指標を検討します．\n",
        "当然良い方策を得ることが目的ですから，より良い方策を得ることが可能な最適化法が優れているということになります．\n",
        "\n",
        "ただし，以下の点に注意が必要です．\n",
        "1. 最適化法を実行するたびに得られる解（方策）が変わる（最適化法自体の確率的な要因，初期値の確率的な要因）\n",
        "2. 最適化法によって，最適化にかかるコスト（ここでは環境とのインタラクション数がボトルネックですので，これをコストと考えます）が異なる\n",
        "\n",
        "一点目に対処するために，最低でも5回程度は同じ最適化を，乱数を変えて実行しましょう．\n",
        "\n",
        "二点目はなかなか難しい問題です．早くある程度良い解を見つけることができる方法と，時間はかかるもののよりよい解を見つけることができる方法のどちらがよいかは，どれだけのコストを許容できるかに依存します．そのため，いくつかのアプローチが取られています．\n",
        "\n",
        "1. 収束したとみなせるほど十分なコストをかけた後に得られる解同士の性能比較\n",
        "2. 十分に良い性能と判断できる性能のしきい値に達するまでに費やしたコスト同士の比較\n",
        "3. 収束曲線を描き，多元的に比較\n",
        "\n",
        "いずれも良し悪しあります．１つ目はコストを気にせずとにかく良い解が得られるのかの評価です．２つ目は同じ性能に達するまでにかかるコストの比較なので，これが可能なのであれば概ね妥当な評価になります．ただし，十分に良い性能と判断できるしきい値の設定に依存してしまいます．３つ目は定量的な評価になりませんが，速度と性能の両者を考慮した比較が可能になります．必ずしも勝者が決まるような比較ではありませんが，方法間の性質の比較を行う際には有益です．ここでは３つ目の方法を採用することにしましょう．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JqXiKZhUX-ME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ２．実験方法の検討"
      ],
      "metadata": {
        "id": "vAs4elsIbPAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "評価方法が決まったので，比較実験の方法を実装していきましょう．\n",
        "比較対象となるアルゴリズムは各自で実装してもらうことになります．\n",
        "\n",
        "\n",
        "今回用いている`Pendulum-v1`環境では，1エピソード毎に実行されるインタラクション数が500で固定です．そのため，コストは単純にエピソード数と考えて問題ありません．また，上で定義した目的関数は一度の90パーセンタイルの計算に10エピソードを必要とします．今回の(1+1)-CMA-ESでは，1イテレーションに1度の目的関数評価を必要とするため，\n",
        "(1+1)-CMA-ESの1イテレーション当たりのコストは10エピソード，となります．アルゴリズムによってこのコストが変わりますので，平等な比較になるように注意が必要です．\n"
      ],
      "metadata": {
        "id": "uu-mzwFVbbCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "比較したい方法を以下のようなかたちで実装しておきます．これは上に示した(1+1)-CMA-ESにほかなりません．ただし，評価方法に合わせてログの取り方を変えています．"
      ],
      "metadata": {
        "id": "q2WV378xpkAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize1(maxeps=10000, logfile='./1p1cmaes.txt'):\n",
        "    # (1+1)-CMA-ESの作成\n",
        "    neps = 10\n",
        "    sigma = 10\n",
        "    mean = sigma * np.random.randn(N)\n",
        "    f_hist = np.empty(maxeps // 10)  # 最適化中の目的関数値のログ\n",
        "    f_hist[:] = np.NAN\n",
        "    f_hist[0] = objective(mean)\n",
        "    es = Cmaes(f_hist[0], mean, sigma)\n",
        "\n",
        "    # 最適化の途中で得られた解を10イテレーション（100エピソード）毎にファイルに書き出し\n",
        "    with open(logfile, 'w') as f:\n",
        "        f.write(str(neps) + ' ' + str(f_hist[0]) + ' ' + ' '.join(map(str, es.x)) + '\\n')\n",
        "\n",
        "    # 最適化の実行\n",
        "    for t in range(1, len(f_hist)):\n",
        "        w = es.ask()\n",
        "        f_hist[t] = objective(w)\n",
        "        neps += 10\n",
        "        es.tell(f_hist[t])\n",
        "\n",
        "        # 最適化の途中で得られた解を10イテレーション（100エピソード）毎にファイルに書き出し\n",
        "        if (t+1) % 10 == 0:\n",
        "            print(t+1, np.min(f_hist[:t+1]), es.s)\n",
        "            with open(logfile, 'a') as f:\n",
        "                f.write(str(neps) + ' ' + str(f_hist[0]) + ' ' + ' '.join(map(str, es.x)) + '\\n')\n",
        "\n",
        "    return es.x"
      ],
      "metadata": {
        "id": "sDTGHzPj_516"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1，1.2節で示した方法で最適化法を評価するために，乱数を変えて最適化を5回繰り返し，その結果を保存します．保存された解を`evaluate`関数を用いて評価することで，それぞれの最適化試行の途中に得られた解を評価します．最後に，5試行分の平均と標準偏差を確認します．"
      ],
      "metadata": {
        "id": "THHmEHFJzlW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 最適化実行\n",
        "# ローカルで実行する場合には，複数試行の実施は並列に実施してしまうと効率的\n",
        "for trial in range(1, 6):\n",
        "    print(\"Trial \", trial)\n",
        "    np.random.seed(trial * 1234)\n",
        "    optimize1(maxeps=10000, logfile='./1p1cmaes_trial{}.txt'.format(trial))"
      ],
      "metadata": {
        "id": "Q_qZw7TVeMxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 方策の評価\n",
        "# 方策の評価も比較的時間がかかるので，ローカルで並列実行するとよい\n",
        "hist = []\n",
        "for trial in range(1, 6):\n",
        "    logfile = './1p1cmaes_trial{}.txt'.format(trial)\n",
        "    dat = np.loadtxt(logfile)\n",
        "    p90_array = np.zeros((2, len(dat)))\n",
        "    for i in range(len(dat)):\n",
        "        p90_array[0, i] = dat[i, 0]\n",
        "        p90_array[1, i] = evaluate(NNPolicy(dat[i, 1:], Na, Ns, K), envname)[0]\n",
        "    hist.append(p90_array)"
      ],
      "metadata": {
        "id": "00aPCXRQ1PO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習曲線の可視化\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "# 各試行の結果をプロット\n",
        "for i, p90_array in enumerate(hist):\n",
        "    plt.plot(p90_array[0], p90_array[1], ':')\n",
        "# 平均と標準偏差をプロット\n",
        "p90_stacked = np.vstack([p90_array[1] for p90_array in hist])\n",
        "p90_avg = np.mean(p90_stacked, axis=0)\n",
        "p90_std = np.std(p90_stacked, axis=0)\n",
        "plt.plot(hist[0][0], p90_avg, '-', color='k')\n",
        "plt.fill_between(hist[0][0], p90_avg - p90_std, p90_avg + p90_std, alpha=0.3, color='k')\n",
        "plt.grid()\n",
        "# 図の保存\n",
        "plt.savefig('1p1cmaes.eps')"
      ],
      "metadata": {
        "id": "MqOk9nUm1qEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "方法間の比較にはこの平均と標準偏差を主に見ていくことになります．各試行での結果もここでは確認していますが，これは平均や標準偏差が5試行を十分に代表していると考えられるかを確認するためです．試行毎にあまりにも大きな偏りがある場合，平均をみることが適切とは言えないので，一度は確認しておくことが重要です．問題なければみやすさのために比較の際にはプロットしないようにしましょう．"
      ],
      "metadata": {
        "id": "gmMGyAlU8_Ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "２つ以上の結果を比較したい場合には，一つの図にして保存しましょう．\n",
        "比較をしたいのに複数の図にまたがっていると比較が難しくなります．\n",
        "また，データを可視化して保存する場合には，ベクトル形式の図（epsなど）で保存しましょう．ビットマップ形式で保存するとデータの情報が欠落してしまうため，科学技術文書では望ましくありません．"
      ],
      "metadata": {
        "id": "7U5ywFMr94P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 アルゴリズムの提案と実験的比較"
      ],
      "metadata": {
        "id": "B8QcyNjc-1OF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "実験の準備は整ったので，あとは(1+1)-CMA-ESよりも優れた方法を検討し，実装しましょう．方法といっても，完全に新しい方法を設計する必要はありません．(1+1)-CMA-ESをベースにして，一部を変更する，といった方法で結構です．抜本的に異なる方法を検討するよりも，一部のみが異なる方法を比較したほうが，どういった工夫がどのように影響するのかを理解しやすいためです（研究の基本です）．\n",
        "\n",
        "結果的に優れているとみなせない方法になっても構いません．ただ，時間の許す限り，試行錯誤してみてください．設計した方法を`optimize1`をまねて`optimize2`などとして実行できる関数として実装しましょう．その後，上の方法に従って比較しましょう．\n",
        "\n",
        "以下に改善案の例を挙げておきます．\n",
        "\n",
        "* 最適化時の目的関数を90パーセンタイルから95パーセンタイルに変更する\n",
        "  - 理由と期待される効果：最適化時に得られている目的関数値と比較して評価時の値が著しく悪いのは，最適化時に用いている乱数系列にオーバーフィットしてしまっているからであると考えられる．ただ，評価時の経験分布関数を見ると，完全に最適化時の乱数系列だけにオーバーフィットしているのではなく，望ましい性能を示す割合が90％よりも低くなってしまっている．そこで，最適化時の目的関数を90パーセンタイルでなく95パーセンタイルや99パーセンタイルなどとしておくことで，この割合が下がったとしても90パーセント以上の試行で優れた性能を示す方策を獲得できるのではないか，と期待される．\n",
        "\n",
        "* 最適化時の目的関数評価におけるエピソード数を100に変更する\n",
        "  - 理由と期待される効果：上述のように，オーバーフィットにより，最適化している目的関数の最小化が評価時の真の目的に対応していない点に改善の余地がある．目的関数はあくまで真の目的の推定値であるから，この推定精度をあげることができれば，乖離を抑えることが可能であると考えられる．目的関数計算時のエピソード数を増加させることで，これは実現可能であると考えられる．しかし，一度の目的関数評価毎に費やすエピソード数が増加するため，最終性能は改善するかもしれないが，コストは多くかかるようになると予想される．\n",
        "\n",
        "上のように，原因となりそうな部分に着目し，これを改善することを考えてみてください．また，その変更による影響を予め予想しておくことが重要です．実験は，この**予想（仮説）が正しいかどうかを検証するプロセス**です．\n",
        "\n"
      ],
      "metadata": {
        "id": "XnPBkask_DbR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vgQh5_hv8j5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. レポートの提出"
      ],
      "metadata": {
        "id": "66IJCsxuHeeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "レポートは次のような章立てで，LaTeXなどを用いて作成してください．\n",
        "\n",
        "1. 実験の目的，検証したい仮説．実験は仮説検証のプロセスですから，必ず目的から書きましょう．目的があって初めて適切な実験設定があります．\n",
        "\n",
        "2. 提案するアルゴリズム\n",
        "\n",
        "  1. 提案法のアイディア，提案の理由と期待される効果（上の記述を参考にしてください）\n",
        "\n",
        "  2. 提案法の具体的な方法．(1+1)-CMA-ESをベースとした方法であれば，アルゴリズムの全体の説明がなくとも，変更点が明確であれば構いませんが，他の人が見てその方法を実装するのに必要な情報を全て記載しましょう．研究論文では再現性といって，重要な評価項目となります．\n",
        "\n",
        "3. 実験方法．この項目も他の人が再現できるように必要な情報を全て記載しましょう．\n",
        "\n",
        "  1. 評価指標の説明\n",
        "  2. 実験設定や具体的な実験方法．使用した環境（'Pendulum-v1'）であること，使用した方策，など，その実験を再現するために必要な情報を記載しましょう．\n",
        "  \n",
        "4. 実験結果．結果として表示する図の読み取り方，図から読み取れること，これが予想と一致しているのかについての結論を述べましょう．\n",
        "\n",
        "5. 考察．今回の結論が得られた原因についての考察を記述しましょう．良い考察は，**単なる予想ではなく，それを裏付ける結果とともに示されているもの**になります．\n",
        "例えば方法Aと方法Bを比較した場合に何らかの違いが性能差として見られたとします．この差は，方法AとBの間のアルゴリズム上の差によって，例えば目的関数の推定精度が変わり，その結果として性能に差が生じていると考えられます．性能の差を見ているだけでは，アルゴリズムの差→推定精度の差，という部分は予想に過ぎないことになります．そこで，性能差だけでなく，推定精度に差が生じているのか，を実際にデータをとって確認することにより，推論に根拠を与えていきます．\n",
        "\n",
        "6. まとめ．今回の実験を振り返り，得られた知見をまとめましょう．また，現在の実験では確認しきれていないこと，検証の弱いところ，考えられる改善点，などについてまとめましょう．\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MK7s6M9mHkls"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XVdK03mmHhfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}