{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMVLzEfmNCTF9N8QfgITt0V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akimotolab/Policy_Optimization_Tutorial/blob/main/1_policy_optimization_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 方策最適化イントロダクション\n",
        "\n",
        "この資料では，方策最適化というものの最適化問題を数値実験を通して理解してもらうことを目的としています．\n",
        "Gymnasiumと呼ばれるシミュレーション環境を用いて，実際に方策を人手で最適化したり，最適化アルゴリズムを用いて最適化する経験を通して，方策最適化を理解します．\n",
        "そのため，理論的な説明は大部分割愛してあります．\n",
        "理論的な部分に興味がある方は，以下の文献などを参考にしてください．\n",
        "\n",
        "Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction, 2nd ed., The MIT Press. (2020) http://incompleteideas.net/book/RLbook2020trimmed.pdf"
      ],
      "metadata": {
        "id": "uAiSyx-A9Y4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Colab上でGymnasiumを利用する準備\n",
        "\n",
        "以下では，必要なパッケージのインストールとインポート，仮想displayの設定を実行しています．\n",
        "手元の環境でGymnasiumを利用する場合には特に難しいことはありませんが，ノートブックで実行する場合には仮想的なdisplayを設定することが必要になるなど，少し面倒です．\n"
      ],
      "metadata": {
        "id": "qWe6P9xqH8be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXoBp0fgHyhe"
      },
      "outputs": [],
      "source": [
        "# 必要なパッケージのインストール\n",
        "!apt update\n",
        "!pip install swig\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "仮想ディスプレイとGPUの設定"
      ],
      "metadata": {
        "id": "Jtb4X5jsQqEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import torch\n",
        "\n",
        "# 仮想ディスプレイの設定\n",
        "_display = Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "# gpuが使用される場合の設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
      ],
      "metadata": {
        "id": "iJ1PYM90IHa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "必要なパッケージをインポート"
      ],
      "metadata": {
        "id": "6dHqfmsc2f8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "d5Oklvxh5nAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 方策最適化とは\n",
        "\n",
        "まずは方策最適化に触れてみましょう．\n",
        "ここでは，Cart Pole環境における方策最適化を実際に行ってみます．\n",
        "\n",
        "この環境では，\n",
        "カート上のポールを垂直上向きに保ち続けることが目標となります．\n",
        "この目標を表現するために，角度が-12度から12度の間であれば報酬1が得られるようになっています．\n",
        "角度がこの範囲から外れたとき，そこで実行が終了されます（terminated フラグが Trueになる）．すなわち，報酬の総和がポールを保ち続けたステップ数，ということになります．\n",
        "\n",
        "取れる行動は，カートを右側にpushする（行動 1）か，左側にpushする（行動 0）かの二択になります．\n",
        "与えられる力は自動的に定まります．\n",
        "\n",
        "適切な行動を設計するには，現在の状態を観測する必要があります．\n",
        "観測は以下の4つの情報からなる実数値ベクトルです．\n",
        "\n",
        "1. cart position\n",
        "2. cart velocity\n",
        "3. pole angle\n",
        "4. pole angular velocity\n",
        "\n",
        "それぞれの範囲については，以下のURLを確認してください．\n",
        "\n",
        "https://gymnasium.farama.org/environments/classic_control/cart_pole/"
      ],
      "metadata": {
        "id": "CuXW05xrRAZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "観測状態に応じて行動を決定するルールが方策です．\n",
        "\n",
        "以下で定義する`rollout`関数は，与えられた方策を用いて，現在の状態を観測し，この観測に基づいて方策が行動選択し，この行動を実際に実行することで次の状態に遷移する，というプロセスを繰り返しています．\n",
        "\n",
        "`visualize`関数は，`rollout`の結果をアニメーションとして可視化する関数です．\n",
        "\n",
        "`cumulative_reward`関数は，`rollout`の結果から方策の望ましさを示す指標値を計算します．\n",
        "大きいほどよい方策ということになります．\n",
        "\n"
      ],
      "metadata": {
        "id": "Mf-snUeN4o2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout(envname, policy=None, render=False, seed=None):\n",
        "    if render:\n",
        "        env = gym.make(envname, render_mode=\"rgb_array\")\n",
        "    else:\n",
        "        env = gym.make(envname)\n",
        "    history = []\n",
        "    img = []\n",
        "\n",
        "    # 乱数の設定\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "    envseed = random.randint(0, 1000)\n",
        "    actseed = random.randint(0, 1000)\n",
        "    observation, info = env.reset(seed=envseed)\n",
        "    env.action_space.seed(actseed)\n",
        "\n",
        "    # 可視化用の設定\n",
        "    if render:\n",
        "        d = Display()\n",
        "        d.start()\n",
        "        img.append(env.render())\n",
        "\n",
        "    # メインループ（環境とのインタラクション）\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # 行動を選択\n",
        "        if policy is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy(observation)\n",
        "\n",
        "        # 行動を実行\n",
        "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "        history.append([observation, action, next_observation, reward, terminated, truncated, info])\n",
        "        observation = next_observation\n",
        "        if render:\n",
        "            display.clear_output(wait=True)\n",
        "            img.append(env.render())\n",
        "    env.close()\n",
        "    return history, img\n",
        "\n",
        "\n",
        "def visualize(img):\n",
        "    dpi = 72\n",
        "    interval = 50\n",
        "    plt.figure(figsize=(img[0].shape[1]/dpi, img[0].shape[0]/dpi), dpi=dpi)\n",
        "    patch = plt.imshow(img[0])\n",
        "    plt.axis=('off')\n",
        "    animate = lambda i: patch.set_data(img[i])\n",
        "    ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(img), interval=interval)\n",
        "    display.display(display.HTML(ani.to_jshtml()))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def cumulative_reward(history):\n",
        "    return sum(hist[3] for hist in history)"
      ],
      "metadata": {
        "id": "9ELUIKjb5vY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上のコードにおいて重要な箇所を解説します．\n",
        "\n",
        "observation, info = env.reset(seed=...) : 状態を初期化しています．observationが初期の状態，infoは環境固有の情報を含んでいます．初期状態はランダムにきまるため，そのシードがseed=..で指定されています．このシードが固定されていると，毎回同じ用に初期化されることになります．\n",
        "\n",
        "action = policy(observation) : 現在の状態の観測値 observation に従って，方策により行動 action を選択します．\n",
        "\n",
        "observation, reward, terminated, truncated, info = env.step(action) : 行動 action を実行し，次の状態の観測値 observation，即時報酬 reward，終了状態に達したことを表すフラグ terminated，最大ステップ数に達したことを表すフラグtruncated，その他の情報 infoを返します．"
      ],
      "metadata": {
        "id": "SO_SMPNZ4aFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ランダム方策を用いてインタラクションしてみる\n",
        "\n",
        "それではまず，取りうる行動の範囲からランダムに行動を選択するランダム方策を用いて，実際に環境とインタラクションしてみましょう．\n",
        "先程定義した`rollout`関数では，`policy`を引数に与えない場合，ランダム方策を用いるように定義されています．\n",
        "それでは以下を実行してみましょう．\n",
        "\n",
        "すぐに倒れてしまう様子が確認できると思います．\n",
        "ランダムに行動を選択しているということは，ポールの状態を考えずにサイコロで行動を決定しているということになりますから，当然ですね．\n",
        "\n"
      ],
      "metadata": {
        "id": "Dmvuo-Qb6ZTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"CartPole-v1\"\n",
        "history, img = rollout(envname, render=True)  # 可視化する場合には render = True\n",
        "visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "LkBKAn4H6nR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 方策を手作りしてみる\n",
        "\n",
        "ここでは，自分で方策を作成してみましょう．\n",
        "\n",
        "簡単に思いつく方法としては，\n",
        "\n",
        "- ポールが右側に倒れていれば（角度が正の値）ならば，右側にカートを押す（行動 1）\n",
        "- ポールが左側に倒れていれば（角度が負の値）ならば，左側にカートを押す（行動 0）\n",
        "\n",
        "かと思います．\n",
        "これをまずは試してみましょう．\n",
        "\n",
        "`rollout`では，方策は観測`observation`を受け取って行動を返すcallableであることを想定しています．\n",
        "ここでは関数として定義します．（後にクラスとして定義します．）"
      ],
      "metadata": {
        "id": "Qph8ZfUxaJbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_policy(observation):\n",
        "    # observation[2] が角度．負は左側に傾いている\n",
        "    if observation[2] < 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "8eeqcsifQhLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この方策に従って行動した場合，どの程度ポールが立ち続けていられるのか，確認してみましょう．\n",
        "最後にプリントされる`cumulative_reward`の値がここではポールを指定された角度に保ち続けていられたステップ数です．"
      ],
      "metadata": {
        "id": "qQrRUR8QcT-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"CartPole-v1\"\n",
        "history, img = rollout(envname, policy=manual_policy, render=True)  # 可視化する場合には render = True\n",
        "visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "W2gONerzSjC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ランダム行動の場合よりは少し長くポールを保てる用になっているかと思います．\n",
        "ただし，ポールの振れ幅がどんどん大きくなってしまっていることが観察されるはずです．\n",
        "これは，ポールの速度に関する情報や，カートの速度の情報を無視していることが原因と考えられます．\n",
        "\n",
        "そこで，ポールの速度も用いて行動選択をしてみましょう．\n",
        "ここでは，まずポールの角度を用いて条件分け，その後ポールの角速度を用いて条件分け，という手続きで行動選択することを考えましょう．\n",
        "上のケースと同じように，ポールが左に傾いていれば，左側に押すことが自然です．\n",
        "その際，角速度が負であるか，もしくは正の小さな値であれば，左側に押すことで，角速度を正側に大きくすることになるため，やはり左側に押すことが適切であるように思います．\n",
        "他方，ポールが左に傾いていても，角速度が正の大きな値であるならば，押さなくても自然にポールは垂直方向に変化します．\n",
        "逆に，ポールの角速度を小さくするほうが，安定して垂直に保てると考えられます．\n",
        "右側に傾いている場合にも，上と同様に考えられます．\n",
        "\n",
        "そこで，下に定義するような方策を考えてみましょう．\n",
        "\n",
        "1. $\\theta < 0$かつ$\\dot{\\theta} < U$ならば左に押す（行動0）\n",
        "2. $\\theta < 0$かつ$\\dot{\\theta} \\geq U$ならば右に押す（行動1）\n",
        "1. $\\theta \\geq 0$かつ$\\dot{\\theta} > L$ならば右に押す（行動1）\n",
        "2. $\\theta \\geq 0$かつ$\\dot{\\theta} \\leq L$ならば左に押す（行動0）\n",
        "\n",
        "ここで，$U$や$L$はパラメータです．\n",
        "上の議論から，$U > 0$，$L < 0$となることが自然です．\n",
        "なお，`manual_policy`は，$U = \\infty$，$L = -\\infty$であるケースに該当していることがわかるかと思います．\n",
        "\n",
        "以下のコードにおいて，$L$や$U$を変更し，長い間（最大500ステップ）ポールを垂直に保つように試行錯誤してみましょう．"
      ],
      "metadata": {
        "id": "RzlcF4x8dXIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def manual_policy2(observation):\n",
        "    U = np.infty\n",
        "    L = - np.infty\n",
        "    if observation[2] < 0:\n",
        "        if observation[3] < U:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    else:\n",
        "        if observation[3] > L:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0"
      ],
      "metadata": {
        "id": "WSVC8pypS3vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "試行錯誤をしている間は，動画は作成する必要ありません．\n",
        "`cumulative_reward` の返り値を最大（500）にするように，$L$と$U$を変更しましょう．\n",
        "動作を確認したいときは，コメントアウトを外して動画を作成しましょう．"
      ],
      "metadata": {
        "id": "cx8iid2UehZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"CartPole-v1\"\n",
        "history, img = rollout(envname, policy=manual_policy2, render=True)  # 可視化する場合には render = True\n",
        "# visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "-o1DkpTxk7-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下，良い方策を示しているのでわざと空白を入れています．\n",
        "自分で試行錯誤してから先に進みましょう．"
      ],
      "metadata": {
        "id": "IimZMOEQ7l48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ],
      "metadata": {
        "id": "BU_rKILd7lz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L = -0.1$，$U = 0.1$などと設定すると，500ステップずっとポールを保ち続けることができます．"
      ],
      "metadata": {
        "id": "UWm2re-FoPyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def manual_policy3(observation):\n",
        "    U = 0.1\n",
        "    L = - 0.1\n",
        "    if observation[2] < 0:\n",
        "        if observation[3] < U:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    else:\n",
        "        if observation[3] > L:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0"
      ],
      "metadata": {
        "id": "dqosQatbn8WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"CartPole-v1\"\n",
        "history, img = rollout(envname, policy=manual_policy3, render=True)  # 可視化する場合には render = True\n",
        "visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "j3xm0T6J733K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "さて，今実際に行ってみたように，$L$や$U$を変えるとそれによって方策が定まり，その結果得られる累積報酬が変化しました．\n",
        "このように，累積報酬を最大にするような方策を見つけ出すことこそが方策最適化です．\n",
        "$L$や$U$は方策を表現するためのパラメータです．\n",
        "基本的には，方策は何らかの方法（ここでは決定木と呼ばれるもの）で表現され，それはパラメータを持つことになります．\n",
        "方策最適化は，このパラメータを最適化することで，得られる累積報酬を最大化します．"
      ],
      "metadata": {
        "id": "7u2VpxmWoygC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ブラックボックス最適化アルゴリズムを用いた方策の直接探索\n",
        "上に見た CartPole 環境では行動が $\\{0, 1\\}$ の離散値であり，少し考えればうまく動く単純な方策を人が容易に設計できました．\n",
        "一般的にはそう簡単には良い方策を人手で設計することはできません．\n",
        "例えば，行動が連続値である場合には，そう簡単にうまく行かないかもしれません．\n",
        "その場合，方策最適化問題を，最適化アルゴリズムを用いて実現する方法が考えられます．\n",
        "ここではその一例を見てみましょう．"
      ],
      "metadata": {
        "id": "-QkEckTnmwEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用環境１：Pendulum（連続状態，連続行動）\n",
        "ここでは，Pendulum環境を用いて方策最適化問題の例を見ていきます．\n",
        "環境については，以下のURLで説明されています．\n",
        "\n",
        "https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "\n",
        "ランダムに初期化されたpendulumの角度を，垂直上向きに振り上げて保ち続けることが目標となります．\n",
        "観測状態は角度$\\theta$に対して，\n",
        "\n",
        "1. $\\cos(\\theta) \\in [-1, 1]$\n",
        "2. $\\sin(\\theta) \\in [-1, 1]$\n",
        "3. $\\dot\\theta \\in [-8, 8]$\n",
        "\n",
        "となっており，行動はpendulumに対するトルクです．\n",
        "以下，ランダム行動を取る場合の振る舞いです．\n",
        "報酬は負の値であり，各ステップ$[-16.2736044, 0]$の報酬が与えられます．\n",
        "200ステップ分の累積が大きくなる，すなわち$0$に近いほど，良い方策であると言えます．"
      ],
      "metadata": {
        "id": "_Uxb7ZsEowis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "history, img = rollout(envname, render=True)  # 可視化する場合には render = True\n",
        "visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "aq79Leejp4Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ランダム行動だと-1000を下回る程度の累積報酬になってしまっていることがわかるかと思います．"
      ],
      "metadata": {
        "id": "FgujJWGKrebN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 線形モデルによる方策の表現\n",
        "\n",
        "それでは方策を最適化することを考えていきましょう．\n",
        "まず，最適化対象となる方策をモデル化する必要があります．\n",
        "ここでは簡単のため，線形モデルを考えましょう．\n",
        "\n",
        "観測 `observation` は三次元のベクトルであり，値は$[-1, 1] \\times [-1, 1] \\times [-8, 8]$です．\n",
        "簡単のため，これをベクトル$\\mathbf{x} = (x_1, x_2, x_3)$ と書きます．\n",
        "行動$a$は一次元の値であるため，これを\n",
        "$$\n",
        "a = \\mathbf{w}^\\mathrm{T} \\mathbf{x} = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3\n",
        "$$\n",
        "と決定する方策を考えましょう．\n",
        "このとき，最適化対象は四次元ベクトル$\\mathbf{w} = (w_0, w_1, w_2, w_3)$です．\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hyFYKaojro_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearPolicy:\n",
        "    def __init__(self, weights):\n",
        "        self.weights = weights\n",
        "    def __call__(self, observation):\n",
        "        return np.array([self.weights[0] + np.dot(observation, self.weights[1:4])])"
      ],
      "metadata": {
        "id": "aQfoMMNcpOr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 目的関数の定義\n",
        "\n",
        "最適化問題としてアプローチするために，目的関数を定義します．\n",
        "ここでの目的は累積報酬を最大化することです．\n",
        "そのため，目的関数$J(\\mathbf{w})$を，パラメータ$\\mathbf{w}$の線形方策を用いて得られる累積報酬としましょう．\n",
        "\n",
        "この目的関数を$\\mathbf{w}$の関数として書き下すことは，状態遷移確率や報酬関数を知らない限りできません．\n",
        "そこで，与えられたパラメータ$\\mathbf{w}$を用いた線形方策を用い，実際に環境とのインタラクションを通して目的関数の値を計算します．\n",
        "\n",
        "以下で，この目的関数の値を計算する関数を定義しています．\n",
        "本来，方策最適化の目的は，任意の初期状態から得られる累積報酬を最大にするような方策を獲得することですが，ここでは簡単のため，初期状態は固定（環境を特定のシードで初期化）することを考えます．\n",
        "中身は，これまでに実施してきたメインループそのものとなっています．\n",
        "また，最適化アルゴリズムは多くの場合，最小化問題を想定しているので，目的関数を$-1$倍することにします．"
      ],
      "metadata": {
        "id": "fhEQ43sCuSLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "dim_state = 3\n",
        "dim_action = 1\n",
        "dim_x = dim_state + 1\n",
        "\n",
        "\n",
        "def objective(x):\n",
        "    policy = LinearPolicy(x)\n",
        "    history, img = rollout(envname, policy, seed=100)\n",
        "    return -cumulative_reward(history)"
      ],
      "metadata": {
        "id": "_XUuwHIUt5Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "適当な値を入れてみましょう．\n",
        "シードを固定しているので，同じパラメータを入力すれば，何度計算しても同じ値が帰ってくるはずです．"
      ],
      "metadata": {
        "id": "LkZyDlqVwhLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "objective(np.array([0, 0, 0, 0]))"
      ],
      "metadata": {
        "id": "95-1TgdYwXbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1+1)-ES による最適化\n",
        "\n",
        "ブラックボックス最適化アプローチである，(1+1)-ESを用いて最適化してみましょう．\n",
        "多くの数値最適化法では，目的関数の勾配を利用します．\n",
        "しかし，今回の問題では，目的関数の勾配を得ることが自明でないと容易にわかるかと思います．\n",
        "(1+1)-ESは，勾配を使わず，目的関数の大小関係のみを用いて探索する方法であるため，非常に適用範囲の広いアプローチです．"
      ],
      "metadata": {
        "id": "5mRu5XmRxpQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ES:\n",
        "    def __init__(self, init_s, init_x, init_fx):\n",
        "        self.fx = init_fx\n",
        "        self.x = np.array(init_x, copy=True)\n",
        "        self.s = init_s\n",
        "        self.dim = len(self.x)\n",
        "        self.alpha = np.exp(1.0 / self.dim)\n",
        "        self.y = np.empty(self.dim)\n",
        "\n",
        "    def ask(self):\n",
        "        # N(x, s^2 * I) により，新しい解の候補を生成\n",
        "        self.y = self.x + self.s * np.random.randn(self.dim)\n",
        "        return self.y\n",
        "\n",
        "    def tell(self, fx):\n",
        "        # x と s の更新\n",
        "        if self.fx < fx:\n",
        "            # 生成された解が x よりも望ましく無い場合， s を小さくする（探索の幅を狭める）\n",
        "            self.s /= self.alpha**(0.25)\n",
        "        else:\n",
        "            # 生成された解が x よりも望ましい場合， s を大きくする（探索の幅を広める）とともに，xを生成された解で上書き\n",
        "            self.s *= self.alpha\n",
        "            self.x = self.y\n",
        "            self.fx = fx"
      ],
      "metadata": {
        "id": "CD59AEa8w9cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 初期分布の設計\n",
        "sigma = 4.0\n",
        "mean = np.ones(4)\n",
        "\n",
        "# 履歴\n",
        "f_hist = np.empty(1000)\n",
        "f_hist[0] = objective(mean)\n",
        "\n",
        "# 実行\n",
        "es = ES(sigma, mean, f_hist[0])\n",
        "for t in range(1, len(f_hist)):\n",
        "    w = es.ask()  # 新しい解を生成\n",
        "    f_hist[t] = objective(w)  # 新しい解を評価\n",
        "    es.tell(f_hist[t])  # 分布を更新"
      ],
      "metadata": {
        "id": "ge2AVPWvyN6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "探索過程で得られた解の評価値をプロットします．"
      ],
      "metadata": {
        "id": "jQa-LikRoWAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(f_hist)"
      ],
      "metadata": {
        "id": "Sr_ndKyDurVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最終的に得られた解（線形方策のパラメータ）を見てみましょう．"
      ],
      "metadata": {
        "id": "9AMMz-BQoecg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = es.x\n",
        "print(weights)"
      ],
      "metadata": {
        "id": "UtCRBLP0tpQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に振る舞いを可視化してみます．"
      ],
      "metadata": {
        "id": "CXEi0qjEonvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "policy = LinearPolicy(weights)\n",
        "history, img = rollout(envname, policy=policy, render=True, seed=100)  # 可視化する場合には render = True\n",
        "visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "eOJjVOOForYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1+1)-ESは乱数を用いた最適化法ですので，試行毎に得られる解が変わります．\n",
        "うまくいくと-100くらいの累積報酬が得られることが確認できます．"
      ],
      "metadata": {
        "id": "J360Ei16cgU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 汎化性能の評価\n",
        "\n",
        "さて，上の最適化の際には，pendulumの初期状態を固定して方策パラメータを最適化しました．\n",
        "実際には，他の初期状態から始めたとしても良い方策となって欲しいので，初期値がランダムに決まるケースでの累積報酬の期待値を計算してみましょう．\n",
        "期待値を計算するといっても，解析的に計算できるわけではないので，複数回ランダムな初期値から実行し，その平均を計算してみます．\n",
        "\n",
        "以下では，与えた回数だけ累積報酬を計算（エピソードを回し）し，その結果（配列）を計算しています．"
      ],
      "metadata": {
        "id": "-BALV-GzEmEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "policy = LinearPolicy(weights)\n",
        "seed_array = np.arange(100, 2100, 100)\n",
        "return_array = np.zeros(len(seed_array))\n",
        "for i, seed in enumerate(seed_array):\n",
        "    history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "    return_array[i] = cumulative_reward(history)"
      ],
      "metadata": {
        "id": "y_N3_aATvLmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "経験分布関数をプロットしてみましょう．\n",
        "ここで表示しているものは，横軸（累積報酬 x -1の値 = 目的関数の値）より小さな目的関数値が得られた割合です．\n",
        "例えば，縦軸が0.5のときの横軸の値が，得られた目的関数値の中央値となります．"
      ],
      "metadata": {
        "id": "kXyM2vi6getv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.ecdfplot(data=-return_array)"
      ],
      "metadata": {
        "id": "_4KwCTy7gdMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この結果を見ると，特定の初期値からの方策を最適化しただけでは，他の初期値から始めた場合に最適な行動が取れていないことがわかります．\n",
        "目的関数が特定の初期値からの方策の性能を評価しているため，他の初期値から始めた場合の性能を考慮していないことが一因です．"
      ],
      "metadata": {
        "id": "LUWQA_M2a5GU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 目的関数の再検討\n",
        "\n",
        "最適化法は目的関数の値を良くすることだけを目標に最適なパラメータを求めていくため，使用者が望ましいと考える方策の性質を実現するためには，そのような性質をもつ方策に対してインセンティブを与えるような目的関数が設計されていることが必要です．\n",
        "今回の場合，様々な初期値から始めたとしても高速にpendulumを振り上げて安定化させることが目的ですから，これを評価するような目的関数を設計する必要があります．"
      ],
      "metadata": {
        "id": "pMTYtrzMfYgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 中央値の最適化\n",
        "そこで，目的関数を，初期値を変えて20回方策を評価した際の中央値，としてみましょう．\n",
        "このようにすることで，半分くらいは良い報酬が得られるような方策を獲得できると期待されます．\n",
        "なお，目的関数の内部で計算するエピソード数が増えるほど，計算時間が増えていきますので，注意してください．"
      ],
      "metadata": {
        "id": "t6RSnrDKiyj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def objective2(weights):\n",
        "    envname = \"Pendulum-v1\"\n",
        "    policy = LinearPolicy(weights)\n",
        "    seed_array = np.arange(100, 2100, 100)\n",
        "    return_array = np.zeros(len(seed_array))\n",
        "    for i, seed in enumerate(seed_array):\n",
        "        history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "        return_array[i] = cumulative_reward(history)\n",
        "    return np.percentile(- return_array, 50)\n",
        "\n",
        "sigma = 4.0\n",
        "mean = np.ones(4)\n",
        "f_hist = np.empty(500)\n",
        "f_hist[0] = objective2(mean)\n",
        "es2 = ES(sigma, mean, f_hist[0])\n",
        "for t in range(1, len(f_hist)):\n",
        "    w = es2.ask()\n",
        "    f_hist[t] = objective2(w)\n",
        "    es2.tell(f_hist[t])"
      ],
      "metadata": {
        "id": "cYeXDLXsGT6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(f_hist)"
      ],
      "metadata": {
        "id": "fX2M0-4CHurN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = es2.x\n",
        "print(weights)"
      ],
      "metadata": {
        "id": "p9VjZZt6H3oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "policy = LinearPolicy(weights)\n",
        "seed_array = np.arange(100, 2100, 100)\n",
        "return_array = np.zeros(len(seed_array))\n",
        "for i, seed in enumerate(seed_array):\n",
        "    history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "    return_array[i] = cumulative_reward(history)"
      ],
      "metadata": {
        "id": "qxeVFAb0r6AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(0, 1800)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "T7M6w_T2saxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "こちらも，試行によって結果は変わるかと思いますが，うまく行くと報酬の中央値が-100付近の値ととるような方策が獲得できます．\n",
        "すなわち，2回に1回は非常にうまくpendulumを垂直上向きに保つことができるような方策が得られるということです．\n",
        "しかし，たとえば75パーセンタイル（縦軸が0.75となるときの目的関数値）は最適化の対象となっていないため，あまり低い値にならないことが確認できると思います．\n",
        "このように，最適化法は，与えられた目的関数が計算する値については最適化するものの，そこに反映されていない人が暗黙に期待するような振る舞いを獲得してくれるわけではありません．\n",
        "\n",
        "以下で振る舞いを確認してみましょう．\n",
        "環境を初期化する際に乱数のシードを指定しなければ，毎回異なる初期値に初期化されます．\n",
        "以下を何度か繰り返し，うまくいく試行，いかない試行の振る舞いを見てみましょう．"
      ],
      "metadata": {
        "id": "-QXQ3BVbdumc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "policy = LinearPolicy(weights)\n",
        "history, img = rollout(envname, policy=policy, render=True)\n",
        "cumulative_reward(history)\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "Rz3teyDRjLqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 第三四分位数の最適化\n",
        "\n",
        "中央値の最適化では，全試行のうちうまくいく半分の試行での性能を最大化することが目的となっています．\n",
        "そのため，半分よりしたの試行においての性能は著しく悪化することがあります．\n",
        "もう少し高い割合で高い性能を示すような方策がほしいと思えば，目的関数を75パーセンタイルなどに変更することも可能です．\n",
        "これを試してみましょう．"
      ],
      "metadata": {
        "id": "FcsUFsEOi6uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective3(weights):\n",
        "    envname = \"Pendulum-v1\"\n",
        "    policy = LinearPolicy(weights)\n",
        "    seed_array = np.arange(100, 2100, 100)\n",
        "    return_array = np.zeros(len(seed_array))\n",
        "    for i, seed in enumerate(seed_array):\n",
        "        history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "        return_array[i] = cumulative_reward(history)\n",
        "    return np.percentile(- return_array, 75)\n",
        "\n",
        "sigma = 4.0\n",
        "mean = np.ones(4)\n",
        "f_hist = np.empty(500)\n",
        "f_hist[0] = objective3(mean)\n",
        "es3 = ES(sigma, mean, f_hist[0])\n",
        "for t in range(1, len(f_hist)):\n",
        "    w = es3.ask()\n",
        "    f_hist[t] = objective3(w)\n",
        "    es3.tell(f_hist[t])"
      ],
      "metadata": {
        "id": "oF6YcebaInEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(f_hist)"
      ],
      "metadata": {
        "id": "c8mEX2UphQb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = es3.x\n",
        "envname = \"Pendulum-v1\"\n",
        "policy = LinearPolicy(weights)\n",
        "seed_array = np.arange(100, 2100, 100)\n",
        "return_array = np.zeros(len(seed_array))\n",
        "for i, seed in enumerate(seed_array):\n",
        "    history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "    return_array[i] = cumulative_reward(history)"
      ],
      "metadata": {
        "id": "4fvz621hfXMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(0, 1800)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "7rLCd38OlU_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "目的関数を 75パーセンタイルに変更した場合，75パーセンタイルの値そのものは50パーセンタイルを目的関数とした場合よりも良くなっているかもしれませんが，あまり望ましい目的関数値とはなっていないことが観測されるのではないかと思います．\n",
        "一つの原因として，方策の表現能力が低いため，色々な状況で望ましい振る舞いを取るような方策を表現できないことが考えられます．"
      ],
      "metadata": {
        "id": "yj9BPnJfimG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 方策表現の再検討\n",
        "\n",
        "線形方策は単純なため，理解しやすいですが，表現能力が不足していました．\n",
        "そこで，これを一般化したニューラルネットワークによる方策の表現方法を考えましょう．\n",
        "ここでは一例として，`softmax`関数を用いた２層のニューラルネットワークを考えることにします．"
      ],
      "metadata": {
        "id": "Eff_tLTBiAph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax関数を用いたニューラルネットワークによる方策表現\n",
        "ここでは，行動を以下のように決定するニューラルネットワークを考えます．\n",
        "$$\n",
        "a = \\mathbf{V} \\cdot \\mathrm{softmax}\\left( \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}\\right)\n",
        "$$\n",
        "一般的な説明をするために，状態の次元数を$N_s$（ここでは$N_s = 3$），行動に次元数を$N_a$（ここでは$N_a = 1$）と書くことにします．\n",
        "最適化対象のパラメータは$\\mathbf{b} \\in R^K$，$\\mathbf{W} \\in R^{K \\times N_s}$，$\\mathbf{V} \\in R^{N_a \\times K}$の３つであり，合計$K (1 + N_s + N_a)$個のパラメータからなります．\n",
        "\n",
        "この方策の意味を少し考えておきましょう．\n",
        "$\\mathrm{softmax}$は入力ベクトル$\\mathbf{y} = [y_1, \\dots, y_n]$に対して\n",
        "$$\n",
        "\\mathrm{softmax}(\\mathbf{y}) = \\frac{[\\exp(y_1), \\dots, \\exp(y_n)]}{\\sum_{i=1}^{n} \\exp(y_i)}\n",
        "$$\n",
        "を計算する関数です．\n",
        "出力は必ず正で和が$1$になるようなベクトルです．\n",
        "名前の通り，$\\max$を近似するような関数です．\n",
        "上の方策の式において，$\\mathrm{softmax}$の出力を$\\mathbf{z}$などと書けば，行動は$\\mathbf{V} = [\\mathbf{v}_1 \\dots \\mathbf{v}_K]$ の列ベクトル$\\mathbf{v}_k$の線形補間\n",
        "$$\n",
        "\\mathbf{a} = \\mathbf{V} \\cdot \\mathbf{z} = \\sum_{k=1}^{K} z_k \\mathbf{v}_k\n",
        "$$\n",
        "で決まります．\n",
        "$\\mathrm{softmax}$関数が$\\max$の近似であることを考慮すれば，\n",
        "\n",
        "* $\\mathbf{V} = [\\mathbf{v}_1 \\dots \\mathbf{v}_K]$ が取る行動の候補$K$個を定める部分\n",
        "\n",
        "* $ \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}$ がどの行動を選択するかを決める部分\n",
        "\n",
        "であることがわかります．\n",
        "これは，状態空間をボロノイ分割し（分割方法は$\\mathbf{W}$と$\\mathbf{b}$を最適化することで変化する），各領域で選択する行動が$\\mathbf{v}_k$で定まる，と解釈されます．\n",
        "実際には，$\\max$ではなく$\\mathrm{softmax}$であるため，その間の区間を線形補間していることになります．\n",
        "\n"
      ],
      "metadata": {
        "id": "WYAIceX1xKK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "class NNPolicy:\n",
        "    def __init__(self, weights):\n",
        "        Na = 1\n",
        "        Ns = 3\n",
        "        K = 5\n",
        "        self.B = weights[:K]\n",
        "        self.W = weights[K:K*Ns+K].reshape((K, Ns))\n",
        "        self.V = weights[K*Ns+K:K*Ns+K+K*Na].reshape((Na, K))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        z = np.dot(observation, self.W.T)\n",
        "        z += self.B\n",
        "        z = softmax(z)\n",
        "        z = np.dot(z, self.V.T)\n",
        "        return z"
      ],
      "metadata": {
        "id": "vSEv5FGrhd9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それではこの方策を用いて，先程は良い方策が獲得できなかった 75 パーセンタイルについて，最適化してみましょう．\n",
        "最適化対象のパラメータ数が25個に増えたので，最適化のためには先程よりも少し長いイテレーションが必要となります．"
      ],
      "metadata": {
        "id": "oyGYy44cQEiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = (1 + 3 + 1) * 5\n",
        "\n",
        "def objective4(weights):\n",
        "    envname = \"Pendulum-v1\"\n",
        "    policy = NNPolicy(weights)\n",
        "    seed_array = np.arange(100, 2100, 100)\n",
        "    return_array = np.zeros(len(seed_array))\n",
        "    for i, seed in enumerate(seed_array):\n",
        "        history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "        return_array[i] = cumulative_reward(history)\n",
        "    return np.percentile(- return_array, 75)\n",
        "\n",
        "sigma = 4.0\n",
        "mean = np.ones(N)\n",
        "f_hist = np.empty(1000)\n",
        "f_hist[0] = objective4(mean)\n",
        "es4 = ES(sigma, mean, f_hist[0])\n",
        "for t in range(1, len(f_hist)):\n",
        "    w = es4.ask()\n",
        "    f_hist[t] = objective4(w)\n",
        "    es4.tell(f_hist[t])\n",
        "    if t % 10 == 0:\n",
        "        print(t, np.min(f_hist[:t]))"
      ],
      "metadata": {
        "id": "uY2NQYEOohYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(f_hist)"
      ],
      "metadata": {
        "id": "aFOCO5rLo_vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = es4.x\n",
        "envname = \"Pendulum-v1\"\n",
        "policy = LinearPolicy(weights)\n",
        "seed_array = np.arange(100, 2100, 100)\n",
        "return_array = np.zeros(len(seed_array))\n",
        "for i, seed in enumerate(seed_array):\n",
        "    history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "    return_array[i] = cumulative_reward(history)"
      ],
      "metadata": {
        "id": "yAeGW2vTq-93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(0, 1800)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "NGIOIDALrKyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "線形方策を用いた場合と比較して，75パーセンタイルでの目的関数値は改善しているのではないかと思います．\n",
        "実際に何度か下のコードを実行して確認してみましょう．"
      ],
      "metadata": {
        "id": "mjxO6kXZx3FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "policy = LinearPolicy(weights)\n",
        "history, img = rollout(envname, policy=policy, render=True)\n",
        "cumulative_reward(history)\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "HYxfSoAAmq-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用環境２：Lunar Lander（連続状態，連続行動）\n",
        "\n",
        "もう一つ，別の環境での例を見てみましょう．\n",
        "次に扱うのは Lunar Lander 環境です．\n",
        "\n",
        "https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "\n",
        "この環境では，エージェントが$8$次元のセンサー情報（連続値）を状態として観測し，$4$つの行動から一つを選択します．\n",
        "その行動は\n",
        "\n",
        "0. 何もしない\n",
        "1. 左向きのエンジンを蒸す\n",
        "2. メインエンジンを蒸す\n",
        "3. 右向きのエンジンを蒸す\n",
        "\n",
        "というものです．\n",
        "すなわち，先程とは異なり，行動が離散値（カテゴリカル）を取ります．\n",
        "\n",
        "まずは以下を実行し，ランダム行動を取る場合の振る舞いを確認してみましょう．"
      ],
      "metadata": {
        "id": "obiyI0sDsJdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(\"LunarLander-v2\", render=True)\n",
        "cumulative_reward(history)\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "roLARKHNsSnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "行動が連続か離散かという違いの他に，もう一点，重要な違いが PendulumとLunar Landerの間に存在します．\n",
        "\n",
        "* Pendulum 環境やCart-Pole環境では，予め定められた回数（200回, 500回）のインタラクションを実行すると， フラグ truncated が Trueとなり，状態が初期化されます．\n",
        "\n",
        "* Lunar Lander環境では，着陸に成功 or 着地失敗など，特定の状態になると，成功 or 失敗という認識になり，フラグterminated がTrueとなり，状態が初期化されます．\n",
        "\n",
        "前者の場合，例えば目的関数を計算する都合上，無限に長いインタラクション数を取ることはできないため，強制的に終了させているとみなせます．\n",
        "他方，後者の場合には，明確に終了となる状態が定義されており，「一度の試行」という概念が定義されているようなタスクであると言えます．\n",
        "後者のようなタスクを「エピソディックタスク」と呼びます．\n",
        "この二つの違いは，強化学習を行う上では重要な違い（使用できる方法に違いが生じる）であるため，認識しておきましょう．\n",
        "ただ，ブラックボックス最適化として考える場合には，大きな差はありません．\n"
      ],
      "metadata": {
        "id": "Fh5I9rvGt2cI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pJwS_-piCN1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax方策\n",
        "\n",
        "行動が離散ですので，先程とは異なる方策が必要になります．\n",
        "行動が4種類であるということは，状態空間を4分割し，各分割では一つの行動を選択るという方法が考えられます．\n",
        "これは，教師あり学習で言うところの分類タスクに該当します．\n",
        "そこで，分類タスクによく使われるSoftmaxを用いた方策を利用しましょう．\n",
        "これは，$4$次元出力の線形モデルに対し，softmax関数を適用し，その結果として得られるベクトルを確率ベクトルとして行動を選択する，という確率的方策になります．\n",
        "\n",
        "ここでも違いが出てきたので紹介しておきます．\n",
        "Pendulum環境で用いていた方策はいずれも状態が与えられると行動が一意に定まる「決定的方策」でした．\n",
        "他方，ここで用いている方策は，状態が与えられるとそれによって得られる条件付き確率に従って行動を選択する「確率的方策」になります．\n",
        "強化学習の場合，良い行動を探索するために確率的方策が必要になる場合があります．\n",
        "一方，ブラックボックス最適化として扱う場合には，探索は探索法が担っているので，確率的方策を使用することは必要ではありません．\n",
        "ここでは，確率的方策の紹介と，分類タスクとの関連を紹介するために，利用しています．\n",
        "\n",
        "Softmax方策において各行動を取る確率は以下のように定義されます．\n",
        "$$\n",
        "\\pi(a=k \\mid s) = \\frac{ \\exp( \\mathbf{w}_k^\\mathrm{T} s + b_k) }{ \\sum_{i=1}^{4} \\exp( \\mathbf{w}_i^\\mathrm{T} s + b_i) }\n",
        "$$\n",
        "ここで，$\\mathbf{w}_1 = \\mathbf{0}$と$b_1 = 0$はいずれも固定とします．\n",
        "そのようにしても，表現能力は損なわれません．\n",
        "逆に，全てのパラメータを変動できるようにした場合，方策がオーバーパラメタライズされた状況となり，「パラメータを変えても表現される方策が変わらず，その結果目的関数が変化しない」ような領域が無数に存在することになり，これが最適化を難しくすることがあります．\n",
        "結果として，この方策はパラメータ$\\theta = (\\mathbf{w}_2, \\mathbf{w}_3, \\mathbf{w}_4, b_2, b_3, b_4)$で表現されることになります．これが最適化の対象となります．\n"
      ],
      "metadata": {
        "id": "lwmppne4yO3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxPolicy:\n",
        "    def __init__(self, x):\n",
        "        num_action = 4\n",
        "        dim_state = 8\n",
        "        self.w = np.zeros((num_action, dim_state))\n",
        "        self.b = np.zeros(num_action)\n",
        "        self.b[1:] = x[:num_action-1]\n",
        "        self.w[1:, :] = x[num_action-1:].reshape((num_action - 1, dim_state))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        logit = np.dot(self.w, observation) + self.b\n",
        "        prob = softmax(logit)\n",
        "        return random.choices(list(range(len(prob))), prob)[0]"
      ],
      "metadata": {
        "id": "uN8rCyXWwYq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それでは，このSoftmax方策を(1+1)-ESを用いて最適化してみましょう．"
      ],
      "metadata": {
        "id": "UTAb-Oaxz98O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "dim_x = (1 + dim_state) * (num_action - 1)\n",
        "\n",
        "\n",
        "def objective(x):\n",
        "    policy = SoftmaxPolicy(x)\n",
        "    history, img = rollout(envname, policy, seed=100)\n",
        "    return -cumulative_reward(history)\n",
        "\n",
        "\n",
        "sigma = 1.0\n",
        "mean = np.zeros(dim_x)\n",
        "f_hist = np.empty(2000)\n",
        "f_hist[0] = objective(mean)\n",
        "es = ES(sigma, mean, f_hist[0])\n",
        "for t in range(1, len(f_hist)):\n",
        "    w = es.ask()\n",
        "    f_hist[t] = objective(w)\n",
        "    es.tell(f_hist[t])"
      ],
      "metadata": {
        "id": "W1TDCtezwYoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = es.x\n",
        "policy = SoftmaxPolicy(weights)\n",
        "seed_array = np.arange(100, 2100, 100)\n",
        "return_array = np.zeros(len(seed_array))\n",
        "for i, seed in enumerate(seed_array):\n",
        "    history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "    return_array[i] = cumulative_reward(history)"
      ],
      "metadata": {
        "id": "t5lbU2UxwYk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "2ZCOefuDwYbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=policy, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "I40cpOgyxivT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "中央値を最適化する場合についても載せておきます．"
      ],
      "metadata": {
        "id": "XgvamJXQ0JOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "dim_x = (1 + dim_state) * (num_action - 1)\n",
        "\n",
        "\n",
        "def objective(weights):\n",
        "    policy = SoftmaxPolicy(weights)\n",
        "    seed_array = np.arange(100, 2100, 100)\n",
        "    return_array = np.zeros(len(seed_array))\n",
        "    for i, seed in enumerate(seed_array):\n",
        "        history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "        return_array[i] = cumulative_reward(history)\n",
        "    return np.percentile(- return_array, 50)\n",
        "\n",
        "\n",
        "sigma = 1.0\n",
        "mean = np.ones(dim_x)\n",
        "f_hist = np.empty(2000)\n",
        "f_hist[0] = objective(mean)\n",
        "es = ES(sigma, mean, f_hist[0])\n",
        "for t in range(1, len(f_hist)):\n",
        "    w = es.ask()\n",
        "    f_hist[t] = objective(w)\n",
        "    es.tell(f_hist[t])\n",
        "    if t % 10 == 0:\n",
        "        print(t, np.min(f_hist[:t]))"
      ],
      "metadata": {
        "id": "GNs_soyb0Qh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = es.x\n",
        "policy = SoftmaxPolicy(weights)\n",
        "seed_array = np.arange(100, 2100, 100)\n",
        "return_array = np.zeros(len(seed_array))\n",
        "for i, seed in enumerate(seed_array):\n",
        "    history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "    return_array[i] = cumulative_reward(history)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "dY-Ak-Mm02JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=policy, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "P7RZXptQ0_n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## この章のまとめ\n",
        "\n",
        "ここでは方策最適化をブラックボックス最適化問題として捉え，(1+1)-ES を用いて実際に最適化してみました．\n",
        "目的関数の設計の仕方，方策の選び方，などによって，最終的に得られる方策に違いが見られる様子が確認できたと思います．\n",
        "これは，(1+1)-ESによらず，これから先にみる強化学習においても同様です．\n",
        "\n",
        "目的関数，もしくはそれを定義する際に用いられる報酬関数$r$は，実際に獲得させたい振る舞いを反映させるように設計されることが必要です．\n",
        "これは実は簡単なことではありません．\n",
        "また，数学的には目的が正しく定義されていても，最適化にとって都合が悪い，という場合もあります．\n",
        "そういった場合，必ずしも実際に最適化している指標が本来最適化したい指標と一致しない場合もあります．\n",
        "このあたりは実用上極めて大切な部分です．\n",
        "常によく検討するようにしましょう．\n",
        "\n",
        "ブラックボックス最適化法を用いて方策最適化を実現することの利点は，その汎用性です．\n",
        "例えば，目的関数を比較的自由に設計することが可能です．\n",
        "今回も中央値や第三四分位の最適化を行いましたが，これを強化学習で実現することは自明ではありません．\n",
        "また，方策についても自由に選ぶことができます．\n",
        "状態や行動が離散か連続かなどの違いも，方策を変更するだけで対応できます．\n",
        "タスクがエピソディックタスクかどうかについても，あまり気にする必要はありません．ただし，目的関数を計算する都合上，本質的には終了状態が存在しないようなタスク（Pendulumなど）であっても，必ずどこかでエピソードを打ち切る必要は出てきます．\n",
        "その点，一部の（非エピソディックなタスクに対応した）強化学習では，そのような人工的なエピソードの終了が必須ではありません．"
      ],
      "metadata": {
        "id": "DfZ7KTmRm617"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自習課題\n",
        "\n",
        "1. 目的関数を変更してみましょう．例えば，初期値分布に対して期待値を最適化するためにはどのように変更すれば良いでしょうか．\n",
        "変更した目的関数を最適化した結果得られる方策を用いて複数試行回したとき，得られる報酬の期待値が他の目的関数を最適化した結果得られる値よりも小さくなっているのか，確認してみましょう．\n",
        "\n",
        "2. 方策を変更してみましょう．例えば，Pendulumの場合，今回のニューラルネットワークを用いた方策表現では，表現能力がハイパーパラメータ$K$に依存します．これを大きくすればより柔軟な表現が可能になりますが，パラメータ数がこれに比例して大きくなります．\n",
        "Lunar Landerの場合，もっと表現能力の高い方策を使用してみましょう．\n",
        "\n",
        "3. 最適化法を変更してみましょう．ブラックボックス最適化法であれば，どのようなアプローチを用いても構いません．\n",
        "方法毎に向き不向きがあります．\n",
        "今回採用した(1+1)-ESは，決して方策最適化に向いているとは言い難いです．\n",
        "一つの理由として，(1+1)-ESは目的関数のノイズ（目的関数の値が決定的でない）に対して適さない方法であることが挙げられます．\n",
        "今回は，あくまで方策最適化というものを理解するために，単純な最適化法を採用したに過ぎません．\n",
        "例えばよく知られたベイズ最適化などは，少ない目的関数の評価回数を想定して設計されているため，多くの目的関数評価を繰り返すことが想定される方策最適化には不向きとなります．\n",
        "他方，強化学習では，クロスエントロピー法と呼ばれる方法が採用されることがあります．これは，別のレポジトリにて紹介しているCMA-ESに良く似た方法ですので，こちらを参考にすると良いと思います．\n",
        "https://github.com/akimotolab/CMAES_Tutorial\n"
      ],
      "metadata": {
        "id": "Y5gvKQlMJ7Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8gwRqGQCCm8X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}