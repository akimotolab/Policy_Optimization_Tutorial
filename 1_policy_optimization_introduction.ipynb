{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWcSrKuGpkKOsePGtUvhTJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akimotolab/Policy_Optimization_Tutorial/blob/main/1_policy_optimization_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 方策最適化イントロダクション\n",
        "\n",
        "この資料は，方策最適化という最適化問題を数値実験を通して理解してもらうことを目的としています．\n",
        "Gymnasiumと呼ばれるシミュレーション環境を用いて，実際に方策を人手で最適化したり，最適化アルゴリズムを用いて最適化する経験を通して，方策最適化を理解します．\n",
        "そのため，理論的な説明は大部分割愛してあります．\n",
        "理論的な部分に興味がある方は，以下の文献などを参考にしてください．\n",
        "\n",
        "Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction, 2nd ed., The MIT Press. (2020) http://incompleteideas.net/book/RLbook2020trimmed.pdf"
      ],
      "metadata": {
        "id": "uAiSyx-A9Y4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Colab上でGymnasiumを利用する準備\n",
        "\n",
        "以下では，必要なパッケージのインストールとインポート，仮想displayの設定を実行しています．\n",
        "手元の環境でGymnasiumを利用する場合には特に難しいことはありませんが，ノートブックで実行する場合には仮想的なdisplayを設定することが必要になるなど，少し事前準備が必要です．\n"
      ],
      "metadata": {
        "id": "qWe6P9xqH8be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXoBp0fgHyhe"
      },
      "outputs": [],
      "source": [
        "# 必要なパッケージのインストール\n",
        "!apt update\n",
        "!pip install swig\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "仮想ディスプレイとGPUの設定（GPUは今回は不要ですが，追々必要になると考えられますので，設定だけしておきます．）"
      ],
      "metadata": {
        "id": "Jtb4X5jsQqEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import torch\n",
        "\n",
        "# 仮想ディスプレイの設定\n",
        "_display = Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "# gpuが使用される場合の設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
      ],
      "metadata": {
        "id": "iJ1PYM90IHa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "必要なパッケージをインポート"
      ],
      "metadata": {
        "id": "6dHqfmsc2f8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "d5Oklvxh5nAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 方策最適化とは\n",
        "\n",
        "まずは方策最適化に触れてみましょう．\n",
        "ここでは，Cart Pole環境における方策最適化を実際に行ってみます．\n",
        "\n",
        "この環境では，\n",
        "カート上のポールを垂直上向きに保ち続けることが目標となります．\n",
        "この目標を表現するために，角度が-12度から12度の間であれば報酬1が得られるようになっています．\n",
        "角度がこの範囲から外れたとき，そこで実行が終了されます（terminated フラグが Trueになる）．すなわち，報酬の総和がポールを上向きに保ち続けたステップ数，ということになります．\n",
        "\n",
        "取れる行動は，カートを右側にpushする（行動 1）か，左側にpushする（行動 0）かの二択になります．\n",
        "与えられる力は自動的に定まります．\n",
        "\n",
        "適切な行動を設計するには，現在の状態を観測する必要があります．\n",
        "観測は以下の4つの情報からなる実数値ベクトルです．\n",
        "\n",
        "1. cart position\n",
        "2. cart velocity\n",
        "3. pole angle\n",
        "4. pole angular velocity\n",
        "\n",
        "それぞれの範囲については，以下のURLを確認してください．\n",
        "\n",
        "https://gymnasium.farama.org/environments/classic_control/cart_pole/"
      ],
      "metadata": {
        "id": "CuXW05xrRAZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "観測状態に応じて行動を決定するルールが方策です．\n",
        "\n",
        "以下で定義する`rollout`関数は，与えられた方策を用いて，現在の状態を観測し，この観測に基づいて方策が行動選択し，この行動を実際に実行することで次の状態に遷移する，というプロセスを繰り返しています．\n",
        "\n",
        "`visualize`関数は，`rollout`の結果をアニメーションとして可視化する関数です．\n",
        "\n",
        "`cumulative_reward`関数は，`rollout`の結果から方策の望ましさを示す指標値を計算します．\n",
        "大きいほどよい方策ということになります．\n",
        "\n"
      ],
      "metadata": {
        "id": "Mf-snUeN4o2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout(envname, policy=None, render=False, seed=None):\n",
        "    if render:\n",
        "        env = gym.make(envname, render_mode=\"rgb_array\")\n",
        "    else:\n",
        "        env = gym.make(envname)\n",
        "    history = []\n",
        "    img = []\n",
        "\n",
        "    # 乱数の設定\n",
        "    if seed is not None:\n",
        "        random.seed(int(seed))\n",
        "    envseed = random.randint(0, 1000)\n",
        "    actseed = random.randint(0, 1000)\n",
        "    observation, info = env.reset(seed=envseed)\n",
        "    env.action_space.seed(actseed)\n",
        "\n",
        "    # 可視化用の設定\n",
        "    if render:\n",
        "        d = Display()\n",
        "        d.start()\n",
        "        img.append(env.render())\n",
        "\n",
        "    # メインループ（環境とのインタラクション）\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # 行動を選択\n",
        "        if policy is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy(observation)\n",
        "\n",
        "        # 行動を実行\n",
        "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "        history.append([observation, action, next_observation, reward, terminated, truncated, info])\n",
        "        observation = next_observation\n",
        "        if render:\n",
        "            display.clear_output(wait=True)\n",
        "            img.append(env.render())\n",
        "    env.close()\n",
        "    return history, img\n",
        "\n",
        "\n",
        "def visualize(img):\n",
        "    dpi = 72\n",
        "    interval = 50\n",
        "    plt.figure(figsize=(img[0].shape[1]/dpi, img[0].shape[0]/dpi), dpi=dpi)\n",
        "    patch = plt.imshow(img[0])\n",
        "    plt.axis=('off')\n",
        "    animate = lambda i: patch.set_data(img[i])\n",
        "    ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(img), interval=interval)\n",
        "    display.display(display.HTML(ani.to_jshtml()))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def cumulative_reward(history):\n",
        "    return sum(hist[3] for hist in history)"
      ],
      "metadata": {
        "id": "9ELUIKjb5vY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上のコードにおいて重要な箇所を解説します．\n",
        "\n",
        "observation, info = env.reset(seed=...) : 状態を初期化しています．observationが初期の状態，infoは環境固有の情報を含んでいます．初期状態はランダムにきまるため，そのシードがseed=..で指定されています．このシードが固定されていると，毎回同じ様に初期化されることになります．\n",
        "\n",
        "action = policy(observation) : 現在の状態の観測値 observation に従って，方策により行動 action を選択します．\n",
        "\n",
        "observation, reward, terminated, truncated, info = env.step(action) : 行動 action を実行し，次の状態の観測値 observation，即時報酬 reward，終了状態に達したことを表すフラグ terminated，最大ステップ数に達したことを表すフラグtruncated，その他の情報 infoを返します．"
      ],
      "metadata": {
        "id": "SO_SMPNZ4aFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ランダム方策を用いてインタラクションしてみる\n",
        "\n",
        "それではまず，取りうる行動の範囲からランダムに行動を選択するランダム方策（観測した状態によらず，取り得る行動の値の範囲から一様ランダムな行動をとる方策）を用いて，実際に環境とインタラクションしてみましょう．\n",
        "先程定義した`rollout`関数では，`policy`を引数に与えない場合，ランダム方策を用いるように定義されています．\n",
        "それでは以下を実行してみましょう．\n",
        "\n",
        "すぐに倒れてしまう様子が確認できると思います．\n",
        "ランダムに行動を選択しているということは，ポールの状態を考えずにサイコロで行動を決定しているということになりますから，当然ですね．\n"
      ],
      "metadata": {
        "id": "Dmvuo-Qb6ZTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"CartPole-v1\"\n",
        "history, img = rollout(envname, render=True)  # 可視化する場合には render = True\n",
        "visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "LkBKAn4H6nR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 方策を手作りしてみる\n",
        "\n",
        "ここでは，自分で方策を作成してみましょう．\n",
        "\n",
        "簡単に思いつく方法としては，\n",
        "\n",
        "- ポールが右側に倒れていれば（角度が正の値）ならば，右側にカートを押す（行動 1）\n",
        "- ポールが左側に倒れていれば（角度が負の値）ならば，左側にカートを押す（行動 0）\n",
        "\n",
        "かと思います．\n",
        "これをまずは試してみましょう．\n",
        "\n",
        "`rollout`では，方策は観測`observation`を受け取って行動を返すcallableであることを想定しています．\n",
        "ここでは関数として方策を定義します．（後にクラスとして定義します．）"
      ],
      "metadata": {
        "id": "Qph8ZfUxaJbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_policy(observation):\n",
        "    # observation[2] が角度．負は左側に傾いている\n",
        "    if observation[2] < 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "8eeqcsifQhLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この方策に従って行動した場合，どの程度ポールが立ち続けていられるのか，確認してみましょう．\n",
        "最後にプリントされる`cumulative_reward`の値がここではポールを指定された角度に保ち続けていられたステップ数です．"
      ],
      "metadata": {
        "id": "qQrRUR8QcT-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"CartPole-v1\"\n",
        "history, img = rollout(envname, policy=manual_policy, render=True)  # 可視化する場合には render = True\n",
        "visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "W2gONerzSjC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ランダム行動の場合よりは少し長くポールを保てる用になっているかと思います．\n",
        "ただし，ポールの振れ幅がどんどん大きくなってしまっていることが観察されるはずです．\n",
        "これは，ポールの速度に関する情報や，カートの速度の情報を無視していることが原因と考えられます．\n",
        "\n",
        "そこで，ポールの速度も用いて行動選択をしてみましょう．\n",
        "ここでは，まずポールの角度を用いて条件分け，その後ポールの角速度を用いて条件分け，という手続きで行動選択することを考えましょう．\n",
        "上のケースと同じように，ポールが左に傾いていれば，左側に押すことが自然です．\n",
        "その際，角速度が負であるか，もしくは正の小さな値であれば，左側に押すことで，角速度を正側に大きくすることになるため，やはり左側に押すことが適切であるように思います．\n",
        "他方，ポールが左に傾いていても，角速度が正の大きな値であるならば，押さなくても自然にポールは垂直方向に変化します．\n",
        "逆に，ポールの角速度を小さくするほうが，安定して垂直に保てると考えられます．\n",
        "右側に傾いている場合にも，上と同様に考えられます．\n",
        "\n",
        "そこで，下に定義するような方策を考えてみましょう．\n",
        "\n",
        "1. $\\theta < 0$かつ$\\dot{\\theta} < U$ならば左に押す（行動0）\n",
        "2. $\\theta < 0$かつ$\\dot{\\theta} \\geq U$ならば右に押す（行動1）\n",
        "1. $\\theta \\geq 0$かつ$\\dot{\\theta} > L$ならば右に押す（行動1）\n",
        "2. $\\theta \\geq 0$かつ$\\dot{\\theta} \\leq L$ならば左に押す（行動0）\n",
        "\n",
        "ここで，$U$や$L$はパラメータです．\n",
        "上の議論から，$U > 0$，$L < 0$となることが自然です．\n",
        "なお，`manual_policy`は，$U = \\infty$，$L = -\\infty$であるケースに該当していることがわかるかと思います．\n",
        "\n",
        "以下のコードにおいて，$L$や$U$を変更し，長い間（最大500ステップ）ポールを垂直に保つように試行錯誤してみましょう．"
      ],
      "metadata": {
        "id": "RzlcF4x8dXIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def manual_policy2(observation):\n",
        "    U = np.infty\n",
        "    L = - np.infty\n",
        "    if observation[2] < 0:\n",
        "        if observation[3] < U:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    else:\n",
        "        if observation[3] > L:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0"
      ],
      "metadata": {
        "id": "WSVC8pypS3vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "試行錯誤をしている間は，動画は作成する必要ありません．\n",
        "`cumulative_reward` の返り値を最大（500）にするように，$L$と$U$を変更しましょう．\n",
        "動作を確認したいときは，コメントアウトを外して動画を作成しましょう．"
      ],
      "metadata": {
        "id": "cx8iid2UehZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"CartPole-v1\"\n",
        "history, img = rollout(envname, policy=manual_policy2, render=True)  # 可視化する場合には render = True\n",
        "# visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "-o1DkpTxk7-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下，良い方策を示しているのでわざと空白を入れています．\n",
        "自分で試行錯誤してから先に進みましょう．"
      ],
      "metadata": {
        "id": "IimZMOEQ7l48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ],
      "metadata": {
        "id": "BU_rKILd7lz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L = -0.1$，$U = 0.1$などと設定すると，500ステップずっとポールを保ち続けることができます．"
      ],
      "metadata": {
        "id": "UWm2re-FoPyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def manual_policy3(observation):\n",
        "    U = 0.1\n",
        "    L = - 0.1\n",
        "    if observation[2] < 0:\n",
        "        if observation[3] < U:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    else:\n",
        "        if observation[3] > L:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0"
      ],
      "metadata": {
        "id": "dqosQatbn8WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"CartPole-v1\"\n",
        "history, img = rollout(envname, policy=manual_policy3, render=True)  # 可視化する場合には render = True\n",
        "visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "j3xm0T6J733K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "さて，今実際に行ってみたように，$L$や$U$を変えるとそれによって方策が定まり，その結果得られる累積報酬が変化しました．\n",
        "このように，累積報酬を最大にするような方策を見つけ出すことこそが方策最適化です．\n",
        "$L$や$U$は方策を表現するためのパラメータです．\n",
        "基本的には，方策は何らかの方法（ここでは決定木と呼ばれるもの）で表現され，それはパラメータを持つことになります．\n",
        "方策最適化は，このパラメータを最適化することで，得られる累積報酬を最大化します．"
      ],
      "metadata": {
        "id": "7u2VpxmWoygC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ブラックボックス最適化アルゴリズムを用いた方策の直接探索\n",
        "上に見た CartPole 環境では行動が $\\{0, 1\\}$ の離散値であり，少し考えればうまく動く単純な方策を人が容易に設計できました．\n",
        "一般的にはそう簡単には良い方策を人手で設計することはできません．\n",
        "例えば，行動が連続値である場合には，そう簡単にうまく行かないかもしれません．\n",
        "その場合，方策最適化問題を，最適化アルゴリズムを用いて解く方法が考えられます．\n",
        "ここではその一例を見てみましょう．"
      ],
      "metadata": {
        "id": "-QkEckTnmwEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用環境１：Pendulum（連続状態，連続行動）\n",
        "ここでは，Pendulum環境を用いて方策最適化問題の例を見ていきます．\n",
        "環境については，以下のURLで説明されています．\n",
        "\n",
        "https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "\n",
        "ランダムに初期化されたpendulumの角度を，垂直上向きに振り上げて保ち続けることが目標となります．\n",
        "観測状態は角度$\\theta$に対して，\n",
        "\n",
        "1. $\\cos(\\theta) \\in [-1, 1]$\n",
        "2. $\\sin(\\theta) \\in [-1, 1]$\n",
        "3. $\\dot\\theta \\in [-8, 8]$\n",
        "\n",
        "となっており，行動はpendulumに対するトルクです．\n",
        "以下，ランダム行動を取る場合の振る舞いです．\n",
        "報酬は負の値であり，各ステップ$[-16.2736044, 0]$の報酬が与えられます．\n",
        "200ステップ分の累積が大きくなる，すなわち$0$に近いほど，良い方策であると言えます．"
      ],
      "metadata": {
        "id": "_Uxb7ZsEowis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "history, img = rollout(envname, render=True)  # 可視化する場合には render = True\n",
        "visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "aq79Leejp4Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ランダム行動だと-1000を下回る程度の累積報酬になってしまっていることがわかるかと思います．"
      ],
      "metadata": {
        "id": "FgujJWGKrebN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 線形モデルによる方策の表現　（連続行動，決定方策）\n",
        "\n",
        "それでは方策を最適化することを考えていきましょう．\n",
        "まず，最適化対象となる方策をモデル化する必要があります．\n",
        "ここでは簡単のため，線形モデルを考えましょう．\n",
        "\n",
        "観測 `observation` は三次元のベクトルであり，値は$[-1, 1] \\times [-1, 1] \\times [-8, 8]$です．\n",
        "簡単のため，これをベクトル$\\mathbf{x} = (x_1, x_2, x_3)$ と書きます．\n",
        "また，便宜上，観測の先頭に定数$1$を加えたベクトルを$\\bar{\\mathbf{x}} = [1, \\mathbf{x}^\\mathrm{T}]^\\mathrm{T}$と書くことにします．\n",
        "行動$a$は一次元の値であるため，これを\n",
        "$$\n",
        "a = 2 \\tanh( \\mathbf{w}^\\mathrm{T} \\bar{\\mathbf{x}}) = 2 \\tanh( w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 )\n",
        "$$\n",
        "と決定する方策を考えましょう．\n",
        "このとき，最適化対象は四次元ベクトル$\\mathbf{w} = (w_0, w_1, w_2, w_3)$です．\n",
        "行動の取り得る範囲が$[-2, 2]$なので，線形関数の出力をtanhで$[-1, 1]$に補正したのち，2倍しています．\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hyFYKaojro_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearPolicy:\n",
        "    def __init__(self, weights):\n",
        "        self.weights = weights\n",
        "    def __call__(self, observation):\n",
        "        return np.array([2 * np.tanh(self.weights[0] + np.dot(observation, self.weights[1:4]))])"
      ],
      "metadata": {
        "id": "aQfoMMNcpOr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは方策をクラスとして定義しています．__call__を定義しておくことで，このクラスのインスタンスをあたかも関数であるかのように呼び出すことができます．関数でなくクラスとして定義しておく理由は，パラメータであるweightsをあとから変更するためです．"
      ],
      "metadata": {
        "id": "MgK_yjRvJnFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 目的関数の定義\n",
        "\n",
        "最適化問題としてアプローチするために，目的関数を定義します．\n",
        "ここでの目的は累積報酬を最大化することです．\n",
        "そのため，目的関数$J(\\mathbf{w})$を，パラメータ$\\mathbf{w}$の線形方策を用いて得られる累積報酬としましょう．\n",
        "\n",
        "この目的関数を$\\mathbf{w}$の関数として書き下すことは，状態遷移確率や報酬関数を知らない限りできません．\n",
        "そこで，与えられたパラメータ$\\mathbf{w}$の線形方策を用い，実際に環境とのインタラクションを通して目的関数の値を計算します．\n",
        "\n",
        "以下で，この目的関数の値を計算する関数を定義しています．\n",
        "本来，方策最適化の目的は，任意の初期状態から得られる累積報酬を最大にするような方策を獲得することですが，ここでは簡単のため，初期状態は固定（環境を特定のシードで初期化）することを考えます．\n",
        "中身は，これまでに実施してきたメインループそのものとなっています．\n",
        "また，最適化アルゴリズムは多くの場合，最小化問題を想定しているので，目的関数を$-1$倍することにします．\n",
        "最後に$10^{-10} \\times \\|\\mathbf{w}\\|^2$を目的関数に足しています．\n",
        "これは，重みパラメータが発散していくことを防ぐ目的のために追加されています．"
      ],
      "metadata": {
        "id": "fhEQ43sCuSLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "dim_state = 3\n",
        "dim_action = 1\n",
        "dim_x = dim_state + 1\n",
        "\n",
        "\n",
        "def objective1(x):\n",
        "    policy = LinearPolicy(x)\n",
        "    history, img = rollout(envname, policy, seed=100)\n",
        "    return -cumulative_reward(history) + 1e-10 * np.dot(x, x)"
      ],
      "metadata": {
        "id": "_XUuwHIUt5Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "適当な値を入れてみましょう．\n",
        "シードを固定しているので，同じパラメータを入力すれば，何度計算しても同じ値が帰ってくるはずです．"
      ],
      "metadata": {
        "id": "LkZyDlqVwhLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "objective1(np.array([0, 0, 0, 0]))"
      ],
      "metadata": {
        "id": "95-1TgdYwXbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1+1)-CMA-ES による最適化\n",
        "\n",
        "ブラックボックス最適化アプローチである，(1+1)-CMA-ESを用いて最適化してみましょう．\n",
        "多くの数値最適化法では，目的関数の勾配を利用します．\n",
        "しかし，今回の問題では，目的関数の勾配を得ることが自明でないと容易にわかるかと思います．\n",
        "(1+1)-CMA-ESは，勾配を使わず，目的関数の大小関係のみを用いて探索する方法であるため，非常に適用範囲の広いアプローチです．\n",
        "\n",
        "(1+1)-CMA-ESは，正規分布からの解生成を繰り返す山登り法です．\n",
        "正規分布の中心が現在の解です．\n",
        "生成された解が現在の解以上に良い目的関数値をとるならば，正規分布の中心をそちらに移動します．\n",
        "さもなくば動きません．\n",
        "効率的に解を探索するために，良い解が生成されやすいように，正規分布の共分散行列も同時に更新していきます．\n",
        "良い解の尤度を高くするような（最尤推定のような）更新を行っていると解釈できます．\n"
      ],
      "metadata": {
        "id": "5mRu5XmRxpQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cmaes:\n",
        "    \"\"\"(1+1)-Active-CMA-ES [Arnold 2010] with Simplified 1/5th Success Rule\"\"\"\n",
        "\n",
        "    def __init__(self, init_fx, init_x, init_s):\n",
        "        self.dim = len(init_x)\n",
        "\n",
        "        self.aup = np.exp(2.0 / (2.0 + self.dim))\n",
        "        self.adown = self.aup ** (-0.25)\n",
        "        self.pthresh = 0.44\n",
        "        self.cp = 1.0 / 12.0\n",
        "        self.cc = 2.0 / (self.dim + 2)\n",
        "        self.ccovp = 2.0 / (self.dim**2 + 6)\n",
        "        self.ccovm = 0.4 / (self.dim**1.6 + 1)\n",
        "\n",
        "        self.x = np.array(init_x, copy=True)\n",
        "        self.xc = np.zeros(self.dim)\n",
        "        self.yc = np.zeros(self.dim)\n",
        "        self.zc = np.zeros(self.dim)\n",
        "        self.s = init_s\n",
        "        self.psucc = 0.5\n",
        "        self.fhist = [init_fx]\n",
        "        self.p = np.zeros(self.dim)\n",
        "        self.chol = np.eye(self.dim)\n",
        "        self.cholinv = np.eye(self.dim)\n",
        "\n",
        "    def ask(self):\n",
        "        self.zc = np.random.randn(self.dim)\n",
        "        self.yc = np.dot(self.chol, self.zc)\n",
        "        self.xc = self.x + self.s * self.yc\n",
        "        return self.xc\n",
        "\n",
        "    def tell(self, fx):\n",
        "        if fx <= self.fhist[0]:\n",
        "            self.fhist = [fx] + self.fhist[:min(4, len(self.fhist))]\n",
        "            self.x = self.xc\n",
        "            self.s *= self.aup\n",
        "            self.psucc = (1.0 - self.cp) * self.psucc + self.cp\n",
        "            if self.psucc > self.pthresh:\n",
        "                self.p = (1.0 - self.cc) * self.p\n",
        "                d = self.ccovp * (1.0 - self.cc * (2.0 - self.cc))\n",
        "            else:\n",
        "                self.p = (1.0 - self.cc) * self.p + np.sqrt(self.cc * (2.0 - self.cc)) * self.yc\n",
        "                d = self.ccovp\n",
        "            w = np.dot(self.cholinv, self.p)\n",
        "            wnorm2 = np.dot(w, w)\n",
        "            if wnorm2 > 1e-6:\n",
        "                a = np.sqrt(1.0 - d)\n",
        "                b = np.sqrt(1.0 - d) * (np.sqrt(1.0 + self.ccovp * wnorm2 / (1.0 - d)) - 1.0) / wnorm2\n",
        "                self.chol = a * self.chol + b * np.outer(self.p, w)\n",
        "                self.cholinv = self.cholinv / a - (b / (a**2 + a * b * wnorm2)) * np.outer(w, np.dot(w, self.cholinv))\n",
        "        else:\n",
        "            self.s *= self.adown\n",
        "            self.psucc = (1.0 - self.cp) * self.psucc\n",
        "            if len(self.fhist) > 4 and fx > self.fhist[4] and self.psucc <= self.pthresh:\n",
        "                znorm2 = np.dot(self.zc, self.zc)\n",
        "                ccov = self.ccovm if 1.0 >= self.ccovm * (2.0 * znorm2 - 1.0) else 1.0 / (2.0 * znorm2 - 1.0)\n",
        "                a = np.sqrt(1.0 + ccov)\n",
        "                b = np.sqrt(1.0 + ccov) * (np.sqrt(1.0 - ccov * znorm2 / (1.0 + ccov)) - 1) / znorm2\n",
        "                self.chol = a * self.chol + b * np.outer(self.yc, self.zc)\n",
        "                self.cholinv = self.cholinv / a - (b / (a**2 + a * b * znorm2)) * np.outer(self.zc, np.dot(self.zc, self.cholinv))"
      ],
      "metadata": {
        "id": "7ixADaED0iiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それではESオブジェクトを生成し，最適化してみましょう．"
      ],
      "metadata": {
        "id": "vXxiy1G_gHsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 初期分布の設計\n",
        "sigma = 10.0\n",
        "mean = sigma* np.random.randn(4)\n",
        "fmean = objective1(mean)\n",
        "\n",
        "# 履歴\n",
        "f_hist = np.zeros(1000)\n",
        "\n",
        "# オブジェクトの作成\n",
        "es1 = Cmaes(fmean, mean, sigma)"
      ],
      "metadata": {
        "id": "ge2AVPWvyN6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下が実行コードです．もし十分に収束していないと判断される場合，実行を繰り返せば続きからの最適化が実行されます．\n",
        "実行後に探索過程で得られた解の評価値をプロットしています．"
      ],
      "metadata": {
        "id": "Ly68TLnFt1a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 実行\n",
        "for t in range(len(f_hist)):\n",
        "    w = es1.ask()  # 新しい解を生成\n",
        "    f_hist[t] = objective1(w)  # 新しい解を評価\n",
        "    es1.tell(f_hist[t])  # 分布を更新\n",
        "    if (t+1) % 10 == 0:\n",
        "        print(t+1, np.min(f_hist[:t+1]), es1.s)\n",
        "    if t >= 300 and np.min(f_hist[:t+1-300]) == np.min(f_hist[:t+1]):\n",
        "        f_hist = f_hist[:t+1]\n",
        "        break\n",
        "\n",
        "plt.plot(f_hist)"
      ],
      "metadata": {
        "id": "pn9pE5GJtuXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に振る舞いを可視化してみます．"
      ],
      "metadata": {
        "id": "CXEi0qjEonvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "policy = LinearPolicy(es1.x)\n",
        "history, img = rollout(envname, policy=policy, render=True, seed=100)  # 可視化する場合には render = True\n",
        "visualize(img)  # 可視化\n",
        "print(cumulative_reward(history))  # 累積報酬"
      ],
      "metadata": {
        "id": "eOJjVOOForYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pendulumタスクの場合，優れた方策であれば，累積報酬が-100程度までよくなります．\n",
        "(1+1)-ESは乱数を用いた最適化法ですので，試行毎に得られる解が変わりますが，概ねそこまでの累積報酬は得られないような方策が獲得されていると思います．"
      ],
      "metadata": {
        "id": "J360Ei16cgU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 方策表現の再検討\n",
        "\n",
        "線形方策は単純なため，理解しやすいですが，表現能力が不足していました．\n",
        "そこで，これを一般化したニューラルネットワークによる方策の表現方法を考えましょう．\n",
        "ここでは一例として，`softmax`関数を用いた２層のニューラルネットワークを考えることにします．"
      ],
      "metadata": {
        "id": "rc4e3_3Wh1d9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 線形方策の表現能力\n",
        "\n",
        "まず，上で用いた線形方策における表現能力について，簡単に見ておきましょう．\n",
        "\n",
        "さて，線形方策は，観測された状態（$\\bar{\\mathbf{x}}$）と重みベクトル（$\\mathbf{w}$）の内積をとり，その値を$\\tanh$により取り得る値に変換しています．\n",
        "行動の正負だけに着目するならば，これは内積の値で一意にさだまりますから，状態空間を重みベクトル$\\mathbf{w}$で定義される超平面で分けたとき，片側では正の行動を出力し，もう片側では負の行動を出力するようになります．\n",
        "\n",
        "Pendulum環境の場合，仮にpendulumがほとんど垂直上向きであるならば，$\\sin(\\theta)$の符号だけを見て制御すれば，おおよその制御は可能と考えられますが，振り上げ動作が必要な場合にはこのような単純な制御では振り上げおよび安定化の両方を実現することは困難と思われます．\n",
        "\n",
        "このような場合，超平面で線引するだけでなく，領域ごとに異なる行動を取れるような表現能力の高い非線形な方策が必要となります．"
      ],
      "metadata": {
        "id": "XlpDgspwt_BK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ニューラルネットワークによる方策表現\n",
        "ここでは，行動を以下のように決定するニューラルネットワークを考えます．\n",
        "$$\n",
        "a = \\mathbf{V} \\cdot \\mathrm{softmax}\\left( \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}\\right)\n",
        "$$\n",
        "一般的な説明をするために，状態の次元数を$N_s$（ここでは$N_s = 3$），行動に次元数を$N_a$（ここでは$N_a = 1$）と書くことにします．\n",
        "最適化対象のパラメータは$\\mathbf{b} \\in R^K$，$\\mathbf{W} \\in R^{K \\times N_s}$，$\\mathbf{V} \\in R^{N_a \\times K}$の３つであり，合計$K (1 + N_s + N_a)$個のパラメータからなります．\n",
        "\n",
        "この方策の意味を少し考えておきましょう．\n",
        "$\\mathrm{softmax}$は入力ベクトル$\\mathbf{y} = [y_1, \\dots, y_n]$に対して\n",
        "$$\n",
        "\\mathrm{softmax}(\\mathbf{y}) = \\frac{[\\exp(y_1), \\dots, \\exp(y_n)]}{\\sum_{i=1}^{n} \\exp(y_i)}\n",
        "$$\n",
        "を計算する関数です．\n",
        "出力は必ず正で和が$1$になるようなベクトルです．\n",
        "名前の通り，$\\max$を近似するような関数です．\n",
        "上の方策の式において，$\\mathrm{softmax}$の出力を$\\mathbf{z}$などと書けば，行動は$\\mathbf{V} = [\\mathbf{v}_1 \\dots \\mathbf{v}_K]$ の列ベクトル$\\mathbf{v}_k$の線形補間\n",
        "$$\n",
        "\\mathbf{a} = \\mathbf{V} \\cdot \\mathbf{z} = \\sum_{k=1}^{K} z_k \\mathbf{v}_k\n",
        "$$\n",
        "で決まります．\n",
        "$\\mathrm{softmax}$関数が$\\max$の近似であることを考慮すれば，\n",
        "\n",
        "* $\\mathbf{V} = [\\mathbf{v}_1 \\dots \\mathbf{v}_K]$ が取る行動の候補$K$個を定める部分\n",
        "\n",
        "* $ \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}$ がK個のうちどの行動を選択するかを決める部分\n",
        "\n",
        "であることがわかります．\n",
        "これは，状態空間をボロノイ分割し（分割方法は$\\mathbf{W}$と$\\mathbf{b}$を最適化することで変化する），各領域で選択する行動が$\\mathbf{v}_k$で定まる，と解釈されます．\n",
        "実際には，$\\max$ではなく$\\mathrm{softmax}$であるため，その間の区間を線形補間していることになります．\n",
        "\n",
        "なお，上記説明からは省略しましたが，出力は定義域に収まるよう，$\\tanh$を用いて出力範囲を変換しています．\n"
      ],
      "metadata": {
        "id": "DQOcFtlUh1bE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "dim_state = 3\n",
        "dim_action = 1\n",
        "K = 20\n",
        "N = (dim_state + dim_action + 1) * K\n",
        "\n",
        "\n",
        "class NNPolicy:\n",
        "    def __init__(self, weights, K=K):\n",
        "        Na = dim_action\n",
        "        Ns = dim_state\n",
        "        self.B = weights[:K]\n",
        "        self.W = weights[K:K*Ns+K].reshape((K, Ns))\n",
        "        self.V = weights[K*Ns+K:K*Ns+K+K*Na].reshape((Na, K))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        z = np.dot(observation, self.W.T)\n",
        "        z += self.B\n",
        "        z = softmax(z)\n",
        "        z = np.dot(z, self.V.T)\n",
        "        return 2 * np.tanh(z)"
      ],
      "metadata": {
        "id": "EZZgFt_ysN5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective2(x):\n",
        "    policy = NNPolicy(x)\n",
        "    history, img = rollout(envname, policy, seed=100)\n",
        "    return -cumulative_reward(history) + 1e-10 * np.dot(x, x)\n",
        "\n",
        "sigma = 10.0\n",
        "mean = sigma * np.random.randn(N)\n",
        "f_hist = np.zeros(1000)\n",
        "fmean = objective2(mean)\n",
        "es2 = Cmaes(fmean, mean, sigma)"
      ],
      "metadata": {
        "id": "jVaSdUC0sN2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(len(f_hist)):\n",
        "    w = es2.ask()\n",
        "    f_hist[t] = objective2(w)\n",
        "    es2.tell(f_hist[t])\n",
        "    if (t+1) % 10 == 0:\n",
        "        print(t+1, np.min(f_hist[:t+1]), es2.s)\n",
        "    if t >= 300 and np.min(f_hist[:t+1-300]) == np.min(f_hist[:t+1]):\n",
        "        f_hist = f_hist[:t+1]\n",
        "        break\n",
        "\n",
        "plt.plot(f_hist)"
      ],
      "metadata": {
        "id": "I8zLmsdDvrK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットワークを用いた方策の場合，数回最適化を実施してみれば，200程度の目的関数値を達成する方策が得られることと思います．\n",
        "- 生成されるグラフは，最適化中に生成した解（方策パラメータ）の目的関数値を示しています．\n",
        "- 出力される数値は，左からイテレーション数，現在得られている解の目的関数値，ESの解探索の標準偏差パラメータ$\\sigma$の値\n",
        "- 収束していない（$\\sigma$の値が十分に小さくない(例えば>1e-2)）場合には，for-loop部分だけもう一度実行してみましょう．途中から最適化が追加で行われます．\n",
        "- 新しい最適化を実行する（新しい方策パラメータの初期値から探索する）場合には，Cmaesインスタンスももう一度作り直しましょう．\n",
        "\n",
        "\n",
        "獲得された方策の振る舞いを確認してみましょう．"
      ],
      "metadata": {
        "id": "uS-aJNduzbzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "policy = NNPolicy(es2.x)\n",
        "history, img = rollout(envname, policy=policy, render=True, seed=100)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "aGnNIISviIqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上より，十分な表現能力をもつ方策モデル選択することが重要であることがわかってもらえたかと思います．"
      ],
      "metadata": {
        "id": "XFC4jPpm0jzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 汎化性能の評価\n",
        "\n",
        "さて，上の最適化の際には，pendulumの初期状態を固定して方策パラメータを最適化しました．\n",
        "実際には，他の初期状態から始めたとしても良い方策となって欲しいので，初期値がランダムに決まるケースでの累積報酬の期待値を計算してみましょう．\n",
        "期待値を計算するといっても，解析的に計算できるわけではないので，複数回ランダムな初期値から実行し，その平均を計算してみます．\n",
        "\n",
        "以下では，与えた回数だけ累積報酬を計算（エピソードを回し）し，その結果（配列）を計算しています．"
      ],
      "metadata": {
        "id": "-BALV-GzEmEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "policy = NNPolicy(es2.x)  # 最適化によって得られた方策\n",
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=policy, render=False)\n",
        "    return_array[i] = cumulative_reward(history)"
      ],
      "metadata": {
        "id": "y_N3_aATvLmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "経験分布関数をプロットしてみましょう．\n",
        "ここで表示しているものは，横軸（累積報酬の -1倍の値 = 目的関数の値）より小さな目的関数値が得られた割合です．\n",
        "例えば，縦軸が0.5のときの横軸の値が，得られた目的関数値の中央値となります．"
      ],
      "metadata": {
        "id": "kXyM2vi6getv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(0, 1800)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "_4KwCTy7gdMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この結果を見ると，特定の初期値からの方策を最適化しただけでは，他の初期値から始めた場合に必ずしも適切な行動が取れていないことがわかります．\n",
        "目的関数が特定の初期値からの方策の性能を評価しているため，他の初期値から始めた場合の性能を考慮していないことが一因です．"
      ],
      "metadata": {
        "id": "LUWQA_M2a5GU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 目的関数の再検討\n",
        "\n",
        "最適化法は目的関数の値を良くすることだけを目標に最適なパラメータを求めていくため，使用者が望ましいと考える方策の性質を実現するためには，そのような性質をもつ方策に対してインセンティブを与えるような目的関数が設計されていることが必要です．\n",
        "今回の場合，様々な初期値から始めたとしても高速にpendulumを振り上げて安定化させることが目的ですから，これを評価するような目的関数を設計する必要があります．"
      ],
      "metadata": {
        "id": "pMTYtrzMfYgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### パーセンタイルの最適化\n",
        "そこで，目的関数を，初期値を変えて10回方策を評価した際の90パーセンタイルとしてみましょう．\n",
        "このようにすることで，9割くらいの初期状態に対しては良い報酬が得られるような方策を獲得できると期待されます．\n",
        "なお，目的関数の内部で計算するエピソード数が増えるほど，計算時間が増えていきますので，注意してください．"
      ],
      "metadata": {
        "id": "t6RSnrDKiyj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def objective3(x):\n",
        "    envname = \"Pendulum-v1\"\n",
        "    policy = NNPolicy(x)\n",
        "    seed_array = np.arange(100, 1100, 100)\n",
        "    return_array = np.zeros(len(seed_array))\n",
        "    for i, seed in enumerate(seed_array):\n",
        "        history, img = rollout(envname, policy=policy, render=False, seed=seed)\n",
        "        return_array[i] = cumulative_reward(history)\n",
        "    return np.percentile(- return_array, 90) + 1e-10 * np.dot(x, x)\n",
        "\n",
        "sigma = 10\n",
        "mean = sigma * np.random.randn(N)\n",
        "f_hist = np.empty(1000)\n",
        "f_hist[0] = objective3(mean)\n",
        "es3 = Cmaes(f_hist[0], mean, sigma)"
      ],
      "metadata": {
        "id": "cYeXDLXsGT6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(len(f_hist)):\n",
        "    w = es3.ask()\n",
        "    f_hist[t] = objective3(w)\n",
        "    es3.tell(f_hist[t])\n",
        "    if (t+1) % 10 == 0:\n",
        "        print(t+1, np.min(f_hist[:t+1]), es3.s)\n",
        "    if t >= 300 and np.min(f_hist[:t+1-300]) == np.min(f_hist[:t+1]):\n",
        "        f_hist = f_hist[:t+1]\n",
        "        break\n",
        "\n",
        "plt.plot(f_hist)"
      ],
      "metadata": {
        "id": "dTG2BDmLkvad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この目的関数（90パーセンタイル）の下でも，うまくいくと200程度の目的関数値が得られます．\n",
        "ただ，一度の目的関数評価に複数回のエピソードを実行しているため，最適化実行時間は長くなります．\n",
        "\n"
      ],
      "metadata": {
        "id": "Li6eTkxXoC5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "目的関数は上においてもシードを固定した複数試行から計算されています．\n",
        "これは，目的関数が確率的に変動してしまうと最適化が困難になるため，簡単化するための工夫です．\n",
        "ただし，この場合，特定のシード（特定の初期状態）にオーバーフィットしてしまう可能性がありますので，様々な環境で評価してみましょう．"
      ],
      "metadata": {
        "id": "5-7UoaQgoZSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "policy = NNPolicy(es3.x)\n",
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=policy, render=False)\n",
        "    return_array[i] = cumulative_reward(history)"
      ],
      "metadata": {
        "id": "qxeVFAb0r6AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(0, 1800)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "T7M6w_T2saxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "こちらも，試行によって結果は変わるかと思いますが，うまく行くと目的関数値の90パーセンタイルが200付近の値をとるような方策が獲得できます．\n",
        "すなわち，10回に9回は非常にうまくpendulumを垂直上向きに保つことができるような方策が得られるということです．\n",
        "ただし，最適化している目的関数で計算しているパーセンタイルはあくまでシードを固定した推定値ですから，多少はズレがあると思います．\n",
        "また，ワーストケースなどを見ると，最適化時に観測された目的関数値よりも大きく悪い結果になっていることが確認できると思います．\n",
        "このように，最適化法は，与えられた目的関数が計算する値については最適化するものの，そこに反映されていない人が暗黙に期待するような振る舞いを獲得してくれるわけではありません．\n",
        "\n",
        "以下で振る舞いを確認してみましょう．\n",
        "環境を初期化する際に乱数のシードを指定しなければ，毎回異なる初期値に初期化されます．\n",
        "以下を何度か繰り返し，うまくいく試行，いかない試行の振る舞いを見てみましょう．"
      ],
      "metadata": {
        "id": "-QXQ3BVbdumc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"Pendulum-v1\"\n",
        "policy = NNPolicy(es3.x)\n",
        "history, img = rollout(envname, policy=policy, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "Rz3teyDRjLqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用環境２：Lunar Lander（連続状態，離散行動）\n",
        "\n",
        "もう一つ，別の環境での例を見てみましょう．\n",
        "次に扱うのは Lunar Lander 環境です．\n",
        "\n",
        "https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "\n",
        "この環境では，エージェントが$8$次元のセンサー情報（連続値）を状態として観測し，$4$つの行動から一つを選択します．\n",
        "その行動は\n",
        "\n",
        "0. 何もしない\n",
        "1. 左向きのエンジンを蒸す\n",
        "2. メインエンジンを蒸す\n",
        "3. 右向きのエンジンを蒸す\n",
        "\n",
        "というものです．\n",
        "すなわち，先程とは異なり，行動が離散値（カテゴリカル）を取ります．\n",
        "\n",
        "まずは以下を実行し，ランダム行動を取る場合の振る舞いを確認してみましょう．"
      ],
      "metadata": {
        "id": "obiyI0sDsJdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(\"LunarLander-v2\", render=True)\n",
        "cumulative_reward(history)\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "roLARKHNsSnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "行動が連続か離散かという違いの他に，もう一点，重要な違いが PendulumとLunar Landerの間に存在します．\n",
        "\n",
        "* Pendulum 環境やCart-Pole環境では，予め定められた回数（200回, 500回）のインタラクションを実行すると， フラグ truncated が Trueとなり，状態が初期化されます．\n",
        "\n",
        "* Lunar Lander環境では，着陸に成功 or 着地失敗など，特定の状態になると，成功 or 失敗という認識になり，フラグterminated がTrueとなり，状態が初期化されます．\n",
        "\n",
        "前者の場合，例えば目的関数を計算する都合上，無限に長いインタラクション数を取ることはできないため，強制的に終了させているとみなせます．\n",
        "他方，後者の場合には，明確に終了となる状態が定義されており，「一度の試行」という概念が定義されているようなタスクであると言えます．\n",
        "後者のようなタスクを「エピソディックタスク」と呼びます．\n",
        "この二つの違いは，強化学習を行う上では重要な違い（使用できる方法に違いが生じる）であるため，認識しておきましょう．\n",
        "ただ，ブラックボックス最適化として考える場合には，大きな差はありません．\n"
      ],
      "metadata": {
        "id": "Fh5I9rvGt2cI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax方策\n",
        "\n",
        "行動が離散ですので，先程とは異なる方策が必要になります．\n",
        "行動が4種類であるということは，状態空間を4分割し，各分割では一つの行動を選択るという方法が考えられます．\n",
        "これは，教師あり学習で言うところの分類タスクに該当します．\n",
        "そこで，分類タスクによく使われるSoftmaxを用いた方策を利用しましょう．\n",
        "これは，$4$次元出力の線形モデルに対し，softmax関数を適用し，その結果として得られるベクトルを確率ベクトルとして行動を選択する，という確率的方策になります．\n",
        "\n",
        "ここでも違いが出てきたので紹介しておきます．\n",
        "Pendulum環境で用いていた方策はいずれも状態が与えられると行動が一意に定まる「決定的方策」でした．\n",
        "他方，ここで用いている方策は，状態が与えられるとそれによって得られる条件付き確率に従って行動を選択する「確率的方策」になります．\n",
        "強化学習の場合，良い行動を探索するために確率的方策が必要になる場合があります．\n",
        "一方，ブラックボックス最適化として扱う場合には，探索は探索法が担っているので，確率的方策を使用することは必要ではありません．\n",
        "ここでは，確率的方策の紹介と，分類タスクとの関連を紹介するために，利用しています．\n",
        "\n",
        "Softmax方策において各行動を取る確率は以下のように定義されます．\n",
        "$$\n",
        "\\pi(a=k \\mid s) = \\frac{ \\exp( \\mathbf{w}_k^\\mathrm{T} s + b_k) }{ \\sum_{i=1}^{4} \\exp( \\mathbf{w}_i^\\mathrm{T} s + b_i) }\n",
        "$$\n",
        "ここで，$\\mathbf{w}_1 = \\mathbf{0}$と$b_1 = 0$はいずれも固定とします．\n",
        "そのようにしても，表現能力は損なわれません．\n",
        "逆に，全てのパラメータを変動できるようにした場合，方策がオーバーパラメタライズされた状況となり，「パラメータを変えても表現される方策が変わらず，その結果目的関数が変化しない」ような領域が無数に存在することになり，これが最適化を難しくすることがあります．\n",
        "結果として，この方策はパラメータ$\\theta = (\\mathbf{w}_2, \\mathbf{w}_3, \\mathbf{w}_4, b_2, b_3, b_4)$で表現されることになります．これが最適化の対象となります．\n"
      ],
      "metadata": {
        "id": "lwmppne4yO3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "N = (1 + dim_state) * (num_action - 1)\n",
        "\n",
        "class SoftmaxPolicy:\n",
        "    def __init__(self, x):\n",
        "        self.w = np.zeros((num_action, dim_state))\n",
        "        self.b = np.zeros(num_action)\n",
        "        self.b[1:] = x[:num_action-1]\n",
        "        self.w[1:, :] = x[num_action-1:].reshape((num_action - 1, dim_state))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        logit = np.dot(self.w, observation) + self.b\n",
        "        prob = softmax(logit)\n",
        "        return random.choices(list(range(len(prob))), prob)[0]"
      ],
      "metadata": {
        "id": "uN8rCyXWwYq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それでは，このSoftmax方策を(1+1)-ESを用いて最適化してみましょう．\n",
        "目的関数は特定のシード（地形に影響）のもとでの累積報酬の$-1$倍とします．\n",
        "今回は，方策が確率的であるため，目的関数の値も確率的となります．"
      ],
      "metadata": {
        "id": "UTAb-Oaxz98O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective4(x):\n",
        "    policy = SoftmaxPolicy(x)\n",
        "    history, img = rollout(envname, policy, seed=100)\n",
        "    return -cumulative_reward(history) + 1e-10 * np.dot(x, x)\n",
        "\n",
        "\n",
        "sigma = 10.0\n",
        "mean = sigma * np.random.randn(N)\n",
        "f_hist = np.empty(1000)\n",
        "f_hist[0] = objective4(mean)\n",
        "es4 = Cmaes(f_hist[0], mean, sigma)"
      ],
      "metadata": {
        "id": "W1TDCtezwYoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(len(f_hist)):\n",
        "    w = es4.ask()\n",
        "    f_hist[t] = objective4(w)\n",
        "    es4.tell(f_hist[t])\n",
        "    if (t+1) % 10 == 0:\n",
        "        print(t+1, np.min(f_hist[:t+1]), es4.s)\n",
        "    if t >= 300 and np.min(f_hist[:t+1-300]) == np.min(f_hist[:t+1]):\n",
        "        f_hist = f_hist[:t+1]\n",
        "        break\n",
        "\n",
        "plt.plot(f_hist)"
      ],
      "metadata": {
        "id": "t5lbU2UxwYk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "汎化性能を評価するために，経験分布関数を表示してみましょう．\n",
        "Pendulumで見た通り，最適化時に特定のシードを用いている（ここでは地形に対応）ため，これが変わるとうまく行かないことが見て取れます．"
      ],
      "metadata": {
        "id": "xXMMg8jQZIT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "policy = SoftmaxPolicy(es4.x)\n",
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=policy, render=False)\n",
        "    return_array[i] = cumulative_reward(history)"
      ],
      "metadata": {
        "id": "qLKGpD9LsDTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "2ZCOefuDwYbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "振る舞いを可視化してみましょう．seed=100は最適化時に利用している環境であり，それ以外の場合と比較してうまく制御されていることが見て取れるかと思います．"
      ],
      "metadata": {
        "id": "O_gvaAA5Zc0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=policy, render=True, seed=100)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "I40cpOgyxivT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=policy, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "DQBugpI4s9Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax方策は表現能力が不十分なため，Pendulumにおいて用いたニューラルネットワーク方策を利用してみましょう．\n",
        "出力が確率ベクトルとなるように，$\\tanh$の代わりにsoftmax関数を出力にかけ，その結果を行動選択確率として行動を選択します．"
      ],
      "metadata": {
        "id": "n0WhOZDOZ7rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "K = 20\n",
        "N = (dim_state + num_action + 1) * K\n",
        "\n",
        "class SoftmaxNNPolicy:\n",
        "    def __init__(self, weights, K=K):\n",
        "        Na = num_action\n",
        "        Ns = dim_state\n",
        "        self.B = weights[:K]\n",
        "        self.W = weights[K:K*Ns+K].reshape((K, Ns))\n",
        "        self.V = weights[K*Ns+K:K*Ns+K+K*Na].reshape((Na, K))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        z = np.dot(observation, self.W.T)\n",
        "        z += self.B\n",
        "        z = softmax(z)\n",
        "        z = np.dot(z, self.V.T)\n",
        "        prob = softmax(z)\n",
        "        return random.choices(list(range(len(prob))), prob)[0]"
      ],
      "metadata": {
        "id": "xq2z6Pw1O6x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective5(x):\n",
        "    policy = SoftmaxNNPolicy(x)\n",
        "    history, img = rollout(envname, policy, seed=100)\n",
        "    return -cumulative_reward(history) + 1e-10 * np.dot(x, x)\n",
        "\n",
        "\n",
        "sigma = 10\n",
        "mean = sigma * np.random.randn(N)\n",
        "f_hist = np.empty(1000)\n",
        "f_hist[0] = objective5(mean)\n",
        "es5 = Cmaes(f_hist[0], mean, sigma)"
      ],
      "metadata": {
        "id": "AnLtHzluPOSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(len(f_hist)):\n",
        "    w = es5.ask()\n",
        "    f_hist[t] = objective5(w)\n",
        "    es5.tell(f_hist[t])\n",
        "    if (t+1) % 10 == 0:\n",
        "        print(t+1, np.min(f_hist[:t+1]), es5.s)\n",
        "    if t >= 300 and np.min(f_hist[:t+1-300]) == np.min(f_hist[:t+1]):\n",
        "        f_hist = f_hist[:t+1]\n",
        "        break\n",
        "\n",
        "plt.plot(f_hist)"
      ],
      "metadata": {
        "id": "HXw2IQGIPqN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "振る舞いを見てみましょう．"
      ],
      "metadata": {
        "id": "1BewEq1pbknC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = SoftmaxNNPolicy(es5.x)\n",
        "history, img = rollout(envname, policy=policy, render=True, seed=100)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "X32YA59VR-A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = SoftmaxNNPolicy(es5.x)\n",
        "history, img = rollout(envname, policy=policy, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "D_hnB1B4T5hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "経験分布関数も確認しておきましょう．\n",
        "やはり，方策を変えるだけでは汎化性能は高くなりません．\n",
        "目的関数をパーセンタイルにするなどの工夫が必要です．"
      ],
      "metadata": {
        "id": "4yNWOULib0TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "policy = SoftmaxNNPolicy(es5.x)\n",
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=policy, render=False)\n",
        "    return_array[i] = cumulative_reward(history)"
      ],
      "metadata": {
        "id": "Nwfkuk--UDPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "zpZBrJEwUF99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## この章のまとめ\n",
        "\n",
        "ここでは方策最適化をブラックボックス最適化問題として捉え，(1+1)-CMA-ES を用いて実際に最適化してみました．\n",
        "目的関数の設計の仕方，方策の選び方，などによって，最終的に得られる方策に違いが見られる様子が確認できたと思います．\n",
        "これは，(1+1)-CMA-ESによらず，強化学習など他の方策最適化においても同様です．\n",
        "\n",
        "目的関数，もしくはそれを定義する際に用いられる報酬関数$r$は，実際に獲得させたい振る舞いを反映させるように設計されることが必要です．\n",
        "これは実は簡単なことではありません．\n",
        "また，数学的には目的が正しく定義されていても，最適化にとって都合が悪い，という場合もあります．\n",
        "そういった場合，必ずしも実際に最適化している指標が本来最適化したい指標と一致しない場合もあります．\n",
        "このあたりは実用上極めて大切な部分です．\n",
        "常によく検討するようにしましょう．\n",
        "今回扱っている環境では，報酬自体はライブラリによって設計されていますので，この難しさは現れませんが，実用上は利用者が報酬を設計しなければならず，これがなかなか難しい場合があります．\n",
        "\n",
        "ブラックボックス最適化法を用いて方策最適化を実現することの利点は，その汎用性です．\n",
        "例えば，目的関数を比較的自由に設計することが可能です．\n",
        "今回もパーセンタイルの最適化を行いましたが，これを強化学習で実現することは自明ではありません．\n",
        "また，方策についても自由に選ぶことができます．\n",
        "状態や行動が離散か連続かなどの違いも，方策を変更するだけで対応できます．\n",
        "タスクがエピソディックタスクかどうかについても，あまり気にする必要はありません．ただし，目的関数を計算する都合上，本質的には終了状態が存在しないようなタスク（Pendulumなど）であっても，必ずどこかでエピソードを打ち切る必要は出てきます．\n",
        "その点，一部の（非エピソディックなタスクに対応した）強化学習では，そのような人工的なエピソードの終了が必須ではありません．"
      ],
      "metadata": {
        "id": "DfZ7KTmRm617"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自習課題\n",
        "\n",
        "1. 目的関数を変更してみましょう．例えば，初期値分布に対して期待値を最適化するためにはどのように変更すれば良いでしょうか．\n",
        "変更した目的関数を最適化した結果得られる方策を用いて複数試行回したとき，得られる報酬の期待値が他の目的関数を最適化した結果得られる値よりも小さくなっているのか，確認してみましょう．\n",
        "\n",
        "2. 方策を変更してみましょう．例えば，Pendulumの場合，今回のニューラルネットワークを用いた方策表現では，表現能力がハイパーパラメータ$K$に依存します．これを大きくすればより柔軟な表現が可能になりますが，パラメータ数がこれに比例して大きくなります．\n",
        "Lunar Landerの場合，もっと表現能力の高い方策を使用してみましょう．\n",
        "\n",
        "3. 最適化法を変更してみましょう．ブラックボックス最適化法であれば，どのようなアプローチを用いても構いません．\n",
        "方法毎に向き不向きがあります．\n",
        "今回採用した(1+1)-ESは，決して方策最適化に向いているとは言い難いです．\n",
        "一つの理由として，(1+1)-ESは目的関数のノイズ（目的関数の値が決定的でない）に対して適さない方法であることが挙げられます．\n",
        "今回は，あくまで方策最適化というものを理解するために，単純な最適化法を採用したに過ぎません．\n",
        "例えばよく知られたベイズ最適化などは，少ない目的関数の評価回数を想定して設計されているため，多くの目的関数評価を繰り返すことが想定される方策最適化には不向きとなります．\n",
        "他方，強化学習では，クロスエントロピー法と呼ばれる方法が採用されることがあります．これは，別のレポジトリにて紹介しているCMA-ESに良く似た方法ですので，こちらを参考にすると良いと思います．\n",
        "https://github.com/akimotolab/CMAES_Tutorial\n"
      ],
      "metadata": {
        "id": "Y5gvKQlMJ7Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8gwRqGQCCm8X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}