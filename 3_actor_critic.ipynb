{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyORJDvHNOTcsojO0ZH/K4yU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akimotolab/Policy_Optimization_Tutorial/blob/main/3_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備\n",
        "\n",
        "まず，必要なパッケージのインストールとインポート，および仮想displayを設定します．"
      ],
      "metadata": {
        "id": "qWe6P9xqH8be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kXoBp0fgHyhe",
        "outputId": "8317135d-5eff-43ed-8d5a-08efb04cf4c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [631 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,204 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,487 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,011 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,512 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,279 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n",
            "Fetched 7,453 kB in 3s (2,248 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "14 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1.post0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 7,814 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n",
            "Fetched 7,814 kB in 3s (2,889 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120880 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1.post0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373074 sha256=d197c7e2356f6f1766a0afa3adb3a20ee5ae7f94db2a5a7ab1f56b50a1ae9a52\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: farama-notifications, box2d-py, gymnasium\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "# 必要なパッケージのインストール\n",
        "!apt update\n",
        "!pip install swig\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import torch\n",
        "\n",
        "# 仮想ディスプレイの設定\n",
        "_display = Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "# gpuが使用される場合の設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
      ],
      "metadata": {
        "id": "iJ1PYM90IHa9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "d5Oklvxh5nAC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "続いて，第１回の資料で定義した`rollout`などの基本的な関数をここでも定義しておきます．"
      ],
      "metadata": {
        "id": "ci2EEsRpD1oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout(envname, policy=None, render=False, seed=None):\n",
        "    if render:\n",
        "        env = gym.make(envname, render_mode=\"rgb_array\")\n",
        "    else:\n",
        "        env = gym.make(envname)\n",
        "    history = []\n",
        "    img = []\n",
        "\n",
        "    # 乱数の設定\n",
        "    if seed is not None:\n",
        "        random.seed(int(seed))\n",
        "    envseed = random.randint(0, 1000)\n",
        "    actseed = random.randint(0, 1000)\n",
        "    observation, info = env.reset(seed=envseed)\n",
        "    env.action_space.seed(actseed)\n",
        "\n",
        "    # 可視化用の設定\n",
        "    if render:\n",
        "        d = Display()\n",
        "        d.start()\n",
        "        img.append(env.render())\n",
        "\n",
        "    # メインループ（環境とのインタラクション）\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # 行動を選択\n",
        "        if policy is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy(observation)\n",
        "\n",
        "        # 行動を実行\n",
        "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "        history.append([observation, action, next_observation, reward, terminated, truncated, info])\n",
        "        observation = next_observation\n",
        "        if render:\n",
        "            display.clear_output(wait=True)\n",
        "            img.append(env.render())\n",
        "    env.close()\n",
        "    return history, img\n",
        "\n",
        "\n",
        "def visualize(img):\n",
        "    dpi = 72\n",
        "    interval = 50\n",
        "    plt.figure(figsize=(img[0].shape[1]/dpi, img[0].shape[0]/dpi), dpi=dpi)\n",
        "    patch = plt.imshow(img[0])\n",
        "    plt.axis=('off')\n",
        "    animate = lambda i: patch.set_data(img[i])\n",
        "    ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(img), interval=interval)\n",
        "    display.display(display.HTML(ani.to_jshtml()))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def cumulative_reward(history):\n",
        "    return sum(hist[3] for hist in history)"
      ],
      "metadata": {
        "id": "9ELUIKjb5vY2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 方策勾配を用いた方策最適化（強化学習）\n",
        "\n",
        "今回は「方策勾配法」を見ていきます．\n",
        "第１回は，汎用的なブラックボックス最適化法を用いた方策最適化の例を見てもらいました．\n",
        "ブラックボックス最適化法を用いていたということは，目的関数$J(\\theta)$が「何らかの方策をパラメータ$\\theta$で用いた場合に，環境とインタラクションした結果得られる累積報酬」を意味しているという情報を用いずに，ただブラックボックスな関数として最適化していることを意味します．\n",
        "ここでは，積極的にこの知識を活用していく方法を検討していきましょう．\n",
        "ブラックボックス最適化としての方策最適化と，強化学習を用いた方策最適化の一番の違いがここにあります．"
      ],
      "metadata": {
        "id": "H_gG0F5uKTYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 状態と行動の価値\n",
        "\n",
        "ブラックボックスな目的関数$J(\\theta)$では，一連のインタラクションを通して得られた報酬の合計を評価しています．\n",
        "これは，最大化したい指標であることに間違いありませんが，一方で，各状態でとった各行動が良かったのかどうか，という情報を与えてくれません．\n",
        "この情報を活用することができれば，ある状態$s$ではある行動$a$を取るとよい，ということがわかり，その確率を高くするように方策を改善することができそうです．"
      ],
      "metadata": {
        "id": "0_V-B-wGJsZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定式化\n",
        "まず，最低限の定式化を行います．\n",
        "今回対象としている方策最適化では，\n",
        "まず初期状態$s_0$を観測します．\n",
        "初期状態は確率分布$p_0$からランダムに生成されます．\n",
        "方策を通して，次にとる行動$a_0 \\sim \\pi(\\cdot \\mid s_0)$を決定します．\n",
        "ここでは，方策として確率的な方策を考えることにします．\n",
        "この行動を実行すると，状態が$s_1$に変わり，これを観測します．\n",
        "それと同時に，$s_0$で行動$a_0$を取ることの良さを表す即時報酬$r_1$が得られます．\n",
        "次状態と即時報酬は，環境が定める条件付き確率$p_T(s_1, r_1 \\mid s_0, a_0)$により定まります．\n",
        "このあとは，$s_1$において方策に従って次の行動$a_1 \\sim \\pi(\\cdot \\mid s_0)$を決定し，次状態と即時報酬を観測する，というステップを繰り返します．\n",
        "この環境との一度のインタラクションをステップと呼びます．\n",
        "\n",
        "注意：「環境が定める」といっても，実際に報酬を設計するのは自分自身（設計者）です．望ましい方策を得るためには，適切な報酬を設計することが極めて重要です．）"
      ],
      "metadata": {
        "id": "e0st6F33vB0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 累積報酬\n",
        "強化学習においては，方策最適化の目的は割引累積報酬の期待値を最大化することと一般に定められます．\n",
        "あるステップ$t$において，その先に得られる割引累積報酬は\n",
        "$$\n",
        "G_t = r_{t+1} + \\gamma r_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+1+k}\n",
        "$$\n",
        "と定義されます．ここで，$\\gamma \\in [0, 1]$は割引率と呼ばれるパラメータです．\n",
        "方策，初期状態分布，状態遷移，即時報酬は確率的ですから，$G_t$自体も確率的に振る舞います．\n",
        "そこで，これの期待値$\\mathrm{E}[G_0]$を考え，これを最大化することを考えます．\n",
        "\n",
        "ここでは，インタラクションが無限に続くことを想定して$G_t$が定義されています．\n",
        "この場合，割引率は$\\gamma < 1$であることが必要です．\n",
        "エピソディックタスクの場合，特別な終了状態（例えば迷路のような問題において，ゴール状態に到達した，落とし穴に落ちて脱落した，など）があり，途中でエピソードが止まることになります．\n",
        "この場合にも，終了状態に達したあとは何をしても終了状態に遷移し，即時報酬はずっと0である，と考えれば，上の定義に当てはまります．\n",
        "このように，インタラクションに一区切りがあるようなタスクはエピソディックタスクと呼ばれ，この一区切りのステップのまとまりをエピソードと呼びます．\n",
        "最適化（学習）の都合上，特定のステップでインタラクションを打ち切り，無理やりエピソディックタスクにするような場合もありますが，この場合にも目的は$\\mathrm{E}[G_0]$の最大化である（有限ステップでの累積報酬ではない）と考えると，以下の議論が成立します．\n",
        "\n",
        "注意：制限時間などのように，特定のステップがすぎると強制的に状態がリセットされるようなケースの場合，注意が必要です．この場合，残り時間などを状態観測に含めることが必要になります．"
      ],
      "metadata": {
        "id": "RXkv9WldvGg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 状態価値と行動価値\n",
        "\n",
        "状態$s$の価値を，「$s_0 = s$からインタラクションを始めて，方策$\\pi$に従って行動選択した際に得られる割引累積報酬の期待値$\\mathrm{E}[G_0 \\mid s_0 = s]$」と定義します．\n",
        "これを$V^{\\pi}(s)$と書きます．\n",
        "定義からわかるように，状態価値は方策$\\pi$に依存しています．\n",
        "割引累積報酬が\n",
        "$$\n",
        "G_{t} = r_{t+1} + \\gamma G_{t+1}\n",
        "$$\n",
        "という再帰的な関係式を満たすことを考えると，状態価値は\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathrm{E}[r_{1} + \\gamma V^{\\pi}(s_{1}) \\mid s_0 = s]\n",
        "$$\n",
        "という関係式を満たすことがわかります．\n",
        "なお，ステップのインデックスに関しては\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathrm{E}[r_{t+1} + \\gamma V^{\\pi}(s_{t+1}) \\mid s_t = s]\n",
        "$$\n",
        "と考えても構いません．\n",
        "状態の価値が高いということは，その方策に従っている場合には，その状態からは得られる割引累積報酬の期待値が高いということを意味しています．\n",
        "ですから，そのような状態を積極的に訪問したいという指針になるでしょう．\n",
        "\n",
        "関連して，行動価値（状態行動価値とも言います）を，「$s_0 = s$からインタラクションを始めて，最初だけ行動$a$を選択し，その後は方策$\\pi$に従って行動選択した際に得られる割引累積報酬の期待値$\\mathrm{E}[G_0 \\mid s_0 = s, a_0 = a]$」と定義します．\n",
        "これを$Q^{\\pi}(s, a)$と書きます．\n",
        "定義からわかるように，もしも最初の行動を$a \\sim \\pi(\\cdot \\mid s)$にしたがって決定していれば行動価値関数の$a \\sim \\pi(\\cdot \\mid s)$についての期待値は，状態$s$の価値\n",
        "$$\n",
        "V^\\pi(s) = \\mathrm{E}[Q^{\\pi}(s, a) \\mid a \\sim \\pi(\\cdot \\mid s)]\n",
        "$$\n",
        "に一致します．\n",
        "\n",
        "価値関数はいずれも解析的に得られるものではありません．\n",
        "しかし，インタラクションを通して，近似していくことが可能です．\n",
        "その代表的な方法の一つに，TD誤差を用いた更新があります．"
      ],
      "metadata": {
        "id": "9PHLZ4LlvJNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 方策勾配\n",
        "\n",
        "目的関数を数値的に最適化する際，まず考えられる方針は勾配法を用いることです．\n",
        "すなわち，$\\theta \\leftarrow \\theta + \\alpha_\\theta \\nabla_\\theta J(\\theta)$と更新する方法です．\n",
        "割引累積報酬をブラックボックスと捉えてしまうと勾配は計算できないのですが，価値関数を明示的に使うことで勾配をうまく近似することが可能です．\n",
        "「方策勾配定理」は，割引累積報酬の期待値の，方策パラメータについての期待値を書き下す方法を提供してくれます．"
      ],
      "metadata": {
        "id": "Lz8VutUocewQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方策勾配定理\n",
        "\n",
        "$J(\\theta) = \\mathrm{E}[V^{\\pi}(s) \\mid s \\sim p_0]$とします．\n",
        "このとき，\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) \\propto \\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "$$\n",
        "と書き下すことができます．\n",
        "ここで，上の期待値は，方策$\\pi_\\theta$のもとでの状態訪問確率と行動選択確率について取られています．\n",
        "実際にインタラクションを通して観測された状態とその状態のもとで方策$\\pi_\\theta$に従って選択された行動を用いれば，右辺は\n",
        "$$\n",
        "Q^{\\pi}(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "で近似することができます．\n",
        "また，行動価値関数をその近似値$q(s_t, a_t)$で置き換えれば，勾配$\\nabla_\\theta J(\\theta)$の推定値として\n",
        "$$\n",
        "q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "を得ることができます．\n",
        "もしくは，1エピソード分の状態遷移履歴を用いて，\n",
        "$$\n",
        "\\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "と近似することができます．\n",
        "これらを用いて，\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + \\alpha_\\theta \\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "などと方策パラメータを更新していきます．ここで，$\\alpha_\\theta$は学習率です．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "awCoxRZr50Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方策勾配の自動計算\n",
        "\n",
        "方策勾配を計算するには，方策の対数の方策パラメータについての勾配 $\\nabla_\\theta \\ln \\pi$ を求める必要があります．\n",
        "方策を変える毎に，これを計算して実行するのは，複雑な方策を採用する際には面倒になります．\n",
        "他方，深層学習で用いられている Pytorch などのライブラリでは，関数の勾配を自動的に計算してくれる機能が備わっています．この機能を活用することで，方策勾配も容易に計算することが可能になります．\n",
        "\n",
        "上に示した1エピソード分のデータを用いた方策勾配\n",
        "$$\n",
        "\\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "を計算したい状況を考えましょう．\n",
        "いま，$q^{\\pi}(s_t, a_t)$は何らかの方法（後述します）で予め計算されているとします．\n",
        "このとき，以下のような関数を考えます．\n",
        "$$\n",
        "L(\\theta; \\{(s_t, a_t)\\}_{t=0}^{T}) = \\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "このような関数$L$を定義すると，上に示した方策勾配は $\\nabla_\\theta L(\\theta; \\{(s_t, a_t)\\}_{t=0}^{T})$ であることが容易にわかります．\n",
        "\n",
        "この事実を用いると，方策勾配を自動計算することが可能になります．\n",
        "行動が離散であれば，各行動の選択確率からなるベクトルを返す関数モデルを用意しておけば，用意に計算できます．"
      ],
      "metadata": {
        "id": "SxNyza3zTfCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下では，Pytorchを用いた方策（ここではActorと呼ぶことにします．これは今後紹介するActor-Criticアルゴリズムでの呼び方に従っています．）の実装方法を紹介します．\n",
        "ここでは四層のニューラルネットワークを用いています．中間層のノード数は`dim_hidden`により指定されています．\n",
        "\n",
        "関数`forward`が観測`x`を受け取って，これに対して各行動の選択確率を出力する関数です．\n",
        "行動選択確率は以下のように計算されます．\n",
        "入力を$h_0 = x$とします．\n",
        "$i = 1, 2, 3, 4$について，以下を計算します．\n",
        "$$\n",
        "h_i = g_i\\left( b_i + W_i h_{i=1}\\right)\n",
        "$$\n",
        "ここで，中間層の活性化関数$g_1, g_2, g_3$は$\\mathrm{ReLU}(s) = \\max(0, s)$であり，\n",
        "出力部分の活性化関数$g_4$は出力の和が$1$となるように，$\\mathrm{softmax}$を用いています．"
      ],
      "metadata": {
        "id": "KBZ3NMsNVNRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REINFORCEアルゴリズム（行動価値関数のMonte Carlo推定を用いた方策勾配法）\n",
        "\n",
        "方策勾配を用いた強化学習の代表的な一つの方法である，REINFORCEアルゴリズムを紹介します．\n",
        "話を簡単にするために，ここでは\n",
        "\n",
        "1. エピソディックタスク，\n",
        "2. 割引率$\\gamma = 1$（割引なし），\n",
        "3. 確率的方策，\n",
        "\n",
        "であることを仮定します．\n",
        "\n",
        "方策勾配法を用いる場合，行動価値関数$Q^{\\pi}(s_t, a_t)$の近似値$q(s_t, a_t)$を得ることが必要になります．\n",
        "この$q(s_t, a_t)$の計算方法の違いにより，様々な方策勾配法のバリエーションが存在します．\n",
        "REINFORCEアルゴリズムでは，Monte-Carlo推定を用いてこれを近似して利用します．"
      ],
      "metadata": {
        "id": "D052tsAPBSKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 行動価値関数のモンテカルロ推定\n",
        "\n",
        "行動価値関数$Q^{\\pi}(s, a)$は「$s_0 = s$からインタラクションを始めて，最初だけ行動$a$を選択し，その後は方策$\\pi$に従って行動選択した際に得られる割引累積報酬の期待値$\\mathrm{E}[G_0 \\mid s_0 = s, a_0 = a]$」と定義されます．\n",
        "すなわち，この期待値は各ステップ$t$での状態の訪問確率と行動の選択確率を知らなければ計算できません．\n",
        "行動の選択確率は方策で決まりますから，これは予め知っている情報ですが，ある状態である行動を取った際の次の状態への遷移確率は未知である（実際に実行して始めて次状態を観測できる）と仮定していますから，これを計算することはできません．\n",
        "期待値を厳密には計算できませんが，実際にインタラクションを通して累積報酬の実現値を観測することは可能です．\n",
        "これを用いて期待値を推定する方法がモンテカルロ推定です．\n",
        "\n",
        "状態遷移はステップ数$t$には依存しないため，行動価値関数$Q^{\\pi}(s, a)$は$\\mathrm{E}[G_t \\mid s_t = s, a_t = a]$と定義しても同じものになります．\n",
        "すなわち，ステップ$t$において観測した状態が$s_t = s$，$a_t = a$であったならば，そこから実際にインタラクションして得られた累積報酬 $G_t = r_{t+1} + \\dots + r_{T}$（$T$は終端ステップ）の期待値がその$(s_t, a_t)$の行動価値であり，観測された$G_t$は確率変数として見た場合の$G_t$の観測値（サンプル）であることがわかります．\n",
        "そのため，観測された$G_t$は行動価値$Q^\\pi(s_t, a_t)$の不偏推定値であることになります．\n",
        "そこで，これを$q^\\pi(s_t, a_t)$として採用することにしましょう．\n"
      ],
      "metadata": {
        "id": "NoMZcO82CzR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "実装上は，1エピソードの結果得られる状態遷移履歴から，各ステップでの将来に得られる累積報酬を計算し，その配列を返します．\n",
        "終了状態から遡ることで計算することでこれを簡単に計算できます．"
      ],
      "metadata": {
        "id": "bFaE6C9oSCjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REINFORCEアルゴリズム\n",
        "\n",
        "REINFORCEアルゴリズムは，価値のモンテカルロ推定を用いた方策勾配法です．\n",
        "REINFORCEにはいくつかのバリエーションがありますが，ここでは，エピソード単位で方策パラメータを更新する最もシンプルな方法を紹介します．\n",
        "（なお，状態遷移毎（ステップ毎）にパラメータを更新するバリエーションもあります．）\n",
        "\n",
        "まず，現在の方策を用いてEエピソード分だけ環境とインタラクションします．\n",
        "この結果から，エピソード内に訪問した各状態についての価値をモンテカルロ推定します．\n",
        "この推定価値を用いて，方策勾配を\n",
        "$$\\begin{aligned}\n",
        "\\nabla_\\theta J(\\theta)\n",
        "&= \\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi(a \\mid s)  \\right] \\\\\n",
        "&\\approx \\frac{1}{E}\\sum_{e=1}^{E}\\frac{1}{T_e}\\sum_{t=1}^{T_e} G_{e,t} \\nabla_{\\theta} \\ln \\pi(a_{e,t} \\mid s_{e,t}) =: \\widehat{\\nabla_\\theta J(\\theta) }\n",
        "\\end{aligned}\n",
        "$$\n",
        "と近似し，方策パラメータを$\\theta \\leftarrow \\theta + \\eta \\widehat{\\nabla_\\theta J(\\theta) }$に従って更新します．\n",
        "ここで，$\\eta > 0$は学習率を表し，問題や方策毎に調整が必要となります．\n",
        "また，状態，行動，累積報酬にはエピソードのインデックスが追加されており，終了ステップもエピソード毎に異なることに注意してください．"
      ],
      "metadata": {
        "id": "VGo279ppMl4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下では，REINFORCEアルゴリズムを実装しています．\n",
        "\n",
        "`select_action`は状態観測を受け取り，方策に従って行動を選択しています．\n",
        "実装上は，観測状態をPytorchのテンソル形式に変換し，これをActorに入力して行動確率確率を計算し，これに従って行動をサンプルしています．\n",
        "パラメータを更新する際に必要になる$\\ln \\pi(a \\mid s)$も出力しています．\n",
        "\n",
        "`rollout`は現在の方策を用いて1エピソード分，環境と状態遷移を繰り返し，その際の即時報酬列と$\\ln \\pi(a \\mid s)$列を返します．\n",
        "\n",
        "`update`はActorのパラメータを更新する関数です．\n",
        "ここでは，エピソード数は$E = 1$を前提としています．\n",
        "確率的傾斜法の理屈から，$E$を大きくして方策勾配の分散を小さくすることと，学習率$\\eta$を小さくしてパラメータ更新の分散を小さくすることには類似の効果があることが知られています．\n",
        "そのため，以下の議論では主に$E = 1$を前提としていきます．\n",
        "\n"
      ],
      "metadata": {
        "id": "XBpIvjyPXvPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReinforceAgent:\n",
        "    def __init__(self, env, model, lr=5e-4):\n",
        "        self.policy = model\n",
        "        self.env = env\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        return self.select_action(observation)[0]\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # 行動選択\n",
        "        observation_ = Variable(torch.Tensor(observation))\n",
        "        action_probs = self.policy(observation_)\n",
        "        log_probs = action_probs.log()\n",
        "        action = Categorical(action_probs).sample()\n",
        "        return action.data.cpu().numpy(), log_probs[action]\n",
        "\n",
        "    def rollout(self):\n",
        "        # 1 エピソード実行\n",
        "        observation, info = self.env.reset()\n",
        "        steps = 0\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        while not (terminated or truncated):\n",
        "            action, log_prob = self.select_action(observation)\n",
        "            next_observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            observation = next_observation\n",
        "            steps +=1\n",
        "        return rewards, log_probs\n",
        "\n",
        "    def update(self, rewards, log_probs):\n",
        "        # パラメータの更新\n",
        "        g_array = np.cumsum(np.array(rewards)[::-1])[::-1]\n",
        "        loss = - sum([g * lp for g, lp in zip(g_array, log_probs)]) / len(rewards)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "vKagOnIhjSFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`LunarLander-v2`環境で，REINFORCEアルゴリズムによる学習を実行してみます．\n",
        "比較的時間がかかりますので，注意してください．（30min程度）"
      ],
      "metadata": {
        "id": "ptJVsdesZOiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "env = gym.make(envname)\n",
        "\n",
        "model = Actor(dim_state = 8, num_action = 4)\n",
        "agent = ReinforceAgent(env, model)"
      ],
      "metadata": {
        "id": "upMhDE3cjSC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは，100エピソード毎に学習された方策を用いて得られる累積報酬の平均値と標準偏差を表示しています．"
      ],
      "metadata": {
        "id": "I2PEmHGOZpXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interval = 100\n",
        "returns = np.zeros((100, interval))\n",
        "\n",
        "for i in range(returns.shape[0]):\n",
        "    for j in range(returns.shape[1]):\n",
        "        rewards, log_probs = agent.rollout()\n",
        "        agent.update(rewards, log_probs)\n",
        "        returns[i, j] = np.sum(rewards)\n",
        "    print(interval * (i+1), np.mean(returns[i]), np.std(returns[i]))"
      ],
      "metadata": {
        "id": "V26zgXlbjSAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "累積報酬の移り変わりを可視化してみます．累積報酬の平均値は改善していく様子が見られると思います．ただ，比較的標準偏差が大きいことが確認できるでしょう．"
      ],
      "metadata": {
        "id": "f6dwfJlXaHTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = np.arange(1, 1+returns.size, returns.shape[1])\n",
        "avg = np.mean(returns, axis=1)\n",
        "std = np.std(returns, axis=1)\n",
        "plt.errorbar(episodes, avg, std, linestyle=':', marker='^')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "hqSNpXX_-3Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習後の経験分布関数を確認してみましょう．\n",
        "横軸は累積報酬を$-1$倍したものになりますので，小さい値が高い割合で得られるほど，望ましい方策が得られていると言えるでしょう．"
      ],
      "metadata": {
        "id": "a_paerEX87NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=agent, render=False)\n",
        "    return_array[i] = cumulative_reward(history)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "LFnPKD7v8IcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それでは学習結果を確認してみましょう．\n",
        "累積報酬の標準偏差が比較的大きいことが予想されますので，何度か実行し，確認してみましょう．"
      ],
      "metadata": {
        "id": "vOwLxBg_76NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=agent, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "pu_cbDuujR65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "前回のブラックボックス最適化法を用いた方策最適化に対して，方策勾配法は累積報酬だけでなく状態遷移の履歴を活用した方策最適化法になっています．しかし，あまり効率的になっている印象は持てないかもしれません．\n",
        "実際には方策モデルが異なるため今回の実験からは実行時間の比較ができません．\n",
        "また，強化学習では「期待累積報酬」を最大化しており，特定の乱数系列（特定の初期状態）のもとでの累積報酬最適化を行っていた前回の結果と比較するのはフェアではありません．\n",
        "\n",
        "ただ，上記のREINFORCEが効率的でないことも事実です．\n",
        "一つの原因として，「方策勾配は推定分散が大きい」ことが挙げられます．\n",
        "REINFORCEでは，エピソード中に得られた累積報酬を用いて価値関数を近似し，これを用いて方策勾配を近似しています．\n",
        "しかし，実行結果を見てもわかるように，累積報酬の値は大きな分散を持っているため，これを用いて推定される方策勾配も大きな分散を持ってしまいます．\n",
        "方策勾配の分散が大きい場合，これに伴って学習率を小さくしなければならず，学習が遅くなります．"
      ],
      "metadata": {
        "id": "Qdn9vkX0_nMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ベースラインの導入による方策勾配の推定分散削減\n",
        "\n",
        "方策勾配の推定分散を削減するために，方策勾配の式を見直してみましょう．\n",
        "方策勾配は以下の式を近似しています．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_s \\mathrm{E}_a \\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_s \\left[ \\sum_{a} Q^{\\pi}(s, a) \\pi_\\theta(a \\mid s)  \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_s \\left[ \\sum_{a} Q^{\\pi}(s, a) \\nabla_{\\theta} \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "この式を修正してみます．まず，ある関数$b(s)$について，\n",
        "$$\n",
        "\\sum_{a} b(s) \\nabla_{\\theta} \\pi_\\theta(a \\mid s)\n",
        "=\n",
        "b(s) \\sum_{a} \\nabla_{\\theta} \\pi_\\theta(a \\mid s)\n",
        "=\n",
        "b(s) \\nabla_{\\theta} \\sum_{a} \\pi_\\theta(a \\mid s)\n",
        "=\n",
        "b(s) \\nabla_{\\theta} 1\n",
        "= 0\n",
        "$$\n",
        "が成り立つことに注意すれば，上の式は，\n",
        "$$\\begin{aligned}\n",
        "\\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "&= \\mathrm{E}_s \\left[ \\sum_{a} \\left( Q^{\\pi}(s, a) - b(s) \\right) \\nabla_{\\theta} \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_{s,a} \\left[ \\left( Q^{\\pi}(s, a) - b(s) \\right) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "と書き直すことができることがわかると思います．\n",
        "すなわち，行動価値関数$Q^{\\pi}(s, a)$から，状態のみに依存する任意の関数$b(s)$を引いたとしても，この期待値は変化しないことがわかります．\n",
        "しかし，これを推定する場合，\n",
        "$$\n",
        "\\left( q(s, a) - b(s) \\right) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)\n",
        "$$\n",
        "の期待値は上の議論から代わりませんが，その推定分散は$b(s)$に依存することになります．\n",
        "そこで，この$b(s)$を工夫することで，推定量分散を小さくする試みがあります．\n",
        "\n",
        "代表的な$b(s)$の選択肢は，行動価値関数の行動についての期待値，すなわち状態価値関数です．\n",
        "行動価値から状態価値を引いた値はアドバンテージなどと呼ばれ，しばしば強化学習の文脈で現れる量になります．\n",
        "状態価値をベースラインとして利用することは，必ずしも最適ではありませんが，合理的と考えられる理由があります．これについて簡単に考察してみましょう．\n",
        "まず，$\\nabla \\ln \\pi(a \\mid s)$に着目します．\n",
        "これは，$\\ln \\pi(a \\mid s)$の勾配ですから，状態$s$で行動$a$を選択する確率が上昇する方向にパラメータが更新されます．\n",
        "上のREINFORCEでは，エピソード中に実際に観測した状態$s_t$とそこで選択された行動$a_t$について，$\\ln \\pi(a_t \\mid s_t)$に累積報酬を乗じた方向にパラメータが更新されています．\n",
        "ここで，Lunar-Landerの例に着目すると，エピソード中を通してほとんどの場合に累積報酬が負の値になっていることがわかると思います．\n",
        "このとき，どのような行動を取ったとしても，選択された行動の選択確率を下げる方向にパラメータは更新されることになります．\n",
        "これでは効率が悪い更新になっていると想像されます．\n",
        "期待値を累積報酬から減ずることにより，その状態で得られる累積報酬の期待値よりも高い価値を持つ行動については行動選択確率を上げ，逆に価値の低い行動については行動選択確率を下げる，といった更新が可能になります．\n",
        "（なお，常に行動選択確率を下げるにもかかわらず方策が収束していくのは，その下げ幅が報酬の大小関係により異なるため，長い目でみれば報酬の高い行動の選択確率が相対的に上がることになるためである）\n",
        "\n"
      ],
      "metadata": {
        "id": "lcx6bQC0V0s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ベースラインを導入したREINFORCEアルゴリズム\n",
        "\n",
        "それでは状態価値をベースラインとしたREINFORCEアルゴリズムを実装してみましょう．\n",
        "追加で必要なコンポーネントは，状態価値の推定部分です．\n",
        "状態価値は，各状態でその先に得られる累積報酬の期待値で与えられます．\n",
        "そこで，状態価値を近似する関数モデルを$b_\\phi(s)$としたとき，1エピソード中の状態遷移履歴を用いて，\n",
        "$$\n",
        "L(\\phi) = \\frac{1}{T}\\sum_{t=1}^{T} (G_t - b_{\\phi}(s_{t-1}))^2\n",
        "$$\n",
        "を最小化するように$\\phi$を学習することが考えられます．"
      ],
      "metadata": {
        "id": "54PYDEl1Fo8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下では，ベースラインを計算するネットワークを作成しています．\n",
        "ここでもActor-Criticにならい，ベースラインをCriticと呼ぶことにします．\n",
        "\n",
        "CriticのアーキテクチャはActorと同様とします．\n",
        "異なる点は，出力が1次元であり，出力には活性化関数を設けないという点です．"
      ],
      "metadata": {
        "id": "IK-B4miwa4IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical"
      ],
      "metadata": {
        "id": "hDnqAPt1jSJh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, dim_state, num_action, dim_hidden=16):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_state, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, dim_hidden)\n",
        "        self.fc3 = nn.Linear(dim_hidden, dim_hidden)\n",
        "        self.fc4 = nn.Linear(dim_hidden, num_action)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.softmax(self.fc4(x), dim=0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3d4NDkjPjSHG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, dim_state, dim_hidden=16):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_state, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, dim_hidden)\n",
        "        self.fc3 = nn.Linear(dim_hidden, dim_hidden)\n",
        "        self.fc4 = nn.Linear(dim_hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eQFnydEz8ovf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベースラインを導入したREINFORCEアルゴリズムを実装します．\n",
        "`update`関数において，criticを更新している点が前述のREINFORCEとの差分です．"
      ],
      "metadata": {
        "id": "ni4vEzqDba5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCriticAgent:\n",
        "    def __init__(self, env, actor, critic, lr_a=5e-4, lr_c=1e-2):\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.env = env\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_a)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_c)\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        return self.select_action(observation)[0]\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # 行動選択\n",
        "        observation_ = Variable(torch.Tensor(observation))\n",
        "        action_probs = self.actor(observation_)\n",
        "        log_probs = action_probs.log()\n",
        "        action = Categorical(action_probs).sample()\n",
        "        return action.data.cpu().numpy(), log_probs[action]\n",
        "\n",
        "    def rollout_with_update(self):\n",
        "        # 1 エピソード実行\n",
        "        observation, info = self.env.reset()\n",
        "        steps = 0\n",
        "        rewards = []\n",
        "        list_loss_a = []\n",
        "        list_loss_c = []\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        while not (terminated or truncated):\n",
        "            action, log_prob = self.select_action(observation)\n",
        "            next_observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "            loss_a, loss_c = self.update(observation, next_observation, reward, terminated, log_prob)\n",
        "            rewards.append(reward)\n",
        "            list_loss_a.append(loss_a.detach().numpy())\n",
        "            list_loss_c.append(loss_c.detach().numpy())\n",
        "            observation = next_observation\n",
        "            steps +=1\n",
        "        return rewards, list_loss_a, list_loss_c\n",
        "\n",
        "    def update(self, observation, next_observation, reward, terminated, log_prob):\n",
        "        vtt = reward + (1 - int(terminated)) * self.critic(torch.Tensor(next_observation)).detach().numpy()[0]\n",
        "        vt = self.critic(torch.Tensor(observation))[0]\n",
        "\n",
        "        # Actor の更新\n",
        "        loss_a = - (vtt - vt.detach().numpy()) * log_prob\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        loss_a.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Critic の更新\n",
        "        loss_c = (vtt - vt)**2\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        loss_c.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        return loss_a, loss_c"
      ],
      "metadata": {
        "id": "GeCzR9UQjR0b"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "env = gym.make(envname)\n",
        "\n",
        "actor = Actor(dim_state = 8, num_action = 4)\n",
        "critic = Critic(dim_state = 8)\n",
        "agent = ActorCriticAgent(env, actor, critic)"
      ],
      "metadata": {
        "id": "4cJI3P6pAT9n"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interval = 100\n",
        "returns = np.zeros((100, interval))\n",
        "\n",
        "for i in range(returns.shape[0]):\n",
        "    for j in range(returns.shape[1]):\n",
        "        rewards, loss_a, loss_c = agent.rollout_with_update()\n",
        "        returns[i, j] = np.sum(rewards)\n",
        "        print(returns[i, j], sum(loss_a), sum(loss_c))\n",
        "    print(interval * (i+1), np.mean(returns[i]), np.std(returns[i]))"
      ],
      "metadata": {
        "id": "lHdMuxUQAife",
        "outputId": "2205bbbd-9da1-4f93-dbc8-7acd5dfd9ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-93.69389723350965 -133.8135915119201 11499.024426620192\n",
            "-53.53797691965568 -67.61296446435153 16603.281419705876\n",
            "-92.2054365159832 -173.98224757611752 10394.305929607712\n",
            "-60.30367109586537 -90.77556121349335 10414.630731440382\n",
            "-56.312859745047284 -81.35229056328535 10462.947722631972\n",
            "-77.15758267988713 -138.82317633461207 10122.798050259993\n",
            "-204.1038865893981 -47.037994630634785 4393.424312255811\n",
            "-281.8205716530594 10908.995046175085 117854093.8253138\n",
            "-505.90765038004383 2171.8576632738113 234036.68336224556\n",
            "42.79764311408849 2391.9947862029076 1242708.8751535146\n",
            "-342.11675195830816 537.0705053601414 137726.11949696438\n",
            "-471.26212017378606 165.79819011688232 173007.41050067917\n",
            "-629.5479984358869 1117.4157463014126 399717.87916311366\n",
            "-535.2323443601916 -397.40812216326594 139789.87836812437\n",
            "-414.2262992910806 34.745593786239624 60971.76960581541\n",
            "-66.31912308516215 326.7153665609658 17757.167155743577\n",
            "-165.66541185522954 11.579835869371891 19602.324987099506\n",
            "-71.05345756999809 121.63401180505753 5881.297003362328\n",
            "-442.8631620845864 -163.43343584239483 43009.647881368175\n",
            "-247.58973774473063 10697.914891719818 38403673.81944154\n",
            "-618.1544941512817 247.65272296220064 183923.73345878907\n",
            "-244.40261710475926 701.2011084444821 121830.2309528673\n",
            "-546.8362056442156 389.375015322119 262962.23312543053\n",
            "-102.59089458879741 256.67344925925136 147228.88750534132\n",
            "-577.8034745826028 14.484961181879044 163927.5683711404\n",
            "-340.6317010845187 88.54624342545867 153691.89638989232\n",
            "-340.8157943911951 -129.16048981249332 109665.55636919197\n",
            "-95.80459630196847 244.9722135066986 56210.95302993059\n",
            "-608.1244990306814 -328.6310781314969 137920.43696586415\n",
            "-424.3137772149751 335.5518736243248 51898.78749802336\n",
            "-496.8402554313103 -12.482096165418625 129564.89964184258\n",
            "-367.57591461989784 253.72570438683033 53051.84442759957\n",
            "-580.1302322678198 -206.51360447052866 214235.45747343078\n",
            "-615.3443439921431 -261.4721131026745 90001.20296225045\n",
            "-398.01161143913214 -23.0236236192286 43009.97309168335\n",
            "-485.62256743981055 -189.6908494643867 56567.95692349225\n",
            "-329.1047676051322 26.994178533554077 24451.72357866168\n",
            "-215.6002523779061 528.4104011841118 47119.692004371434\n",
            "-377.83385955167876 -56.96069988980889 98837.57841613144\n",
            "-437.6287426597826 -174.0193809568882 35142.28701493703\n",
            "-312.6741566951023 353.92266961559653 38230.30742044491\n",
            "-383.27810400493684 -29.227898001670837 24138.230807334185\n",
            "-482.2502178547266 147.3595518618822 36308.01494256407\n",
            "-349.5940547333005 -17.36021826020442 26220.884891308844\n",
            "-427.73382210578393 -102.34884765651077 30500.622153294273\n",
            "-656.9339775481582 -322.8132186476141 107296.74758881424\n",
            "-583.336791469379 -212.53041234612465 67726.2868097024\n",
            "31.783257985568362 304.43510644487105 1114141.4223007557\n",
            "-688.5354401178997 -243.26010323339142 127139.65546477214\n",
            "-262.6752508422726 264.2873560115695 10723.870246031322\n",
            "-570.1435321381327 -173.11154794692993 139600.40646994114\n",
            "-453.2154190898735 -29.03898185491562 83394.05796790775\n",
            "-543.8090880194101 170.09931054711342 39003.63477664068\n",
            "-241.54053325522614 104.7123792283237 25125.20403373055\n",
            "-418.8472660625588 -136.27671706676483 37869.723072947\n",
            "-136.28069565985064 1257.553099754732 689758.0802239077\n",
            "-95.66172094952091 288.9013568684459 2440.040218744427\n",
            "-416.51577796771477 -142.6894600391388 15705.685048299842\n",
            "-101.10080790487089 201.58394569717348 2559.5686554908752\n",
            "-340.7644616826352 -72.59734343737364 19115.305344513617\n",
            "-733.5620040538953 -426.8575102314353 44405.51602894813\n",
            "-502.79078608480745 -74.1937307715416 209521.3766694069\n",
            "-330.8163866466025 403.72561623714864 33979.93117460422\n",
            "-371.1407256099995 -61.90345522761345 8433.898887179792\n",
            "-461.83555100526326 367.49792850762606 80693.31529884133\n",
            "-275.4235292735063 114.29023532569408 2296.2338206935674\n",
            "-88.37756071648822 238.86999695375562 2852.266211790964\n",
            "-5.24652024971769 334.74218928813934 9973.24091534625\n",
            "-217.14377855405195 -18.411448592320085 2074.6547557387967\n",
            "-603.5946344144568 -335.1708386167884 12361.95914278971\n",
            "-349.12016954959677 -61.529245391488075 11907.462167830206\n",
            "-137.79581268740654 128.59642436611466 2844.6385388253257\n",
            "-246.08072505244314 -21.478123361244798 4220.310088164173\n",
            "-87.61775471860994 104.73281081067398 1118.0349736073986\n",
            "-524.6773125974662 -268.191200315021 8539.89765956346\n",
            "-652.43482652566 -238.73134349286556 44719.3471121192\n",
            "-105.62048841140343 178.10689693316817 1395.8116147334222\n",
            "-793.9167728219155 -116.087905571796 19395.620776042342\n",
            "-497.96621352211037 276.67927794903517 140246.5319835879\n",
            "-412.10072651143213 -61.81042654812336 2890.5247546928003\n",
            "-377.9102012435005 -53.781580328941345 4059.186915894039\n",
            "-409.190415360501 -127.27192195132375 2281.798306746641\n",
            "-502.5923591833292 -224.93966705724597 2375.3716225391254\n",
            "-468.74745371612937 -175.24347990751266 34227.229081671685\n",
            "-490.5984931200342 -197.02548282721546 11217.13165269047\n",
            "-343.3569314509171 -14.248861819505692 1683.068018997088\n",
            "-649.5600212467467 -341.73585951142013 13783.14070434682\n",
            "-479.6469448480039 -52.7313222438097 27796.133429445326\n",
            "-448.11131966624856 -60.92748076678254 10857.05702997651\n",
            "-475.3189192872216 -71.69555132091045 18641.41929014586\n",
            "-601.3151207997378 132.7737568616867 351249.3034069538\n",
            "-723.059246376605 -147.2437284886837 46151.60907873884\n",
            "-645.5087846439177 -62.014101097593084 22160.93211854156\n",
            "-578.1032749057243 -28.891581531614065 29761.329999522306\n",
            "-341.4092399479774 70.24464341253042 5737.779589984566\n",
            "-314.429506764336 56.08425160497427 11527.048315962777\n",
            "-792.3297089218314 -125.84507662057877 42904.82943639159\n",
            "-785.3775524680217 17.555695965886116 4445.536839149892\n",
            "-291.5206191145397 -92.48648054874502 6088.287080075592\n",
            "-453.96979323386057 54.49683325923979 46519.999080715934\n",
            "100 -379.7371894315873 203.58117376977333\n",
            "-673.3767191156575 47.51152598299086 29955.355892967433\n",
            "-310.4624911924778 15.718812642153352 4144.725832406431\n",
            "-716.2534787070737 -246.7192941121757 2321.490110841114\n",
            "-536.8618570853428 -7.054272182285786 28316.51981798932\n",
            "-507.1950755960354 -5.174379186704755 2176.9002971686423\n",
            "-993.1787157952292 -263.42569801211357 14227.797486853786\n",
            "-489.5587659312445 310.0615471601486 7261.190852016211\n",
            "-387.08987806351814 -7.283350810408592 8581.4855164662\n",
            "-716.1638096632918 -2.0707084257155657 27265.370989409275\n",
            "-539.4914387724349 213.4730513393879 17678.010664083064\n",
            "-347.1020244698753 7.989012472331524 9230.073988670483\n",
            "-751.2756341835487 -38.67456298135221 30220.321236985736\n",
            "-318.7672637871936 130.69822774082422 2359.877142199315\n",
            "-531.6402412890326 56.35968863964081 47007.34130712226\n",
            "-727.4345221015063 -31.511919252574444 3270.911588128656\n",
            "-416.96888818783356 208.14186657965183 2151.1591242551804\n",
            "-509.68761587661794 -59.83054453879595 957.3855127515271\n",
            "-130.6330912604953 240.90751213021576 1370.8543462965754\n",
            "-150.72294052843247 53.46718956530094 1536.7704940198455\n",
            "-567.5390485272419 -172.97406853735447 2824.6297467574477\n",
            "-376.62854725838275 25.840633273124695 979.1479878127575\n",
            "-62.42243501518588 282.84735939092934 6481.009288690286\n",
            "-160.4702811626954 -62.386773243546486 4859.319673237391\n",
            "-190.3318123383408 7058.506950899959 40961630.83748014\n",
            "-117.97809641182785 600.684637889266 160778.7959813699\n",
            "-128.00142309806623 -74.3488906249404 74337.14358621562\n",
            "-675.2432913497528 -393.4295726418495 4519.589097857475\n",
            "-489.16764293834706 -102.33242702484131 4465.612260259688\n",
            "-481.3108324225359 -105.75764288380742 2093.8477757973596\n",
            "-472.0804342052651 -150.7584687527269 18400.683113577543\n",
            "-590.426107421008 -87.9351519048214 8519.4595842259\n",
            "-309.14043496502154 144.02283163368702 5173.019978083903\n",
            "-317.5079472574605 190.9730059877038 1497.2418347438797\n",
            "-370.8672422501292 185.01797759905457 3527.397091899067\n",
            "-124.14005460485184 19.8130364716053 8707.10422776267\n",
            "-471.5503571303622 34.934168100357056 2849.465603623539\n",
            "-367.81040515605036 -54.069538303650916 20045.272383788833\n",
            "-497.512223069866 -10.35251646861434 2260.4014446418732\n",
            "-393.14680754255176 140.40920856734738 1156.3382887989283\n",
            "-266.0958096296659 86.24037978239357 9381.109420552908\n",
            "-167.9242255566823 166.41945871710777 6091.406215217896\n",
            "-422.1888479109327 -82.61073008179665 1005.5794930756092\n",
            "-142.60021437325446 -621.836578309536 85129.03842238197\n",
            "-545.235459038563 60.22844536602497 4341.5992319993675\n",
            "-609.3624450634998 -71.9847522675991 4619.376663032919\n",
            "-347.84204888510794 114.88655937463045 1244.8276122957468\n",
            "-274.6021361271921 118.52679352834821 5089.447010179982\n",
            "-197.7273815706825 182.62241556681693 11778.42968329048\n",
            "-288.82265879402064 23.707153941504657 14084.739615173836\n",
            "-507.1147298242254 -127.57003340125084 2212.036936567165\n",
            "-254.2737416865907 -27.02734107337892 17792.696951299906\n",
            "-390.13759517791965 -64.92164209485054 2456.2793719572946\n",
            "-233.15224736371923 83.61972019821405 1382.3337575607002\n",
            "-424.9127451773556 -192.07980525493622 2589.764038405381\n",
            "-346.8594202412645 47.29719553887844 2440.4075507475063\n",
            "-127.59521598118883 168.90974043309689 8551.290525643155\n",
            "-168.5804736109391 -203.7892481982708 195388.46000163257\n",
            "-484.25452212401734 -45.958892703056335 8950.686492582783\n",
            "-350.8477669880971 -90.02916811592877 7276.210132841254\n",
            "-214.5248868146649 19.46991328150034 24915.515184270218\n",
            "-130.9850488198265 172.83639249950647 1970.8314285203815\n",
            "-291.18358474677973 -83.72736267000437 78393.70336279646\n",
            "-346.58377543913537 -18.99723570048809 2702.0685788374394\n",
            "-390.10554585841584 -79.06622752547264 5329.443676844239\n",
            "-145.83488215214135 147.35174145177007 981.5648810770363\n",
            "-211.76555692203544 73.05997601523995 314.4753545380663\n",
            "-221.17782247512244 45.73477266356349 1133.216785961995\n",
            "-134.7053723102961 137.04242196679115 14629.76959872339\n",
            "-175.24838701954454 -36.472301923669875 7461.531982236076\n",
            "-139.43558006004557 76.86874312907457 3231.2921121625695\n",
            "-183.88498494269285 57.781595543026924 2577.1828193070833\n",
            "-130.06749566701993 113.48288950324059 3185.3739956861828\n",
            "-186.5347789792895 -2811.6182767786086 20521092.315425176\n",
            "-146.9119796042262 43.90385517477989 9107.314807774965\n",
            "-149.0521068409999 -130.0127510689199 11281.433484457899\n",
            "-210.46374152079156 358.7716609761119 8997832.112610037\n",
            "-62.66243464408413 75.83458425942808 36158.35383924615\n",
            "-120.95649639360703 -35.725319385528564 12966.840210407972\n",
            "-283.45358188632304 58.36606051772833 372458.7256192621\n",
            "-550.646200556152 52.55912570655346 4699598.405584753\n",
            "-221.52787462118107 259.6553815305233 628977.0688515455\n",
            "-141.88101870369573 549.2710412144661 54422.91943529993\n",
            "-24.811414965030536 285.5729349851608 56883.473510339856\n",
            "-106.10704172706448 175.12007362954319 37549.53796733788\n",
            "-496.6183289427096 594.6299585551023 1155207.0069716275\n",
            "-122.02875525668605 151.94796280562878 21205.64975352306\n",
            "-388.03314922035725 63.76166534423828 285110.84385780245\n",
            "-342.7145571507591 343.5184025615454 371706.2212042101\n",
            "-343.03457453000965 282.1512202322483 214897.70412864164\n",
            "-163.11266637496834 82.68170726299286 21773.40505291242\n",
            "-133.0626414847585 157.2856806218624 16073.998169459403\n",
            "-120.3568424193107 233.9088471394498 4856.265668707201\n",
            "-262.819339596627 95.50812225788832 109999.62934759725\n",
            "-326.44732674317925 456.4578665494919 70218.7047248818\n",
            "-141.70636821110585 292.3745515048504 4038.3716062754393\n",
            "-99.10182749336253 165.22018060088158 4310.710583975073\n",
            "-301.90203113711533 -179.41618524491787 11940.200762394816\n",
            "-128.68662812170726 44.52444232068956 10440.833807774645\n",
            "-138.6846486246763 4.238129373639822 3891.875067414716\n",
            "-185.38663786416276 -72.68683262169361 3974.6928678704426\n",
            "200 -320.8147747769836 189.34525175852895\n",
            "-119.01303406398398 -88.64420023560524 14638.442705937254\n",
            "32.24495870884812 195.62902653217316 26237.409897157457\n",
            "-161.74220792590177 -125.51667924773938 15147.932265429059\n",
            "-108.4020184036613 -78.89439663477242 5857.1563872435145\n",
            "-154.31075393506472 -78.52658979501575 2320.035640008253\n",
            "-232.5221582974806 -239.7033309340477 23799.34950219607\n",
            "-116.68086876221591 17.626208901405334 4798.736748103052\n",
            "-293.00063054660006 -204.0582638680935 136418.4477597077\n",
            "-104.90291748810374 -45.945561587810516 11692.330637305975\n",
            "-156.1035541226382 -17.784462571144104 3181.4395602196455\n",
            "-240.37894901108365 -165.37065096199512 154989.99743853137\n",
            "-162.74880324841098 4.328757897019386 4955.443644812098\n",
            "-133.32003688486347 45.61651285737753 10958.278663102537\n",
            "-153.27975176058413 88.11328771989793 13702.210827240953\n",
            "-170.40491867101304 77.59275872819126 3807.7781284891535\n",
            "-163.7863399386352 -31.7166787981987 1833.977470141137\n",
            "-225.63399354717248 -68.21761578321457 8130.532312229276\n",
            "-128.2778001765876 60.23050497844815 4594.936582505063\n",
            "-394.3549655379536 -130.67538633570075 10932.709277487593\n",
            "-142.4375208606417 21.096996873617172 20460.544637315208\n",
            "-369.171007093218 2.1957084387540817 71815.56393916719\n",
            "-223.46814077929469 -66.48780342936516 8156.856592841446\n",
            "-89.01880817127582 116.73083038628101 4860.201601745561\n",
            "-133.14625012758614 43.08317165262997 2694.7796354471357\n",
            "-158.99408693833567 1.9019574522972107 3579.171087462455\n",
            "-105.06692907198028 -27.74923848360777 12105.300872314721\n",
            "-169.13630133146847 -103.32835631072521 3724.244447243167\n",
            "-192.04917044784696 -56.8738057911396 1875.8051075562835\n",
            "-148.22343437780725 16.734655678272247 3755.0129979460035\n",
            "-121.47544240672872 23.01450027525425 1787.9360630847514\n",
            "-177.52162086380702 -38.67032970301807 7596.482873982051\n",
            "-2.9794659782573234 234.67386020720005 24666.012515012175\n",
            "-18.943842000203034 181.6076684826985 20533.78975860076\n",
            "-153.79285118465688 3.762843425385654 3118.3352440892486\n",
            "-107.82454243897502 49.417082354426384 2256.279742348939\n",
            "-249.89544856398115 -57.762019189074636 6039.892989063228\n",
            "37.647381765936075 318.72745103389025 26388.524871833622\n",
            "-144.127773098153 4.108685910701752 5641.575437782856\n",
            "-36.379916727088656 192.1179082468152 25095.38551889453\n",
            "-122.91260768890677 28.977897614240646 4450.968079225626\n",
            "-75.64974946081885 96.62511644139886 7757.509095066343\n",
            "-113.01375861255825 1.0888875564560294 9631.084332606304\n",
            "-198.10229792078133 -100.41599494591355 2035.0214618394384\n",
            "-226.7396576760337 -150.06943253427744 3220.837986584753\n",
            "-157.20376937945252 -41.92599430773407 3562.2892653162125\n",
            "-87.78028505122707 42.026910938322544 2197.1844554059207\n",
            "-115.5375097028513 -0.4033596068620682 9981.234664354473\n",
            "-430.3065508775663 -3.1772835850715637 4525.417394593358\n",
            "-100.54520604694805 251.3853179961443 9944.136191606522\n",
            "-125.49990967676942 40.68529972212855 2421.9455889130477\n",
            "-97.98939977292352 88.25966726616025 8482.931052887347\n",
            "-198.01799135169907 -74.54679227620363 16138.023261732422\n",
            "22.040154766788348 213.0260718241334 11339.149853116367\n",
            "-340.3303350083228 215.07505767047405 360736.0552770216\n",
            "-100.74179611565364 -29.07226742617786 5581.587958659045\n",
            "-142.09710511841766 -0.26429842971265316 8683.808299634547\n",
            "-47.61077576097652 145.93159545958042 12138.091433646157\n",
            "-247.88637956634736 -110.2762122452259 5137.226783877239\n",
            "-284.62443639570984 -79.3959400895983 20640.107922071125\n",
            "-117.09222317462877 -1.4641407579183578 3201.4341029305942\n",
            "-104.6983787331478 16.172524102032185 3005.1153828829993\n",
            "-234.60367500785338 -163.91750180721283 14892.283701375127\n",
            "-114.46255725335513 -31.478061634581536 7581.601458602876\n",
            "-11.064243387018792 121.88100124895573 4977.171892520972\n",
            "-367.5248643010906 33.60586475720629 330276.71360017685\n",
            "-129.59307909474 23.360019916668534 9022.39700057765\n",
            "-352.92232957145586 -225.87580743432045 3307.020256705582\n",
            "-184.98941223598237 -163.23886197805405 9731.472647316754\n",
            "-385.62751024675714 -411.6122026219964 14023.420401180163\n",
            "-159.2641255925961 -69.95119344908744 10836.150123087456\n",
            "-89.21034935307918 -0.36304832249879837 7479.835123507306\n",
            "-116.90115902607495 -13.148074717260897 9166.806841085665\n",
            "-156.0361542575507 -126.16985439881682 9041.829334221547\n",
            "-488.55298727149705 -326.8405347885564 1941.8440807634033\n",
            "-248.89357071291693 117.8721548691392 68646.9251973799\n",
            "-401.5072618517056 -158.30922688147984 1046.3546409613919\n",
            "-133.07473771442753 11.371788766235113 1447.6169605602045\n",
            "-105.48039493483688 11.882118028588593 4197.211620653747\n",
            "-153.39225665465415 45.85781279578805 10958.412295260234\n",
            "-381.5289153479778 -250.65368656814098 6106.717424780363\n",
            "-110.16297435483152 81.66879126057029 2282.1148472796194\n",
            "-370.33323834225575 -263.772406257689 136562.49146684638\n",
            "-105.59251725521808 3.7225120663642883 3013.5856069996953\n",
            "-207.09312918475308 -79.8208852885291 14850.110097612545\n",
            "-83.79512210342534 -38.8513801721856 5064.649506618458\n",
            "-200.8907573175925 -123.55564946681261 20455.964862723195\n",
            "-120.48273721435328 -71.02438586950302 1072.216613471508\n",
            "-305.20164419157595 -306.4716605618596 1150.6709060613066\n",
            "-168.37628570269794 -87.38834389485419 1620.743039863708\n",
            "-151.4922801834454 -57.27297502756119 2143.3031344870105\n",
            "-273.7128541288027 -243.64476938545704 8225.483266068622\n",
            "-172.23349235847712 -53.00864287931472 1614.7637028387398\n",
            "-253.2994998576479 -103.84324419125915 13577.112919077277\n",
            "-129.96532728265424 -48.20179561525583 3785.6017530062236\n",
            "-202.08487429175761 -131.66399230994284 3851.6785149278585\n",
            "-136.31982001721423 -34.20173549000174 981.6530917690252\n",
            "-130.33587466372973 -24.051179982721806 1241.7952349213883\n",
            "-195.38346807470828 -155.16441097017378 14903.560868009707\n",
            "-116.21655484163489 17.6138904877007 1079.071244280145\n",
            "-219.83264590717897 -73.31082808971405 4082.0150939002633\n",
            "300 -171.72398563722953 101.06280482691693\n",
            "-120.86181135766418 10.49698719382286 1637.4111667957623\n",
            "-469.64073714082724 -337.03023121505976 4903.47831939999\n",
            "-138.77579158027663 64.92796650528908 2425.2267620414495\n",
            "-130.86539106381673 54.80804415605962 2118.2496351350565\n",
            "-288.3948994779857 -157.97736716270447 1680.0383326336741\n",
            "-104.92325825995457 62.841235507279634 499.3655598377809\n",
            "-116.42724609744366 80.11570709943771 2336.5561006391654\n",
            "-376.9372980708133 -135.13393174111843 4298.647774353623\n",
            "-94.03134737981445 131.3362354002893 1518.9051797830034\n",
            "-131.60417064603192 57.43218430108391 900.771972327726\n",
            "-381.2973171135413 -240.22679691016674 270198.3691696152\n",
            "-307.2531940847559 -152.0444630123675 1496.8054173965938\n",
            "-106.29690949102418 63.92132338904776 1316.6154790227301\n",
            "-235.5466350754742 -133.4554966390133 2719.7887310120277\n",
            "-84.45209966118409 26.21079647913575 2658.3414303963073\n",
            "-129.6946549981667 -27.706641759723425 1099.2956910450011\n",
            "-153.82268892786027 -50.516868961043656 1137.355306894984\n",
            "-105.38386830650776 41.330875255167484 896.5483627412468\n",
            "-232.43785310927956 -88.6143901720643 2991.6343272097874\n",
            "-180.80589230783752 -55.68636158853769 1544.516469788039\n",
            "-105.73654310323603 24.392962783575058 1619.2141564977355\n",
            "-68.16533971161778 60.72624788060784 3950.8045681805816\n",
            "-219.6318871159945 -196.40793474018574 1836.6023036115803\n",
            "-381.25113724751816 127.68717575445771 44838.362757925\n",
            "-285.27477452685986 -276.4308963306248 26580.75579963997\n",
            "-68.40561025042295 78.1212586518377 2433.1513690255233\n",
            "-216.44460360118845 -107.43937740847468 2750.788562377449\n",
            "-149.9027002107739 9.54903507232666 1555.236056121299\n",
            "-152.62973012856435 16.655952589586377 1160.3037977887434\n",
            "-160.7941050407723 90.51840568520129 3201.648787550861\n",
            "-99.40240840388354 9.53947351872921 1709.4677687285584\n",
            "-92.77724050211009 -3.5820844247937202 1953.700028022984\n",
            "-257.220909121811 -176.7755970461294 2090.899688903766\n",
            "-327.1049930550795 -184.44805173762143 18780.138704642886\n",
            "-128.44469047153518 5.035354647785425 1013.457191955822\n",
            "-306.18132703621563 -168.07810674607754 1570.5617747949436\n",
            "-213.66010016401418 140.05040036444552 29831.25909134955\n",
            "-253.77973968286017 -121.42058656923473 57767.234546112886\n",
            "-199.47208407865622 -115.4843315500766 608.450153806014\n",
            "-317.0344041349763 -116.52058784663677 712.0298273274675\n",
            "-86.50151311487011 79.29638563422486 2963.7108059395687\n",
            "-263.70946867646444 -17.536355290561914 2034.724717164936\n",
            "-135.70507218467228 45.23488559527323 764.0588256146293\n",
            "-59.89563908143761 70.07392437011003 2091.2941855806857\n",
            "-92.08438127440537 -23.40965054911794 2898.4628598724958\n",
            "-153.0272273307721 -76.62531531974673 1863.882147888653\n",
            "-164.64606941287337 -45.89327894477174 1638.4853820845601\n",
            "-77.39081336398628 74.99551053531468 2260.627126092615\n",
            "-340.0603898983681 124.95461786910892 51414.24554357119\n",
            "-95.15742841342659 -6.688957823906094 4552.006981710962\n",
            "-98.72675562266396 -53.64407690055668 2105.696135289094\n",
            "-45.793701023323806 22.037935994565487 3535.999636025168\n",
            "-47.95875534422805 59.845637106802315 1907.5331173497252\n",
            "-130.4495137800131 -53.673204228281975 2608.132910198765\n",
            "-260.51977593985623 -239.48774895910174 4639.8252994928625\n",
            "5.407989607419523 88.84749198239297 7060.154931234196\n",
            "-118.49503755456428 -154.20569312386215 4426.022374466993\n",
            "-129.06995509898522 12.928860556334257 1975.18931693607\n",
            "-232.21813018783735 -93.90889719687402 3207.454072823748\n",
            "-205.70497298753787 -120.39533068612218 2815.16864056146\n",
            "-69.67019141680244 80.19566418230534 5262.678319157101\n",
            "-34.217117809281206 -14.832476898562163 6120.559220603507\n",
            "-83.31218504083496 -5.065869137644768 5562.79117787024\n",
            "-237.41640475456404 -34.432618773542345 6425.015626855689\n",
            "-151.38107409338352 -360.7507175579667 17120.524435248226\n",
            "-168.38167762334098 -117.73527897149324 1142.4649818725884\n",
            "-32.83067434441301 -29.349139228463173 3533.5532788089477\n",
            "-248.789369840637 -281.0301557276398 19411.19736873894\n",
            "-11.933133538438206 -44.90665396489203 1036.1760078789666\n",
            "-78.8096962973705 -160.05520953051746 2327.2285859156545\n",
            "-308.39706898157664 -277.3894431544468 4434.769534720515\n",
            "-242.58143240850893 -298.0480710552074 50662.42283464089\n",
            "-152.69350003812968 -147.39173697680235 2441.842490031384\n",
            "-39.80505667977975 -72.74912723526359 2005.9007215420716\n",
            "-35.79107223361776 -91.28550014458597 3192.8748408704996\n",
            "-622.4814519965367 -276.1858967891894 7493.49178322457\n",
            "-119.38398833764776 167.06683543222607 20232.097732719034\n",
            "-384.40966918247364 -149.2405660743534 1199.5414652721956\n",
            "-170.52151356059352 14.929760862141848 3523.2282596412115\n",
            "-285.0634406141903 -64.94683268293738 37583.33578031603\n",
            "-218.4793672863329 -20.14226750843227 3839.0344634149224\n",
            "-260.030055677015 -53.98480786860455 853.8143370701\n",
            "-92.80370983040692 82.3761600482976 1866.9143001297489\n",
            "-126.78486279760905 -2.180909477174282 3401.6615879579913\n",
            "-104.1745997512511 -6.439023116603494 3883.659660999663\n",
            "-187.05005470929626 -43.05265574110672 3203.0930244214833\n",
            "113.80424693695528 -259.1150691981311 11738.201994313888\n",
            "-253.72354056494117 -67.61938970861956 4565.036635616794\n",
            "-124.16655576211373 -52.61453716084361 2047.6888902503997\n",
            "-129.23549200532676 10.188432316295803 488.92502840497764\n",
            "-100.34547799011524 24.72797468304634 3937.2741821496747\n",
            "27.54995203875427 32.44702081172727 10293.23712042952\n",
            "-96.401984508811 -20.401209948584437 3154.1430540938163\n",
            "-130.75789197656363 -34.257446171715856 1957.475017974386\n",
            "-99.00525327525514 -223.81710748001933 4540.840276971692\n",
            "-68.5919422270839 38.62161906063557 3233.9729394891765\n",
            "-87.39746931277587 -17.978097090497613 1248.8837307046633\n",
            "-88.55037723613034 -11.4472118569538 3045.582883822499\n",
            "-84.63839535571434 -33.81553779589012 719.0268644523458\n",
            "145.0484705023382 -65.36610081037361 30159.448189431045\n",
            "400 -160.6807808200203 114.85944959369316\n",
            "-101.92021657497703 -22.47109438292682 5230.2203395309625\n",
            "-180.9816167639747 -10.026196775957942 6383.90982550662\n",
            "158.92339016325167 -19.4171898374625 20600.180074248292\n",
            "-162.78383730033133 -100.61282842233777 2949.310531159659\n",
            "-29.76671493374024 -42.381464425474405 9218.291997042252\n",
            "-233.3002725302356 -150.68926398083568 17991.07394115813\n",
            "-168.84696492604348 -138.93009885959327 7402.24339241744\n",
            "-85.4110368536309 -124.26641259435564 7684.137653217622\n",
            "-50.067949114579875 25.798061709851027 8699.952356228605\n",
            "-13.117901444909677 29.000583625398576 11982.882828048023\n",
            "-35.30870002919439 -94.78381815180182 5616.120750415052\n",
            "-215.46210056902567 -152.02061232971027 22061.366069466923\n",
            "-44.35089623589977 -200.9677750049159 9481.483808884193\n",
            "-19.335781163619444 -439.421892112754 8760.66077359204\n",
            "-187.98005012733952 -35.28979929466732 2564.520709863864\n",
            "-76.53122305168422 -13.436433263123035 2446.9166453732178\n",
            "-97.4028131192636 -155.38630417408422 7618.08363350993\n",
            "-137.2512052174271 -123.1896212787833 10415.90522494656\n",
            "108.09314987618174 -93.8957547377795 6924.432642656862\n",
            "-21.92827343432804 -78.9620900368318 6479.136228325908\n",
            "-3.3196497274937906 -26.337942003272474 3355.39779852223\n",
            "245.54298983417993 -43.30914689722704 18686.325516914403\n",
            "-31.016976830453665 -54.93484400771558 9410.779472788345\n",
            "-269.33635964899065 -120.66636195662431 8923.929075170308\n",
            "-54.50703151921586 -20.02906676568091 8026.678433855763\n",
            "-180.20741984216824 -303.13291293848306 6865.776986968034\n",
            "-44.007575471029696 -30.978007026016712 8152.826934618151\n",
            "35.009736441996225 -490.16855469858274 7280.692290429084\n",
            "-23.279346503549604 7.194343662791653 6046.059969413225\n",
            "-37.72331804698267 23.521817250177264 6763.794993419171\n",
            "-165.4537624259943 -2.5211509138098336 28015.02272361185\n",
            "-76.46440296115631 -214.12212706613354 4335.146535449239\n",
            "7.469929739860298 -85.22862683539279 3328.3068537628715\n",
            "-125.10308900350563 -61.59130002558231 6754.636256070007\n",
            "-55.258588094674785 -184.0408965833485 8450.536593533587\n",
            "14.660096391125157 -452.6047107411214 5498.152509602798\n",
            "-64.60746869528305 -240.88315065857023 7222.707583776271\n",
            "-82.83026227855052 -230.03097049170174 4527.251970371333\n",
            "70.13764839754396 -173.0818062092585 6738.511895156305\n",
            "44.02215607347467 16.922212184406817 6947.741634243983\n",
            "-12.311307185185383 0.8580498450901359 6125.945885783702\n",
            "-104.52909571129476 -339.39646688355424 7925.941268087348\n",
            "-184.27485289646373 -71.35832389770076 136202.74459198292\n",
            "246.92617960743507 -113.89810596231621 26845.16497654155\n",
            "2.185998715802498 -73.26977129949955 9172.96392464079\n",
            "146.8164943224751 -291.46711306389625 24070.93472209944\n",
            "-169.2614190088332 -277.62264003866585 3287.455807634611\n",
            "43.60022345810559 -275.9397217257647 3663.089028757\n",
            "-45.1513534587442 -51.78974682101398 5496.316779989604\n",
            "187.4296046714758 -87.6304189798011 18613.904677281138\n",
            "156.91898837382266 -5.8893134909844775 16684.3970425543\n",
            "-33.57176763875371 51.40628379935515 8696.601878958114\n",
            "-180.62015514492367 17.448212472632633 74036.5037931674\n",
            "5.516989765238407 -66.66562858130783 1383.799901189137\n",
            "-54.76529623040685 -190.28457503172103 7909.348993874184\n",
            "241.59593260842445 31.0859000903547 19775.831946105416\n",
            "203.97057010282742 -111.28543197573163 15794.78409726789\n",
            "141.55116425563932 23.870237724488106 3097.452578168649\n",
            "14.352667568428998 -10.451893755744095 16513.24937071043\n",
            "-46.26015623847716 -509.74641958839493 3907.3076942321495\n",
            "-175.0050807544638 15.489676795541527 15047.453243376993\n",
            "-60.458120044138845 -81.1537602561184 11360.582606155964\n",
            "-125.95015163899421 -283.85001974715806 17051.44938724242\n",
            "-69.62182657388338 -15.677249707096053 16931.018713073543\n",
            "-118.46097776819944 -81.46475269802613 6146.0695095523115\n",
            "-59.863108997229645 -125.54037796687044 3146.9837710157235\n",
            "-30.237022968399117 9.304927785938162 4541.329143610354\n",
            "74.46268389463322 -77.2165050321637 2216.684907155617\n",
            "-58.0751451383168 -291.72386136823843 4210.295234666927\n",
            "-34.67616732806968 -408.6163986036845 2650.728214661118\n",
            "-68.96866426866569 -365.38118763131206 2090.452803102148\n",
            "-49.84903249926083 -409.4835745197124 264401.98992428504\n",
            "-108.49544446551437 -67.84510064404458 6477324.785808042\n",
            "-352.11705243569554 -236.02594529427392 154836.5795888612\n",
            "-48.94515958384588 68.3020072417317 417537.34488613624\n",
            "-55.84840307636442 803.3387491562171 1228190.7496755682\n",
            "31.40721413789106 320.38878468279836 36398.602259865496\n",
            "-28.76012610224383 290.93008899025153 28664.2940197813\n",
            "9.753683201407526 210.57960193639883 52940.39510299452\n",
            "-44.12761975709936 153.8595100519451 19025.311757354066\n",
            "-99.35961442063677 32.30362076498568 11396.791414950974\n",
            "-113.36843711130662 -57.593817031010985 17624.9661686318\n",
            "-186.00171485710302 -56.605685259215534 5784.803585525602\n",
            "-122.6289927065113 98.21666163485497 6720.2570627219975\n",
            "-216.66454499362067 -104.09963964894268 231663.04760388285\n",
            "-107.99624924596014 88.39340811036527 8577.453139136313\n",
            "-146.62498301436761 -45.55700444430113 8064.919908162206\n",
            "-149.13946846482196 96.26553872041404 17602.875883074477\n",
            "-161.9746194490835 -140.29102292745665 4980.075295745068\n",
            "-167.60973350909092 -44.58753336966038 7782.003448970616\n",
            "-199.10641040567765 -164.03617551084608 6617.666031911271\n",
            "-253.37349781957934 -234.07994449837133 29418.987805634737\n",
            "-164.27965216998297 -149.8566070497036 5196.589762836695\n",
            "-185.99188156923472 -155.03321860544384 8619.390204950934\n",
            "-197.03847782283447 -99.27823650673963 8579.648208564147\n",
            "-246.64040686506883 -235.77652311810738 42104.93399281986\n",
            "-169.27733612419084 -50.80712591810152 10237.639446769841\n",
            "-220.52244292168967 -157.7887954755671 95129.79154136777\n",
            "-159.69048311673598 -102.97755393850753 19354.679948338307\n",
            "-163.38734553646484 -63.79764359071851 9104.323265247047\n",
            "500 -66.35396111875461 116.75739967643557\n",
            "-98.08913094817007 -3.751050208695233 21693.88470203802\n",
            "-157.81934180254711 11.417586829513311 4924.559815008193\n",
            "-155.85272009411244 -4.131754307076335 3884.130101895891\n",
            "-144.43671417923207 -35.47222972024247 4897.784078255761\n",
            "-116.45290558537071 10.125711222179234 3304.7123830922064\n",
            "42.614899361877036 40.464083739705075 49134.67694196291\n",
            "-119.81960646725756 -30.240221748827025 5816.20461145998\n",
            "-89.77032821826947 -4.414213427808136 7497.034244812094\n",
            "-171.6917728979825 -30.923836683881746 11682.419066990959\n",
            "-36.326675650071266 -50.323852747305864 114785.01891423605\n",
            "-9.751630283340397 -38.592868293206266 5080.320793163803\n",
            "-95.86727013188545 -335.96263667102903 49245.5065907666\n",
            "-101.74687678298588 -298.6823753860008 13586.276253672855\n",
            "-48.272030301297406 -120.00556087824009 62035.435671369196\n",
            "-72.10123520405007 -61.09997149919127 57665.956759013294\n",
            "-144.7090790437627 -403.9093041314118 3194.630067945982\n",
            "-126.84208070380902 -186.08008576763677 16693.65114750649\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-696112ab3a4b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_with_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-50ca8bc8bff2>\u001b[0m in \u001b[0;36mrollout_with_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mlist_loss_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-50ca8bc8bff2>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, observation, next_observation, reward, terminated, log_prob)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mloss_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Critic の更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 state_steps)\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;31m# bake-in time before making it the default, even if it is typically faster.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforeach\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_to_fused_or_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;31m# Do not flip on foreach for the unsupported case where lr is a Tensor and capturable=False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforeach\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcapturable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_default_to_fused_or_foreach\u001b[0;34m(params, differentiable, use_fused)\u001b[0m\n\u001b[1;32m    120\u001b[0m                       torch.is_floating_point(p)) for p in params\n\u001b[1;32m    121\u001b[0m     )\n\u001b[0;32m--> 122\u001b[0;31m     foreach = not fused and all(\n\u001b[0m\u001b[1;32m    123\u001b[0m         p is None or (type(p) in _foreach_supported_types and\n\u001b[1;32m    124\u001b[0m                       p.device.type in foreach_supported_devices) for p in params\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベースラインの有無による学習曲線の差を可視化してみましょう．\n",
        "比較のため，Actorの学習率をそろえていますが，ベースラインの導入により，方策勾配の分散が削減されるので，少し大きめの学習率を設定することも可能になり，その結果として高速化することも可能になると期待されます．\n",
        "ただ，一方で，ActorとCriticの学習率の両方を調整することが必要となるため，パラメータ調整が実用上は面倒になりえることは述べておきます．"
      ],
      "metadata": {
        "id": "j5j-ip9ZcDxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = np.arange(1, 1+returns.size, returns.shape[1])\n",
        "avg = np.mean(returns, axis=1)\n",
        "std = np.std(returns, axis=1)\n",
        "plt.errorbar(episodes, avg, std, linestyle=':', marker='^')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "SOXTLc4oxCXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "経験分布関数についても確認しておきましょう．"
      ],
      "metadata": {
        "id": "pPZYoMrmdNjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=agent2, render=False)\n",
        "    return_array[i] = cumulative_reward(history)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "iVWlj9sJSXlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習結果の確認は以下のコードで行います．"
      ],
      "metadata": {
        "id": "O5QmAygcdr9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=agent2, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "TW0fJ1TswjLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自習課題\n",
        "\n",
        "* 方策を変えてみましょう．特に，中間層のノード数を変更した場合に，学習効率がどの程度変わるのか，グラフを作成するなどして確認しましょう．\n",
        "\n",
        "* 学習率を調整してみましょう．特に，ベースラインを導入したREINFORCEでは，Actorの学習率とCriticの学習率について，効率的なパラメータの関係を確認してみましょう．\n",
        "\n",
        "* タスクを変えてみましょう．タスクが異なれば，適切な方策（ノード数など）や適切な学習率も変化する可能性があります．これを確認してみましょう．"
      ],
      "metadata": {
        "id": "uwEbG_9Bd26g"
      }
    }
  ]
}