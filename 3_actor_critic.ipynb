{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOiGnUHyN3dxMq7rRdVwYiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akimotolab/Policy_Optimization_Tutorial/blob/main/3_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備\n",
        "\n",
        "まず，必要なパッケージのインストールとインポート，および仮想displayを設定します．"
      ],
      "metadata": {
        "id": "qWe6P9xqH8be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kXoBp0fgHyhe",
        "outputId": "1e9fc80f-d913-43db-ec43-e71d6f8f44e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [631 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,281 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,240 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,512 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,487 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,456 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,013 kB]\n",
            "Fetched 8,856 kB in 2s (4,268 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "15 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1.post0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 7,814 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n",
            "Fetched 7,814 kB in 1s (8,199 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120880 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1.post0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373072 sha256=0c994d3e1d7a375339c7d76f9000670ddd6138c4b962822ca74f8e21cbf391f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: farama-notifications, box2d-py, gymnasium\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "# 必要なパッケージのインストール\n",
        "!apt update\n",
        "!pip install swig\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import torch\n",
        "\n",
        "# 仮想ディスプレイの設定\n",
        "_display = Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "# gpuが使用される場合の設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
      ],
      "metadata": {
        "id": "iJ1PYM90IHa9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "d5Oklvxh5nAC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "続いて，第１回の資料で定義した`rollout`などの基本的な関数をここでも定義しておきます．"
      ],
      "metadata": {
        "id": "ci2EEsRpD1oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout(envname, policy=None, render=False, seed=None):\n",
        "    if render:\n",
        "        env = gym.make(envname, render_mode=\"rgb_array\")\n",
        "    else:\n",
        "        env = gym.make(envname)\n",
        "    history = []\n",
        "    img = []\n",
        "\n",
        "    # 乱数の設定\n",
        "    if seed is not None:\n",
        "        random.seed(int(seed))\n",
        "    envseed = random.randint(0, 1000)\n",
        "    actseed = random.randint(0, 1000)\n",
        "    observation, info = env.reset(seed=envseed)\n",
        "    env.action_space.seed(actseed)\n",
        "\n",
        "    # 可視化用の設定\n",
        "    if render:\n",
        "        d = Display()\n",
        "        d.start()\n",
        "        img.append(env.render())\n",
        "\n",
        "    # メインループ（環境とのインタラクション）\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # 行動を選択\n",
        "        if policy is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy(observation)\n",
        "\n",
        "        # 行動を実行\n",
        "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "        history.append([observation, action, next_observation, reward, terminated, truncated, info])\n",
        "        observation = next_observation\n",
        "        if render:\n",
        "            display.clear_output(wait=True)\n",
        "            img.append(env.render())\n",
        "    env.close()\n",
        "    return history, img\n",
        "\n",
        "\n",
        "def visualize(img):\n",
        "    dpi = 72\n",
        "    interval = 50\n",
        "    plt.figure(figsize=(img[0].shape[1]/dpi, img[0].shape[0]/dpi), dpi=dpi)\n",
        "    patch = plt.imshow(img[0])\n",
        "    plt.axis=('off')\n",
        "    animate = lambda i: patch.set_data(img[i])\n",
        "    ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(img), interval=interval)\n",
        "    display.display(display.HTML(ani.to_jshtml()))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def cumulative_reward(history):\n",
        "    return sum(hist[3] for hist in history)"
      ],
      "metadata": {
        "id": "9ELUIKjb5vY2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 方策勾配を用いた方策最適化（強化学習）\n",
        "\n",
        "今回は「方策勾配法」を見ていきます．\n",
        "第１回は，汎用的なブラックボックス最適化法を用いた方策最適化の例を見てもらいました．\n",
        "ブラックボックス最適化法を用いていたということは，目的関数$J(\\theta)$が「何らかの方策をパラメータ$\\theta$で用いた場合に，環境とインタラクションした結果得られる累積報酬」を意味しているという情報を用いずに，ただブラックボックスな関数として最適化していることを意味します．\n",
        "ここでは，積極的にこの知識を活用していく方法を検討していきましょう．\n",
        "ブラックボックス最適化としての方策最適化と，強化学習を用いた方策最適化の一番の違いがここにあります．"
      ],
      "metadata": {
        "id": "H_gG0F5uKTYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 状態と行動の価値\n",
        "\n",
        "ブラックボックスな目的関数$J(\\theta)$では，一連のインタラクションを通して得られた報酬の合計を評価しています．\n",
        "これは，最大化したい指標であることに間違いありませんが，一方で，各状態でとった各行動が良かったのかどうか，という情報を与えてくれません．\n",
        "この情報を活用することができれば，ある状態$s$ではある行動$a$を取るとよい，ということがわかり，その確率を高くするように方策を改善することができそうです．"
      ],
      "metadata": {
        "id": "0_V-B-wGJsZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定式化\n",
        "まず，最低限の定式化を行います．\n",
        "今回対象としている方策最適化では，\n",
        "まず初期状態$s_0$を観測します．\n",
        "初期状態は確率分布$p_0$からランダムに生成されます．\n",
        "方策を通して，次にとる行動$a_0 \\sim \\pi(\\cdot \\mid s_0)$を決定します．\n",
        "ここでは，方策として確率的な方策を考えることにします．\n",
        "この行動を実行すると，状態が$s_1$に変わり，これを観測します．\n",
        "それと同時に，$s_0$で行動$a_0$を取ることの良さを表す即時報酬$r_1$が得られます．\n",
        "次状態と即時報酬は，環境が定める条件付き確率$p_T(s_1, r_1 \\mid s_0, a_0)$により定まります．\n",
        "このあとは，$s_1$において方策に従って次の行動$a_1 \\sim \\pi(\\cdot \\mid s_0)$を決定し，次状態と即時報酬を観測する，というステップを繰り返します．\n",
        "この環境との一度のインタラクションをステップと呼びます．\n",
        "\n",
        "注意：「環境が定める」といっても，実際に報酬を設計するのは自分自身（設計者）です．望ましい方策を得るためには，適切な報酬を設計することが極めて重要です．）"
      ],
      "metadata": {
        "id": "e0st6F33vB0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 累積報酬\n",
        "強化学習においては，方策最適化の目的は割引累積報酬の期待値を最大化することと一般に定められます．\n",
        "あるステップ$t$において，その先に得られる割引累積報酬は\n",
        "$$\n",
        "G_t = r_{t+1} + \\gamma r_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+1+k}\n",
        "$$\n",
        "と定義されます．ここで，$\\gamma \\in [0, 1]$は割引率と呼ばれるパラメータです．\n",
        "方策，初期状態分布，状態遷移，即時報酬は確率的ですから，$G_t$自体も確率的に振る舞います．\n",
        "そこで，これの期待値$\\mathrm{E}[G_0]$を考え，これを最大化することを考えます．\n",
        "\n",
        "ここでは，インタラクションが無限に続くことを想定して$G_t$が定義されています．\n",
        "この場合，割引率は$\\gamma < 1$であることが必要です．\n",
        "エピソディックタスクの場合，特別な終了状態（例えば迷路のような問題において，ゴール状態に到達した，落とし穴に落ちて脱落した，など）があり，途中でエピソードが止まることになります．\n",
        "この場合にも，終了状態に達したあとは何をしても終了状態に遷移し，即時報酬はずっと0である，と考えれば，上の定義に当てはまります．\n",
        "このように，インタラクションに一区切りがあるようなタスクはエピソディックタスクと呼ばれ，この一区切りのステップのまとまりをエピソードと呼びます．\n",
        "最適化（学習）の都合上，特定のステップでインタラクションを打ち切り，無理やりエピソディックタスクにするような場合もありますが，この場合にも目的は$\\mathrm{E}[G_0]$の最大化である（有限ステップでの累積報酬ではない）と考えると，以下の議論が成立します．\n",
        "\n",
        "注意：制限時間などのように，特定のステップがすぎると強制的に状態がリセットされるようなケースの場合，注意が必要です．この場合，残り時間などを状態観測に含めることが必要になります．"
      ],
      "metadata": {
        "id": "RXkv9WldvGg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 状態価値と行動価値\n",
        "\n",
        "状態$s$の価値を，「$s_0 = s$からインタラクションを始めて，方策$\\pi$に従って行動選択した際に得られる割引累積報酬の期待値$\\mathrm{E}[G_0 \\mid s_0 = s]$」と定義します．\n",
        "これを$V^{\\pi}(s)$と書きます．\n",
        "定義からわかるように，状態価値は方策$\\pi$に依存しています．\n",
        "割引累積報酬が\n",
        "$$\n",
        "G_{t} = r_{t+1} + \\gamma G_{t+1}\n",
        "$$\n",
        "という再帰的な関係式を満たすことを考えると，状態価値は\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathrm{E}[r_{1} + \\gamma V^{\\pi}(s_{1}) \\mid s_0 = s]\n",
        "$$\n",
        "という関係式を満たすことがわかります．\n",
        "なお，ステップのインデックスに関しては\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathrm{E}[r_{t+1} + \\gamma V^{\\pi}(s_{t+1}) \\mid s_t = s]\n",
        "$$\n",
        "と考えても構いません．\n",
        "状態の価値が高いということは，その方策に従っている場合には，その状態からは得られる割引累積報酬の期待値が高いということを意味しています．\n",
        "ですから，そのような状態を積極的に訪問したいという指針になるでしょう．\n",
        "\n",
        "関連して，行動価値（状態行動価値とも言います）を，「$s_0 = s$からインタラクションを始めて，最初だけ行動$a$を選択し，その後は方策$\\pi$に従って行動選択した際に得られる割引累積報酬の期待値$\\mathrm{E}[G_0 \\mid s_0 = s, a_0 = a]$」と定義します．\n",
        "これを$Q^{\\pi}(s, a)$と書きます．\n",
        "定義からわかるように，もしも最初の行動を$a \\sim \\pi(\\cdot \\mid s)$にしたがって決定していれば行動価値関数の$a \\sim \\pi(\\cdot \\mid s)$についての期待値は，状態$s$の価値\n",
        "$$\n",
        "V^\\pi(s) = \\mathrm{E}[Q^{\\pi}(s, a) \\mid a \\sim \\pi(\\cdot \\mid s)]\n",
        "$$\n",
        "に一致します．\n",
        "\n",
        "価値関数はいずれも解析的に得られるものではありません．\n",
        "しかし，インタラクションを通して，近似していくことが可能です．\n",
        "その代表的な方法の一つに，TD誤差を用いた更新があります．"
      ],
      "metadata": {
        "id": "9PHLZ4LlvJNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 方策勾配\n",
        "\n",
        "目的関数を数値的に最適化する際，まず考えられる方針は勾配法を用いることです．\n",
        "すなわち，$\\theta \\leftarrow \\theta + \\alpha_\\theta \\nabla_\\theta J(\\theta)$と更新する方法です．\n",
        "割引累積報酬をブラックボックスと捉えてしまうと勾配は計算できないのですが，価値関数を明示的に使うことで勾配をうまく近似することが可能です．\n",
        "「方策勾配定理」は，割引累積報酬の期待値の，方策パラメータについての期待値を書き下す方法を提供してくれます．"
      ],
      "metadata": {
        "id": "Lz8VutUocewQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方策勾配定理\n",
        "\n",
        "$J(\\theta) = \\mathrm{E}[V^{\\pi}(s) \\mid s \\sim p_0]$とします．\n",
        "このとき，\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) \\propto \\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "$$\n",
        "と書き下すことができます．\n",
        "ここで，上の期待値は，方策$\\pi_\\theta$のもとでの状態訪問確率と行動選択確率について取られています．\n",
        "実際にインタラクションを通して観測された状態とその状態のもとで方策$\\pi_\\theta$に従って選択された行動を用いれば，右辺は\n",
        "$$\n",
        "Q^{\\pi}(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "で近似することができます．\n",
        "また，行動価値関数をその近似値$q(s_t, a_t)$で置き換えれば，勾配$\\nabla_\\theta J(\\theta)$の推定値として\n",
        "$$\n",
        "q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "を得ることができます．\n",
        "もしくは，1エピソード分の状態遷移履歴を用いて，\n",
        "$$\n",
        "\\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "と近似することができます．\n",
        "これらを用いて，\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + \\alpha_\\theta \\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "などと方策パラメータを更新していきます．ここで，$\\alpha_\\theta$は学習率です．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "awCoxRZr50Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方策勾配の自動計算\n",
        "\n",
        "方策勾配を計算するには，方策の対数の方策パラメータについての勾配 $\\nabla_\\theta \\ln \\pi$ を求める必要があります．\n",
        "方策を変える毎に，これを計算して実行するのは，複雑な方策を採用する際には面倒になります．\n",
        "他方，深層学習で用いられている Pytorch などのライブラリでは，関数の勾配を自動的に計算してくれる機能が備わっています．この機能を活用することで，方策勾配も容易に計算することが可能になります．\n",
        "\n",
        "上に示した1エピソード分のデータを用いた方策勾配\n",
        "$$\n",
        "\\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\nabla_{\\theta} \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "を計算したい状況を考えましょう．\n",
        "いま，$q^{\\pi}(s_t, a_t)$は何らかの方法（後述します）で予め計算されているとします．\n",
        "このとき，以下のような関数を考えます．\n",
        "$$\n",
        "L(\\theta; \\{(s_t, a_t)\\}_{t=0}^{T}) = \\frac{1}{T}\\sum_{t=0}^{T} q(s_t, a_t) \\ln \\pi_\\theta(a_t \\mid s_t)\n",
        "$$\n",
        "このような関数$L$を定義すると，上に示した方策勾配は $\\nabla_\\theta L(\\theta; \\{(s_t, a_t)\\}_{t=0}^{T})$ であることが容易にわかります．\n",
        "\n",
        "この事実を用いると，方策勾配を自動計算することが可能になります．\n",
        "行動が離散であれば，各行動の選択確率からなるベクトルを返す関数モデルを用意しておけば，用意に計算できます．"
      ],
      "metadata": {
        "id": "SxNyza3zTfCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下では，Pytorchを用いた方策（ここではActorと呼ぶことにします．これは今後紹介するActor-Criticアルゴリズムでの呼び方に従っています．）の実装方法を紹介します．\n",
        "ここでは四層のニューラルネットワークを用いています．中間層のノード数は`dim_hidden`により指定されています．\n",
        "\n",
        "関数`forward`が観測`x`を受け取って，これに対して各行動の選択確率を出力する関数です．\n",
        "行動選択確率は以下のように計算されます．\n",
        "入力を$h_0 = x$とします．\n",
        "$i = 1, 2, 3, 4$について，以下を計算します．\n",
        "$$\n",
        "h_i = g_i\\left( b_i + W_i h_{i=1}\\right)\n",
        "$$\n",
        "ここで，中間層の活性化関数$g_1, g_2, g_3$は$\\mathrm{ReLU}(s) = \\max(0, s)$であり，\n",
        "出力部分の活性化関数$g_4$は出力の和が$1$となるように，$\\mathrm{softmax}$を用いています．"
      ],
      "metadata": {
        "id": "KBZ3NMsNVNRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REINFORCEアルゴリズム（行動価値関数のMonte Carlo推定を用いた方策勾配法）\n",
        "\n",
        "方策勾配を用いた強化学習の代表的な一つの方法である，REINFORCEアルゴリズムを紹介します．\n",
        "話を簡単にするために，ここでは\n",
        "\n",
        "1. エピソディックタスク，\n",
        "2. 割引率$\\gamma = 1$（割引なし），\n",
        "3. 確率的方策，\n",
        "\n",
        "であることを仮定します．\n",
        "\n",
        "方策勾配法を用いる場合，行動価値関数$Q^{\\pi}(s_t, a_t)$の近似値$q(s_t, a_t)$を得ることが必要になります．\n",
        "この$q(s_t, a_t)$の計算方法の違いにより，様々な方策勾配法のバリエーションが存在します．\n",
        "REINFORCEアルゴリズムでは，Monte-Carlo推定を用いてこれを近似して利用します．"
      ],
      "metadata": {
        "id": "D052tsAPBSKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 行動価値関数のモンテカルロ推定\n",
        "\n",
        "行動価値関数$Q^{\\pi}(s, a)$は「$s_0 = s$からインタラクションを始めて，最初だけ行動$a$を選択し，その後は方策$\\pi$に従って行動選択した際に得られる割引累積報酬の期待値$\\mathrm{E}[G_0 \\mid s_0 = s, a_0 = a]$」と定義されます．\n",
        "すなわち，この期待値は各ステップ$t$での状態の訪問確率と行動の選択確率を知らなければ計算できません．\n",
        "行動の選択確率は方策で決まりますから，これは予め知っている情報ですが，ある状態である行動を取った際の次の状態への遷移確率は未知である（実際に実行して始めて次状態を観測できる）と仮定していますから，これを計算することはできません．\n",
        "期待値を厳密には計算できませんが，実際にインタラクションを通して累積報酬の実現値を観測することは可能です．\n",
        "これを用いて期待値を推定する方法がモンテカルロ推定です．\n",
        "\n",
        "状態遷移はステップ数$t$には依存しないため，行動価値関数$Q^{\\pi}(s, a)$は$\\mathrm{E}[G_t \\mid s_t = s, a_t = a]$と定義しても同じものになります．\n",
        "すなわち，ステップ$t$において観測した状態が$s_t = s$，$a_t = a$であったならば，そこから実際にインタラクションして得られた累積報酬 $G_t = r_{t+1} + \\dots + r_{T}$（$T$は終端ステップ）の期待値がその$(s_t, a_t)$の行動価値であり，観測された$G_t$は確率変数として見た場合の$G_t$の観測値（サンプル）であることがわかります．\n",
        "そのため，観測された$G_t$は行動価値$Q^\\pi(s_t, a_t)$の不偏推定値であることになります．\n",
        "そこで，これを$q^\\pi(s_t, a_t)$として採用することにしましょう．\n"
      ],
      "metadata": {
        "id": "NoMZcO82CzR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "実装上は，1エピソードの結果得られる状態遷移履歴から，各ステップでの将来に得られる累積報酬を計算し，その配列を返します．\n",
        "終了状態から遡ることで計算することでこれを簡単に計算できます．"
      ],
      "metadata": {
        "id": "bFaE6C9oSCjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REINFORCEアルゴリズム\n",
        "\n",
        "REINFORCEアルゴリズムは，価値のモンテカルロ推定を用いた方策勾配法です．\n",
        "REINFORCEにはいくつかのバリエーションがありますが，ここでは，エピソード単位で方策パラメータを更新する最もシンプルな方法を紹介します．\n",
        "（なお，状態遷移毎（ステップ毎）にパラメータを更新するバリエーションもあります．）\n",
        "\n",
        "まず，現在の方策を用いてEエピソード分だけ環境とインタラクションします．\n",
        "この結果から，エピソード内に訪問した各状態についての価値をモンテカルロ推定します．\n",
        "この推定価値を用いて，方策勾配を\n",
        "$$\\begin{aligned}\n",
        "\\nabla_\\theta J(\\theta)\n",
        "&= \\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi(a \\mid s)  \\right] \\\\\n",
        "&\\approx \\frac{1}{E}\\sum_{e=1}^{E}\\frac{1}{T_e}\\sum_{t=1}^{T_e} G_{e,t} \\nabla_{\\theta} \\ln \\pi(a_{e,t} \\mid s_{e,t}) =: \\widehat{\\nabla_\\theta J(\\theta) }\n",
        "\\end{aligned}\n",
        "$$\n",
        "と近似し，方策パラメータを$\\theta \\leftarrow \\theta + \\eta \\widehat{\\nabla_\\theta J(\\theta) }$に従って更新します．\n",
        "ここで，$\\eta > 0$は学習率を表し，問題や方策毎に調整が必要となります．\n",
        "また，状態，行動，累積報酬にはエピソードのインデックスが追加されており，終了ステップもエピソード毎に異なることに注意してください．"
      ],
      "metadata": {
        "id": "VGo279ppMl4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下では，REINFORCEアルゴリズムを実装しています．\n",
        "\n",
        "`select_action`は状態観測を受け取り，方策に従って行動を選択しています．\n",
        "実装上は，観測状態をPytorchのテンソル形式に変換し，これをActorに入力して行動確率確率を計算し，これに従って行動をサンプルしています．\n",
        "パラメータを更新する際に必要になる$\\ln \\pi(a \\mid s)$も出力しています．\n",
        "\n",
        "`rollout`は現在の方策を用いて1エピソード分，環境と状態遷移を繰り返し，その際の即時報酬列と$\\ln \\pi(a \\mid s)$列を返します．\n",
        "\n",
        "`update`はActorのパラメータを更新する関数です．\n",
        "ここでは，エピソード数は$E = 1$を前提としています．\n",
        "確率的傾斜法の理屈から，$E$を大きくして方策勾配の分散を小さくすることと，学習率$\\eta$を小さくしてパラメータ更新の分散を小さくすることには類似の効果があることが知られています．\n",
        "そのため，以下の議論では主に$E = 1$を前提としていきます．\n",
        "\n"
      ],
      "metadata": {
        "id": "XBpIvjyPXvPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReinforceAgent:\n",
        "    def __init__(self, env, model, lr=5e-4):\n",
        "        self.policy = model\n",
        "        self.env = env\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        return self.select_action(observation)[0]\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # 行動選択\n",
        "        observation_ = Variable(torch.Tensor(observation))\n",
        "        action_probs = self.policy(observation_)\n",
        "        log_probs = action_probs.log()\n",
        "        action = Categorical(action_probs).sample()\n",
        "        return action.data.cpu().numpy(), log_probs[action]\n",
        "\n",
        "    def rollout(self):\n",
        "        # 1 エピソード実行\n",
        "        observation, info = self.env.reset()\n",
        "        steps = 0\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        while not (terminated or truncated):\n",
        "            action, log_prob = self.select_action(observation)\n",
        "            next_observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            observation = next_observation\n",
        "            steps +=1\n",
        "        return rewards, log_probs\n",
        "\n",
        "    def update(self, rewards, log_probs):\n",
        "        # パラメータの更新\n",
        "        g_array = np.cumsum(np.array(rewards)[::-1])[::-1]\n",
        "        loss = - sum([g * lp for g, lp in zip(g_array, log_probs)]) / len(rewards)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "vKagOnIhjSFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`LunarLander-v2`環境で，REINFORCEアルゴリズムによる学習を実行してみます．\n",
        "比較的時間がかかりますので，注意してください．（30min程度）"
      ],
      "metadata": {
        "id": "ptJVsdesZOiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "env = gym.make(envname)\n",
        "\n",
        "model = Actor(dim_state = 8, num_action = 4)\n",
        "agent = ReinforceAgent(env, model)"
      ],
      "metadata": {
        "id": "upMhDE3cjSC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは，100エピソード毎に学習された方策を用いて得られる累積報酬の平均値と標準偏差を表示しています．"
      ],
      "metadata": {
        "id": "I2PEmHGOZpXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interval = 100\n",
        "returns = np.zeros((100, interval))\n",
        "\n",
        "for i in range(returns.shape[0]):\n",
        "    for j in range(returns.shape[1]):\n",
        "        rewards, log_probs = agent.rollout()\n",
        "        agent.update(rewards, log_probs)\n",
        "        returns[i, j] = np.sum(rewards)\n",
        "    print(interval * (i+1), np.mean(returns[i]), np.std(returns[i]))"
      ],
      "metadata": {
        "id": "V26zgXlbjSAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "累積報酬の移り変わりを可視化してみます．累積報酬の平均値は改善していく様子が見られると思います．ただ，比較的標準偏差が大きいことが確認できるでしょう．"
      ],
      "metadata": {
        "id": "f6dwfJlXaHTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = np.arange(1, 1+returns.size, returns.shape[1])\n",
        "avg = np.mean(returns, axis=1)\n",
        "std = np.std(returns, axis=1)\n",
        "plt.errorbar(episodes, avg, std, linestyle=':', marker='^')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "hqSNpXX_-3Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習後の経験分布関数を確認してみましょう．\n",
        "横軸は累積報酬を$-1$倍したものになりますので，小さい値が高い割合で得られるほど，望ましい方策が得られていると言えるでしょう．"
      ],
      "metadata": {
        "id": "a_paerEX87NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=agent, render=False)\n",
        "    return_array[i] = cumulative_reward(history)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "LFnPKD7v8IcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それでは学習結果を確認してみましょう．\n",
        "累積報酬の標準偏差が比較的大きいことが予想されますので，何度か実行し，確認してみましょう．"
      ],
      "metadata": {
        "id": "vOwLxBg_76NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=agent, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "pu_cbDuujR65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "前回のブラックボックス最適化法を用いた方策最適化に対して，方策勾配法は累積報酬だけでなく状態遷移の履歴を活用した方策最適化法になっています．しかし，あまり効率的になっている印象は持てないかもしれません．\n",
        "実際には方策モデルが異なるため今回の実験からは実行時間の比較ができません．\n",
        "また，強化学習では「期待累積報酬」を最大化しており，特定の乱数系列（特定の初期状態）のもとでの累積報酬最適化を行っていた前回の結果と比較するのはフェアではありません．\n",
        "\n",
        "ただ，上記のREINFORCEが効率的でないことも事実です．\n",
        "一つの原因として，「方策勾配は推定分散が大きい」ことが挙げられます．\n",
        "REINFORCEでは，エピソード中に得られた累積報酬を用いて価値関数を近似し，これを用いて方策勾配を近似しています．\n",
        "しかし，実行結果を見てもわかるように，累積報酬の値は大きな分散を持っているため，これを用いて推定される方策勾配も大きな分散を持ってしまいます．\n",
        "方策勾配の分散が大きい場合，これに伴って学習率を小さくしなければならず，学習が遅くなります．"
      ],
      "metadata": {
        "id": "Qdn9vkX0_nMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ベースラインの導入による方策勾配の推定分散削減\n",
        "\n",
        "方策勾配の推定分散を削減するために，方策勾配の式を見直してみましょう．\n",
        "方策勾配は以下の式を近似しています．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_s \\mathrm{E}_a \\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_s \\left[ \\sum_{a} Q^{\\pi}(s, a) \\pi_\\theta(a \\mid s)  \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_s \\left[ \\sum_{a} Q^{\\pi}(s, a) \\nabla_{\\theta} \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "この式を修正してみます．まず，ある関数$b(s)$について，\n",
        "$$\n",
        "\\sum_{a} b(s) \\nabla_{\\theta} \\pi_\\theta(a \\mid s)\n",
        "=\n",
        "b(s) \\sum_{a} \\nabla_{\\theta} \\pi_\\theta(a \\mid s)\n",
        "=\n",
        "b(s) \\nabla_{\\theta} \\sum_{a} \\pi_\\theta(a \\mid s)\n",
        "=\n",
        "b(s) \\nabla_{\\theta} 1\n",
        "= 0\n",
        "$$\n",
        "が成り立つことに注意すれば，上の式は，\n",
        "$$\\begin{aligned}\n",
        "\\mathrm{E}\\left[ Q^{\\pi}(s, a) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "&= \\mathrm{E}_s \\left[ \\sum_{a} \\left( Q^{\\pi}(s, a) - b(s) \\right) \\nabla_{\\theta} \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\\\\n",
        "&= \\mathrm{E}_{s,a} \\left[ \\left( Q^{\\pi}(s, a) - b(s) \\right) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)  \\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "と書き直すことができることがわかると思います．\n",
        "すなわち，行動価値関数$Q^{\\pi}(s, a)$から，状態のみに依存する任意の関数$b(s)$を引いたとしても，この期待値は変化しないことがわかります．\n",
        "しかし，これを推定する場合，\n",
        "$$\n",
        "\\left( q(s, a) - b(s) \\right) \\nabla_{\\theta} \\ln \\pi_\\theta(a \\mid s)\n",
        "$$\n",
        "の期待値は上の議論から代わりませんが，その推定分散は$b(s)$に依存することになります．\n",
        "そこで，この$b(s)$を工夫することで，推定量分散を小さくする試みがあります．\n",
        "\n",
        "代表的な$b(s)$の選択肢は，行動価値関数の行動についての期待値，すなわち状態価値関数です．\n",
        "行動価値から状態価値を引いた値はアドバンテージなどと呼ばれ，しばしば強化学習の文脈で現れる量になります．\n",
        "状態価値をベースラインとして利用することは，必ずしも最適ではありませんが，合理的と考えられる理由があります．これについて簡単に考察してみましょう．\n",
        "まず，$\\nabla \\ln \\pi(a \\mid s)$に着目します．\n",
        "これは，$\\ln \\pi(a \\mid s)$の勾配ですから，状態$s$で行動$a$を選択する確率が上昇する方向にパラメータが更新されます．\n",
        "上のREINFORCEでは，エピソード中に実際に観測した状態$s_t$とそこで選択された行動$a_t$について，$\\ln \\pi(a_t \\mid s_t)$に累積報酬を乗じた方向にパラメータが更新されています．\n",
        "ここで，Lunar-Landerの例に着目すると，エピソード中を通してほとんどの場合に累積報酬が負の値になっていることがわかると思います．\n",
        "このとき，どのような行動を取ったとしても，選択された行動の選択確率を下げる方向にパラメータは更新されることになります．\n",
        "これでは効率が悪い更新になっていると想像されます．\n",
        "期待値を累積報酬から減ずることにより，その状態で得られる累積報酬の期待値よりも高い価値を持つ行動については行動選択確率を上げ，逆に価値の低い行動については行動選択確率を下げる，といった更新が可能になります．\n",
        "（なお，常に行動選択確率を下げるにもかかわらず方策が収束していくのは，その下げ幅が報酬の大小関係により異なるため，長い目でみれば報酬の高い行動の選択確率が相対的に上がることになるためである）\n",
        "\n"
      ],
      "metadata": {
        "id": "lcx6bQC0V0s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ベースラインを導入したREINFORCEアルゴリズム\n",
        "\n",
        "それでは状態価値をベースラインとしたREINFORCEアルゴリズムを実装してみましょう．\n",
        "追加で必要なコンポーネントは，状態価値の推定部分です．\n",
        "状態価値は，各状態でその先に得られる累積報酬の期待値で与えられます．\n",
        "そこで，状態価値を近似する関数モデルを$b_\\phi(s)$としたとき，1エピソード中の状態遷移履歴を用いて，\n",
        "$$\n",
        "L(\\phi) = \\frac{1}{T}\\sum_{t=1}^{T} (G_t - b_{\\phi}(s_{t-1}))^2\n",
        "$$\n",
        "を最小化するように$\\phi$を学習することが考えられます．"
      ],
      "metadata": {
        "id": "54PYDEl1Fo8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下では，ベースラインを計算するネットワークを作成しています．\n",
        "ここでもActor-Criticにならい，ベースラインをCriticと呼ぶことにします．\n",
        "\n",
        "CriticのアーキテクチャはActorと同様とします．\n",
        "異なる点は，出力が1次元であり，出力には活性化関数を設けないという点です．"
      ],
      "metadata": {
        "id": "IK-B4miwa4IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical"
      ],
      "metadata": {
        "id": "hDnqAPt1jSJh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, dim_state, num_action, dim_hidden=128):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_state, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, num_action)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.softmax(self.fc2(x), dim=0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3d4NDkjPjSHG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, dim_state, dim_hidden=128):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_state, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eQFnydEz8ovf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベースラインを導入したREINFORCEアルゴリズムを実装します．\n",
        "`update`関数において，criticを更新している点が前述のREINFORCEとの差分です．"
      ],
      "metadata": {
        "id": "ni4vEzqDba5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCriticAgent:\n",
        "    def __init__(self, env, actor, critic, lr_a=5e-4, lr_c=1e-4):\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.env = env\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_a)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_c)\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        return self.select_action(observation)[0]\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # 行動選択\n",
        "        observation_ = Variable(torch.Tensor(observation))\n",
        "        action_probs = self.actor(observation_)\n",
        "        log_probs = action_probs.log()\n",
        "        action = Categorical(action_probs).sample()\n",
        "        return action.data.cpu().numpy(), log_probs[action]\n",
        "\n",
        "    def rollout_with_update(self):\n",
        "        # 1 エピソード実行\n",
        "        observation, info = self.env.reset()\n",
        "        steps = 0\n",
        "        rewards = []\n",
        "        list_loss_a = []\n",
        "        list_loss_c = []\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        while not (terminated or truncated):\n",
        "            action, log_prob = self.select_action(observation)\n",
        "            next_observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "            loss_a, loss_c = self.update(observation, next_observation, reward, terminated, log_prob)\n",
        "            rewards.append(reward)\n",
        "            list_loss_a.append(loss_a.detach().numpy())\n",
        "            list_loss_c.append(loss_c.detach().numpy())\n",
        "            observation = next_observation\n",
        "            steps +=1\n",
        "        return rewards, list_loss_a, list_loss_c\n",
        "\n",
        "    def update(self, observation, next_observation, reward, terminated, log_prob):\n",
        "        if terminated:\n",
        "            vtt = reward\n",
        "        else:\n",
        "            vtt = reward + self.critic(torch.Tensor(next_observation)).detach().numpy()[0]\n",
        "        vt = self.critic(torch.Tensor(observation))[0]\n",
        "\n",
        "        # Actor の更新\n",
        "        loss_a = - (vtt - vt.detach().numpy()) * log_prob\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        loss_a.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Critic の更新\n",
        "        loss_c = (vtt - vt)**2\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        loss_c.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        return loss_a, loss_c"
      ],
      "metadata": {
        "id": "GeCzR9UQjR0b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "env = gym.make(envname)\n",
        "\n",
        "actor = Actor(dim_state = 8, num_action = 4)\n",
        "critic = Critic(dim_state = 8)\n",
        "agent = ActorCriticAgent(env, actor, critic)"
      ],
      "metadata": {
        "id": "4cJI3P6pAT9n"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interval = 100\n",
        "returns = np.zeros((100, interval))\n",
        "\n",
        "for i in range(returns.shape[0]):\n",
        "    for j in range(returns.shape[1]):\n",
        "        rewards, loss_a, loss_c = agent.rollout_with_update()\n",
        "        returns[i, j] = np.sum(rewards)\n",
        "        print(returns[i, j], sum(loss_a), sum(loss_c))\n",
        "    print(interval * (i+1), np.mean(returns[i]), np.std(returns[i]))"
      ],
      "metadata": {
        "id": "lHdMuxUQAife",
        "outputId": "68adc4fe-4e13-4c1c-925d-bd47d6e08e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-170.55526822335378 -182.81555378064513 10300.472972665215\n",
            "-129.57101017411063 -189.72034125216305 10119.75235315565\n",
            "-357.8362289453814 -491.3130881264806 11350.30495189596\n",
            "-269.0709658065987 -237.96641109883785 10543.344123406336\n",
            "-190.5558514732821 -311.42700131237507 10122.062433238665\n",
            "-105.26877530767808 -95.62339997466188 10160.220949422817\n",
            "-126.19347952592443 -106.16882803663611 10173.400387224974\n",
            "-109.94183029144169 -125.74524318892509 10264.750005738955\n",
            "-227.68185676703123 -255.4379977993667 12703.066914838506\n",
            "-86.75881978458952 -74.45330530405045 10202.953154609539\n",
            "-94.59458383442333 -112.75111450999975 10086.72108038151\n",
            "-378.8045401359503 -493.554326787591 11071.604032937437\n",
            "-149.4207063034625 -181.48277108371258 10269.547827281676\n",
            "-258.82122622707743 -292.6073094345629 11525.312616570445\n",
            "-201.9530849214761 -261.2013596780598 10287.630953787782\n",
            "-140.41466751495034 -172.8194871544838 10251.948825024534\n",
            "-133.02132701379549 -221.12610647827387 11400.941346233245\n",
            "-100.59562529874826 -147.51414946839213 10024.142375652213\n",
            "-85.25115459261724 -150.00696371449158 10213.302121791228\n",
            "-133.8642474523492 -191.19805617257953 10278.790293740167\n",
            "-250.9627862206836 -342.87984402664006 10302.109706943793\n",
            "-176.80137909821514 -292.9560793302953 10203.800530418055\n",
            "-92.85458470606196 -150.65226797387004 19524.71027654488\n",
            "-183.6620998314353 -250.4766872804612 11065.796731400063\n",
            "-278.47251013282 -347.5701001435518 11047.828730851877\n",
            "-198.1091956114879 -284.07993776351213 10739.758537324611\n",
            "-246.82854164622321 -312.37182774581015 10967.169224462958\n",
            "-263.79791082700876 -412.58326988620684 11238.795702766789\n",
            "-256.7236987547882 -315.54274705797434 12560.676806751639\n",
            "-75.29739234954717 -155.41906721144915 12548.155137374531\n",
            "-252.4808457040728 -230.12603795155883 10630.960815988015\n",
            "-174.578057758397 -188.09552364051342 10144.583092533692\n",
            "-75.6035556103237 -135.55720138177276 19623.26425674022\n",
            "-210.40699771086554 -329.2938822815195 10580.820257828156\n",
            "-91.85468534977396 -379.98085625097156 10626.105069041587\n",
            "-195.6827784982967 -153.99290957301855 12950.570861395914\n",
            "-226.64613327707838 -349.7420722693205 11136.843153536553\n",
            "-171.5716372470559 -235.41750482469797 11375.215874683112\n",
            "-193.9499320880547 -192.66773618012667 10983.203389706192\n",
            "-133.76826187153554 -149.76815467327833 10664.94052854355\n",
            "-374.3149648978171 -530.7943019466475 11393.742325728497\n",
            "-247.42001795566645 -308.2081372421235 12222.694663287599\n",
            "-183.58868023537173 -131.5148776061833 10715.772654966626\n",
            "-114.84059230263216 -201.74303843965754 10762.378226724017\n",
            "-19.477555628823126 -109.58385236809409 10677.216001918663\n",
            "-414.7305143691959 -557.7128756381571 11755.653854109463\n",
            "-127.89325153957503 -197.90620564110577 10891.377304713082\n",
            "-16.87921843133003 -221.79381970781833 10516.109920316681\n",
            "-269.0966444339283 -170.24061161186546 11845.450550829992\n",
            "-46.60310885382739 -86.8362808804959 13430.262226551014\n",
            "-238.29719542891132 -305.09805746749043 11309.886370100001\n",
            "-282.314252464049 -129.49685902660713 12699.966535139742\n",
            "-228.1141983811852 -146.87875949405134 13057.711478811412\n",
            "-173.4640385098312 -208.36431528715184 13278.442650823625\n",
            "-149.46680424810393 -154.83498964086175 11345.282736952417\n",
            "-282.19020886052044 166.46542285452597 15873.075901123055\n",
            "-215.4113262934628 -608.6437344960868 13053.380485319067\n",
            "-206.37307391603292 -147.3196512865834 13935.931855633637\n",
            "-170.08799448091867 -145.18001460109372 13990.963015376065\n",
            "-118.22567229340682 -360.19945383118466 11102.940471303184\n",
            "-252.9551699099592 -241.97195150796324 11367.996179586342\n",
            "-200.42114105214048 -62.66789129702374 12683.275316192885\n",
            "-24.979924272202098 -117.5133034219034 11914.202583798382\n",
            "-249.3838546232528 -160.89208930643508 11684.575059599565\n",
            "-45.24540389279438 -162.32692190632224 11490.476112493168\n",
            "-36.59869005186971 -99.62148974556476 15596.410614756489\n",
            "-239.85404077413705 -355.22264743666165 11474.505433240873\n",
            "-172.73038122768162 -161.1972680214676 10977.417095262412\n",
            "-314.75212411954203 -506.1726519801887 13454.755964472353\n",
            "-100.92975748433716 -266.8821932338178 11398.174663151149\n",
            "-62.35064304441067 -138.02083045407198 11169.72917918206\n",
            "-99.22561611130823 -90.10650189896114 11470.126181935893\n",
            "-140.08008546188998 -129.68102067965083 10620.907160919305\n",
            "-46.092676883945664 -71.69905590597773 11214.230570783333\n",
            "100.96073925690821 -32.72321762882234 16950.88460570085\n",
            "-99.94286986796348 -100.47430035588332 10919.658996565136\n",
            "-177.6607850399746 -158.01482589333318 12307.004075900099\n",
            "15.33023698953221 -249.83857319771778 13861.452992783252\n",
            "-44.078505249970775 -258.2120202560909 11706.485927472619\n",
            "-158.3288699960732 -124.69074202299817 10843.732188450253\n",
            "151.12933093516259 66.42654426547233 24286.581086777445\n",
            "-167.61546328453443 -166.4496518871747 10282.128923230339\n",
            "-83.47338804410349 -293.7482718608808 11755.361602005287\n",
            "-196.23352454055555 -177.30479850812117 11533.516373153761\n",
            "-215.5098476637838 -431.5331574464217 11642.780189898796\n",
            "-179.02205313977396 -88.88062505080597 12527.000504605589\n",
            "74.92767657526859 -98.88002755741763 6766.768499184702\n",
            "-29.35895386049701 -127.34329975256696 11846.081546179921\n",
            "-183.21269324476813 -78.00810052966699 12999.713201610371\n",
            "-195.18892678535866 -158.4083781701047 10303.227285125758\n",
            "-239.93451888304622 -276.86473158653826 12227.617184971008\n",
            "-12.70623383280379 -101.03921476239339 10978.587170776096\n",
            "-135.39167151081114 -99.02479096269235 12537.124696369574\n",
            "231.67846074651607 -16.15366303972221 13734.364102880496\n",
            "-44.564363689024574 -131.8784105440136 11463.977973298053\n",
            "-45.301912300154825 -49.294271271442994 11345.023183112025\n",
            "-210.47236055210678 -286.39883518603165 10390.452869857185\n",
            "-71.55816286117593 -250.44172148313373 11281.803284762413\n",
            "-14.105661904226082 -129.01913617318496 11021.047581981693\n",
            "-156.415880299506 -195.59780076015159 11123.902700533275\n",
            "100 -151.50667258368853 106.31690398030841\n",
            "-205.14952990387707 -145.48901385453064 11493.12578556023\n",
            "-16.663620230698335 -32.01387459150283 11011.80872663384\n",
            "-77.4518471109425 -174.28205360885477 13508.287109632642\n",
            "-54.67676912903446 -126.69994238577783 11992.06608055919\n",
            "-171.03910020410427 -197.44570905377623 11103.828245009077\n",
            "-3.020288763835211 -85.61610682075843 11443.620836058923\n",
            "26.52904747236486 -295.5539168387186 6919.69573305848\n",
            "-63.05046942498044 -10.73822467844002 10950.62835067045\n",
            "-203.62309804189886 -214.1038700346544 12620.269632745334\n",
            "-39.33096311338897 -97.18505078881572 13879.03353762794\n",
            "35.24882289314675 -111.97388686925115 17472.701655009852\n",
            "-30.99158977996416 -263.91716379194986 11117.282056431402\n",
            "263.5744419664756 -24.256091907092497 14005.456964868532\n",
            "-209.2246066459102 -256.32578325293434 9392.131000958158\n",
            "-179.76320143367494 -274.0545172415368 10051.388264750292\n",
            "-215.09029286879428 -167.37860383541556 9942.572819728724\n",
            "-216.43356371602505 -77.62766421026754 12911.394305164635\n",
            "-49.48463589448951 -115.02606191474479 11162.64636609191\n",
            "37.65679990662197 -189.27910958451685 14003.984345130273\n",
            "-176.9792082171523 -93.87891448669143 11553.327422692182\n",
            "12.163335424429164 13.909823563648388 11609.751807393484\n",
            "-25.023688857394614 -36.422306556254625 10345.217291838024\n",
            "-42.921047847205 -29.569202240629238 10923.032171812698\n",
            "-174.0771753941462 -253.60972253920045 10156.309217954462\n",
            "-205.3898876022483 -76.46678055197117 9198.281383347468\n",
            "-202.7595603636552 -43.93760114764882 11293.665330572287\n",
            "-168.75715273527416 -99.84727508039214 10939.272744382324\n",
            "-207.15871451698501 -82.64186149905436 11366.416691336282\n",
            "28.784163914861658 -0.09685059578623623 11022.197348058631\n",
            "-163.06229128296957 -88.43652418046258 9997.58387338792\n",
            "-71.75377333165093 -88.5416646861704 10238.163587042189\n",
            "-32.40410238844222 -43.17101813550107 10396.172925558058\n",
            "-259.62935763692377 -112.13054570385066 11507.078163120124\n",
            "-39.03747456307471 -119.70945966767613 11703.08531380209\n",
            "-195.88299771982486 -105.29786219995003 10439.757918126576\n",
            "-166.71233621072054 -105.26405427284772 10366.193586359383\n",
            "-263.01343755857215 -343.81919325596573 13524.164985194271\n",
            "-9.658703367124026 -156.54262320371345 10263.938700276689\n",
            "-17.12571942941691 -133.40418367576785 10596.002233312349\n",
            "-42.768238841555046 -101.73414515180048 10661.331704356882\n",
            "-17.920218010225852 -176.94570330617717 10819.19014696253\n",
            "-74.7079287142557 -43.22183491161559 10486.14059122435\n",
            "-14.96955331024979 10.627416856761556 11258.307474504814\n",
            "96.38824071556434 27.029082044635288 5767.960235787816\n",
            "-14.219264205815541 -36.13146854110528 10864.667347638402\n",
            "0.6881187364186765 -173.36436814256012 9785.73238133872\n",
            "-187.50916753078693 -296.4563655029051 10426.702975596218\n",
            "-15.40784452270503 -93.15240469202399 10559.254868041433\n",
            "-178.9151590512244 -40.16084870816121 12078.646744500496\n",
            "-152.97445500637312 -91.43798161989253 8655.896160924109\n",
            "6.619551208147442 -83.55639606727345 13211.557429164852\n",
            "-27.314159219330428 -36.190417523961514 11431.339222232345\n",
            "-174.55235681462952 -111.0343700086487 12468.610477217037\n",
            "-8.295818726551985 -109.7451050959935 11078.869106362865\n",
            "-192.76116549259106 -234.10522218741244 8945.557139271463\n",
            "-220.82534141576335 -220.36514274368528 9275.359718128733\n",
            "-189.31434875979664 -167.82677927202167 8536.43266469927\n",
            "1.2591161996699256 -16.09173178311903 9674.494392641562\n",
            "-53.23426681348325 -118.39297346916283 10345.571351294057\n",
            "-32.206935099293965 -91.74881876230938 11519.121723082964\n",
            "-186.55084186661747 -11.378428831812926 10081.514177700796\n",
            "-207.87617992638627 -222.70814180084926 9371.090655641608\n",
            "26.631937536952996 -96.11954794058693 12376.598980291645\n",
            "-195.10842091533368 -134.51286947191693 9131.874529523659\n",
            "258.53787537545196 -7.836822131459748 14420.229497612945\n",
            "-175.4509816451466 -59.309325819029254 9790.65238942565\n",
            "-175.65794032746217 -104.38020456758409 11294.060709697893\n",
            "-227.23526400978076 -9.403395747724062 9230.56431099652\n",
            "-243.53380018485916 -134.69542578501296 10075.067564009256\n",
            "-168.13642513638524 -56.77897275616124 10005.196231660175\n",
            "-217.07880912414058 -155.51546659687665 9812.979324627031\n",
            "-118.24513823911414 21.263045173632236 9888.11215406128\n",
            "-186.53116032353577 -120.59554695338011 8959.390472529307\n",
            "-28.957473983986503 -111.52346810069866 9938.36776299501\n",
            "-201.24856917095173 -381.80572585281334 7390.059516680634\n",
            "-50.99149230675414 -17.962311957002385 10549.393589966832\n",
            "206.71028589412668 -15.288870458058987 16289.357031058622\n",
            "-193.5557036328859 -37.940557366935536 8540.749711388347\n",
            "-174.50013848265843 5.89488048304338 10139.528543165303\n",
            "203.13260128592142 -48.5387469382715 15895.163214172277\n",
            "-57.9627715991131 -67.16450842893391 10229.86683724774\n",
            "-26.974382154713457 35.36336753550859 10395.086722071024\n",
            "239.5813274061142 113.12151748758338 16069.754033401186\n",
            "23.57002771480032 33.104154010128696 12845.154799221084\n",
            "21.44179557853218 23.60793226977694 9458.006018900895\n",
            "-174.23304185228557 -95.43006001216963 10541.238779133433\n",
            "-200.9476758013006 -75.14162620253046 9909.204207452887\n",
            "-6.466886116693743 1.1307707422965905 9977.481119868782\n",
            "-176.65618602083043 -82.89020256907838 9985.347274335567\n",
            "-26.468137200706437 -71.48853231398971 9394.456994798504\n",
            "-231.0471688931087 -64.65287831459227 9123.047237459308\n",
            "-142.42704124818675 52.655455940635875 11345.798800690332\n",
            "-198.2934090071669 2.12891863124014 8558.607550192159\n",
            "-185.9528347196146 -12.719240564189931 9224.193295989768\n",
            "-247.12562213764664 -132.73305030341726 7786.178394651215\n",
            "258.78024953112833 80.73203451397795 17581.84446758963\n",
            "-159.9167841574176 -109.98854702457902 6735.987272094942\n",
            "-238.28108308861852 -40.73349674855889 9409.531521509576\n",
            "-193.52778256098256 -59.010101413019584 9995.055542089976\n",
            "-237.321674782574 -176.83602752769366 7389.824672871633\n",
            "200 -92.61199508675261 123.73024605457478\n",
            "-200.19182688887088 -77.52880034772079 7497.997163996915\n",
            "-16.799939174714048 -3.0651719510933617 9406.847372534452\n",
            "-247.02630264651168 -126.82564340200042 9197.870727795278\n",
            "-15.527802618384545 62.51467531949311 11918.18300874914\n",
            "-179.08612040224295 -99.65884724722855 7673.87224633107\n",
            "219.2887948255084 -55.27029914405895 17725.852315506847\n",
            "-243.85189008274034 -147.53385416338278 7104.832291732193\n",
            "-268.9341015219684 -176.08342372454354 8128.962974231719\n",
            "-218.8426727182398 -132.87861386858276 6701.227668184278\n",
            "-49.82827423924542 -24.14639920933405 9143.996221733061\n",
            "6.167444457762404 -255.1362852306192 12034.898478326344\n",
            "228.5643589760361 25.931404073635655 16864.048511784873\n",
            "116.6822632821359 -18.323362852621358 4268.808003832928\n",
            "-188.2776274921483 -6.740999946487136 8735.934426126132\n",
            "-221.53359185550823 -154.94884710667247 7374.187067751773\n",
            "-21.72025678395125 -58.34302635124186 9556.815798757132\n",
            "13.135173532396642 -61.773444433303666 11530.82231903501\n",
            "-17.774011392472694 27.08859715264407 10064.814740022062\n",
            "23.630867121838428 -173.3114613068392 4943.114281281064\n",
            "-242.15716984747047 -262.34751590836095 7603.133323681308\n",
            "9.485907419860325 -42.71345175587339 9278.354836653714\n",
            "251.38764202197177 5.741907271185482 16110.829217402414\n",
            "-204.7366203312951 -238.9579553171061 7676.398357207541\n",
            "119.15709487793202 -32.61783384244495 6884.583382771172\n",
            "-36.69844496853595 -8.37360775857087 9684.026944658865\n",
            "-191.47760770984522 -115.87891568476334 7050.230053631021\n",
            "230.35370338104406 -28.23772128611089 16621.82706869698\n",
            "212.9622488265769 -27.17183708981247 15284.026878991748\n",
            "33.08521829088865 25.795876937841967 9552.42739280314\n",
            "-24.15473668824246 -17.199080742080696 9669.813544128789\n",
            "-173.20962562356166 4.244570961012357 7946.557902176635\n",
            "248.55298836693925 36.59213831750014 18390.221677556423\n",
            "-191.5845450570509 -24.849639240419492 6691.85071876139\n",
            "-14.183515659250169 22.06807369717353 9268.852622024657\n",
            "-213.74377812746096 -140.0052994361613 5586.913824077463\n",
            "-238.89159607009427 -226.7574186400161 5435.857688295513\n",
            "-230.9082466868997 -350.0070682200312 7800.675823080805\n",
            "-254.4060416046929 -257.07557396271113 4904.2953781542055\n",
            "-28.460700092052853 -33.918050076905274 9202.10050272569\n",
            "-8.338692615391082 -33.09696614976565 9127.743326159078\n",
            "-189.14528241564363 -51.06049707042439 7002.650799993658\n",
            "-161.61712367896328 -129.93426082160295 3732.4507556566714\n",
            "-227.2699343865043 -73.1159753332031 6074.059599595101\n",
            "-186.12082358062014 -50.90789845593076 7824.770130464924\n",
            "46.40456156247848 -39.1399990839127 9324.984662377741\n",
            "-188.84393717966012 -140.23814310691523 3514.8472542450763\n",
            "-219.02118817848577 -35.03219398826468 7561.588604306278\n",
            "-2.237680698293417 -73.35418606060557 10293.06233345531\n",
            "-5.315054214735818 -50.87598162326458 7671.3691070012865\n",
            "-14.388295016807088 -59.87802665156778 7678.403540255618\n",
            "-238.97616571489536 -23.5657527798704 7051.753640590934\n",
            "260.5909987360452 104.83100103502292 18898.3716787228\n",
            "24.662540740810087 32.82263449092716 7760.49997901138\n",
            "-208.66252412983513 -116.71609727782197 4333.851139949402\n",
            "-33.62788103066982 -29.95797093171859 7674.320581078762\n",
            "-151.4799587849284 -33.59884915081261 8580.000780344475\n",
            "-256.14038349098405 -96.75092949997321 7623.222495461159\n",
            "-87.36736036233481 -103.01882868530447 8212.962955189698\n",
            "206.58276812827881 -100.28547473759014 20336.157245090708\n",
            "-33.59506790363242 -49.55019806589553 8132.695052221257\n",
            "196.03820113361132 12.490986707402445 18015.27507800341\n",
            "-220.24813944249945 -147.9388468763791 6017.816019131162\n",
            "280.83824407165855 155.55592057109956 18372.04853694719\n",
            "303.36549407666786 3.266962778167566 22974.364959753184\n",
            "242.95641893046775 -8.774768412333561 17919.90478019855\n",
            "-230.5532362522291 -189.18921266153484 5841.3634074326\n",
            "-76.66142641962294 0.5739482509670779 9459.94428551558\n",
            "-173.99844974445742 7.044155602316323 6227.7106027568525\n",
            "249.15242793065465 24.261146108528084 18516.780714556124\n",
            "-200.4299467255142 2.859460989748186 5485.556211318879\n",
            "218.55512060633382 -60.30713501370439 17257.849010014485\n",
            "-45.26414474217539 -115.85570643199026 9274.21185606328\n",
            "-11.09993622950661 42.38751277207484 8285.321709433862\n",
            "-233.90131934131102 -57.3822558486645 6441.629737098716\n",
            "-200.2312406070222 -191.33866049534117 4419.364946503985\n",
            "-16.346485243173902 -93.83008027262986 9558.957949866835\n",
            "278.1976600497423 59.975163117262255 16806.016915079563\n",
            "-208.54540020028617 -52.33523355698162 5817.465807577013\n",
            "-213.0280975972292 -52.17231024475768 4882.303519906622\n",
            "-10.35271523351571 49.82126574026188 10123.885705913159\n",
            "-199.10894690102904 -91.79413709588698 4661.026960551229\n",
            "-194.5115789018539 16.4280925901694 4063.167429957786\n",
            "-240.88456377568951 -114.32428789483492 5835.9690132870455\n",
            "-30.466533563898267 -60.992917083262 9274.752555842675\n",
            "-203.51638021297893 -22.81331327772068 5608.481311030511\n",
            "-74.7505888959438 -77.01533579966053 8536.97405877299\n",
            "-178.72477426376076 -24.79017172032286 6888.179914616048\n",
            "-43.382215527927194 -55.689484946815355 8407.353207645938\n",
            "-201.14847797787283 -8.937502961521204 5186.214167065002\n",
            "-38.030347687337475 46.07951128749119 7827.325384632659\n",
            "-215.9920234516329 -2.6240710433926324 5865.5818280615495\n",
            "-190.79063048576273 -128.3327019536373 3635.7566232227255\n",
            "-195.6022202731043 -88.2970087530266 6175.64905136032\n",
            "-38.29491966922983 20.886017404438462 6692.990682763106\n",
            "-18.40514386928 -104.58526925079059 8279.134429055615\n",
            "201.33253974413762 -139.39881177995957 19392.95402150672\n",
            "-220.82224843128375 15.157816933115328 4830.83612609905\n",
            "-164.86770857798888 -16.924188473973118 3134.9537683842245\n",
            "121.80846022959707 154.26402185353072 2756.835307200956\n",
            "5.791206612378261 -14.4819120708853 7361.25215567743\n",
            "300 -60.873838919772005 164.00545064849695\n",
            "-162.94194222821992 27.467210194430663 2733.395168022984\n",
            "2.1061102909153053 20.797940638813543 8441.54653795436\n",
            "257.6951150411438 134.6838550101589 18425.84049977034\n",
            "239.5704732519987 -39.93923772162874 17798.888666831463\n",
            "-170.91426686971408 -28.645877847542465 4454.440733925629\n",
            "9.294502949836275 -4.974955952502114 9137.087420080032\n",
            "-172.60917146454128 -34.68565671531542 2979.306212160227\n",
            "19.18042607687412 -206.34902156578028 5890.897630625055\n",
            "-9.530959590768274 46.61671203331207 6180.077307073341\n",
            "21.801937172966525 35.85409776328197 9818.27340057306\n",
            "-128.75942452867747 1.7936614812351763 4408.552465792178\n",
            "17.01131416933906 91.02609830684378 8373.759189091856\n",
            "116.39448261072947 39.29170387696786 2459.8107924436918\n",
            "226.1582676962812 -32.76850903836068 18219.35391230088\n",
            "1.1525179627386954 -193.7569946636322 9230.658178046608\n",
            "210.20351915951883 -62.781801594180365 17700.810801827643\n",
            "75.98898356075645 -20.46064174361527 2798.358420058714\n",
            "-41.33115219843458 74.92688125848917 5075.468666497385\n",
            "-151.24522920637727 -9.38152708438247 3525.4746275226353\n",
            "-182.736053435838 -42.22507645322548 3640.8037974700565\n",
            "-14.07786619914063 46.661749618826434 9537.826197570204\n",
            "231.990184040155 39.684672554552286 18352.23964679817\n",
            "49.9950387871652 -176.22681203890068 4463.111774244808\n",
            "25.403385286993075 17.477410696435413 5534.96062693531\n",
            "-35.49469351390402 54.58378961020935 7995.132947718288\n",
            "-30.698460762624133 -94.52343907784962 7546.707980614854\n",
            "-164.36376376853107 55.473458676516884 3176.6395325957565\n",
            "-148.41594175305391 2.168081786510129 2769.761155916378\n",
            "-4.17976663667201 27.45094279316163 6530.604376128409\n",
            "257.7779606356978 -14.340309888006725 19595.04685196019\n",
            "247.8031822694383 48.851401600890284 17825.971907092735\n",
            "-28.25768845343815 -75.31523911472323 7903.2738427137665\n",
            "-106.63943691715913 -258.01708591837087 9042.369105522987\n",
            "-44.85592041401854 52.1207027860728 4401.150598494467\n",
            "188.50473863746635 36.21471525754441 18459.205075316062\n",
            "-199.48248876172363 0.22180725971702486 4398.662675652959\n",
            "-161.74435912598958 40.5288367660867 3011.8510725850356\n",
            "223.0761804532776 -9.90048313455625 18171.01126279056\n",
            "254.25773008417067 22.43231420575271 18423.03769062071\n",
            "52.04183324311693 62.432597725117375 8649.980459869083\n",
            "253.09752482415925 43.622433965033295 17454.264281482836\n",
            "-182.6680641408052 -26.64554623454319 2435.0682853639155\n",
            "-145.42879519344177 40.76790578164184 2663.3831850336865\n",
            "0.27516747094301763 42.17803787626326 8280.592159578853\n",
            "285.0968436251154 96.07122744250407 18324.31376321035\n",
            "241.17061369972052 98.30692685375789 17682.284113997324\n",
            "218.14366877241528 108.3442327353599 17348.910119781376\n",
            "221.47257836788884 50.96429575236898 18855.92096408849\n",
            "-20.77276881797539 -88.63096767218667 8882.984626498961\n",
            "283.02263310170173 25.981051328692306 17774.128747534043\n",
            "-114.83071340986959 -109.87149539715392 3086.2580617001076\n",
            "19.17999769822282 90.29397830850792 10075.991052267436\n",
            "239.86232073631177 61.00598517440369 16807.160523400908\n",
            "200.61733188765277 31.68929225404447 17092.663827689892\n",
            "213.22237398175716 47.34027509716102 16706.93545369981\n",
            "16.457180534823834 53.10432217781636 7189.302218069264\n",
            "107.5828025946058 8.756794515493311 16976.52325991221\n",
            "203.650436953266 -24.68389101132518 16182.457461223297\n",
            "105.83203641357969 -39.59313903856673 4806.383861480015\n",
            "-178.0509620550813 -23.22937817436332 2620.9873141469434\n",
            "-85.76561494484734 -69.7185072141001 5815.561802452459\n",
            "-150.75094930972912 -46.60254159456963 2356.1358302415465\n",
            "61.972572866902766 100.02921894702013 8645.3983858063\n",
            "198.35924744950137 14.100062676677894 18297.637475242278\n",
            "240.50607198204472 123.7972252834582 17376.949853426686\n",
            "243.40776138237453 72.17391992996777 18213.392877480903\n",
            "298.2776472018176 98.73334669031186 17612.80814855946\n",
            "215.0696072322169 19.1618768004178 16485.275710344133\n",
            "242.45754048187803 72.59811844085347 16921.45102318296\n",
            "208.22083500108877 68.91135530117987 18020.267089479363\n",
            "-30.768575912667107 7.403381029609591 5702.408430381944\n",
            "-200.47393948026965 -38.277502325800015 2930.2905777875567\n",
            "211.85337418356434 -35.53098064821429 16614.939396434514\n",
            "192.6715488927087 -41.84885656813948 17880.745118136456\n",
            "230.0077329101095 35.60826532376904 19779.17346681738\n",
            "259.62734166281996 96.26164803148406 16166.722636400056\n",
            "-1.9880998807981385 -62.56458396790549 8907.195724401143\n",
            "8.98643730547713 37.008741299495114 6450.149332625326\n",
            "266.9565795200947 79.16658257429478 18402.7143474921\n",
            "244.6634852894062 -19.524153041135882 18291.071018026152\n",
            "20.602709848149154 67.16330930696483 9984.740982711548\n",
            "173.73961194306054 -92.47533501721078 15506.969916569673\n",
            "-213.18470926040752 -63.06243525413447 3171.121892120398\n",
            "25.603703242236655 26.10960876049495 3488.4684987426735\n",
            "45.608272274712284 42.681064505828544 10366.047506070012\n",
            "-218.958786165616 -39.36380591057241 4991.926861978631\n",
            "-11.217182150507547 -56.766101878805785 11037.904506518491\n",
            "0.8466156047527349 48.54169798102885 7548.114960227933\n",
            "275.7742143299751 83.62444176547 17199.827820566676\n",
            "220.52113881982308 54.209066870540255 16214.491228986146\n",
            "-10.276950460749042 -2.0652924068344873 6736.470282686132\n",
            "28.580930973973356 -81.50235144747421 8350.889399581123\n",
            "-14.829867408571435 -6.512012299528578 9534.826867025025\n",
            "113.75987281308298 3.350496331296654 1918.1396766671278\n",
            "212.32753604593742 -36.34699952126539 16101.222110793111\n",
            "11.0391481118195 60.70825769197836 8392.912859951859\n",
            "-23.635181686599175 67.20221772137342 10564.484448879375\n",
            "265.93483575370203 84.73355107591462 16990.124244231924\n",
            "242.16479684415782 148.06837887828036 16889.72348090856\n",
            "225.10930477460644 14.42446566557124 18001.82033683303\n",
            "400 67.58862122677976 154.73060258569393\n",
            "-136.4069445468868 28.328055981893613 3775.604590737028\n",
            "16.232912540629314 38.64685698714206 4359.47449552\n",
            "270.32941119727366 133.48734369851059 16719.83455091767\n",
            "202.78437867233708 21.263367994794198 16103.7947734488\n",
            "227.673105440052 69.61724947910574 16192.491910515084\n",
            "61.44621423414452 -100.65304320998982 1139.7571786695598\n",
            "107.18015948862642 -139.69328303676957 17754.58314052392\n",
            "190.94594435258836 6.746698946244578 16248.37916520582\n",
            "-131.24328782362855 -102.52616069023134 7530.466136311985\n",
            "8.312142824582686 65.11090598354909 3367.6014885297045\n",
            "17.998003863254258 85.31886574200234 11514.460393901914\n",
            "111.05232986831243 -48.947036927500676 1105.1617428689242\n",
            "221.16926890216024 -2.5068035554423957 14475.423005156332\n",
            "212.07441201233473 33.55419283473735 14285.56087161793\n",
            "113.52962822809914 -132.72350885297055 13925.646234742428\n",
            "52.783122436118674 -99.53311967645277 1615.2304793596197\n",
            "146.4120325625804 -64.02483772019696 15107.028295295113\n",
            "184.62379964966527 -23.398680664598942 15364.816257876186\n",
            "226.22942911545454 -33.59482150981687 13397.237387528181\n",
            "152.10129694819824 -63.520030682426295 16900.376919020397\n",
            "72.18341354515204 -238.61226641256565 15740.268927809233\n",
            "124.83103227024998 -152.65646369218825 13078.13669888785\n",
            "158.23726773464534 -83.00182004427188 15856.780425281682\n",
            "-83.58518790850447 -100.4786919646358 6148.938268702726\n",
            "-124.99034775327581 19.810083646094427 8061.522098300273\n",
            "268.7169538679984 -61.499634609124826 12931.65080810399\n",
            "-101.45319149953352 28.18403451062477 7387.556732427491\n",
            "-69.04961413320747 -39.73395084993001 6217.398705325549\n",
            "-35.13504621105423 -2.4839110254915795 8101.270760149453\n",
            "-53.632791710724774 8.817867211830475 7209.04421027255\n",
            "-56.216394730610446 -76.86524779903084 9297.135309859717\n",
            "217.52100743071506 -50.31508485950363 12949.652031824413\n",
            "-80.88322569624812 -65.90732035510428 8545.635677864815\n",
            "-160.75271697284137 -249.27568638138473 10880.287665302667\n",
            "-165.02767714740816 -56.87410518703109 7798.61495298629\n",
            "-60.873369163784474 -4.710866662589979 4729.588943722405\n",
            "-94.21098116647988 -59.29550947599819 7853.02700183745\n",
            "-93.46614252972269 -151.25165873632068 10572.942226964533\n",
            "-82.81117991225656 -70.60029838885748 9254.239440480032\n",
            "-102.27490168852877 -89.3886031911266 5903.397200697021\n",
            "-131.24202372325752 -126.01873475654884 7951.479953172931\n",
            "-142.74538161236808 -40.370421459667 5925.181186320659\n",
            "-61.992937722462756 -26.379198003357715 6550.735303981522\n",
            "-89.97424254546547 -130.20889164577238 8461.932321017153\n",
            "-87.06987948460817 -127.82251968125638 9117.508575760874\n",
            "-120.31982248285924 -82.07069362862967 10981.159535516133\n",
            "155.53637701901914 -115.61937260075194 14442.42563925488\n",
            "-92.59183309961716 -8.15784225531388 10806.230849802694\n",
            "-83.10479474354496 -65.67041609113221 4317.074309072868\n",
            "-90.93048027415142 -89.64032722533739 11189.412710880437\n",
            "87.22422944815038 -212.34236178223867 14982.283717648288\n",
            "-48.40558496372162 -248.38090275262584 2208.242923465432\n",
            "-122.84974264060439 -86.03390231006779 10311.591133728518\n",
            "106.70645468241256 -53.65076278658367 16451.419493381803\n",
            "185.68687206831424 -63.97935564647301 15051.879574330138\n",
            "-51.5071474356023 -325.214510343445 831.3647383315856\n",
            "-156.86680227121258 -380.16417891322635 11618.73758507171\n",
            "-79.52323658581574 -82.5458886041306 9983.851071856217\n",
            "-69.38699156567756 -69.09298959042644 8907.566385136319\n",
            "-96.60046370833955 -164.56028004083782 9739.497504619112\n",
            "-104.37418785450623 -40.91805525646487 10055.553849259759\n",
            "-101.46845944401137 -67.07034110830864 9659.282986591003\n",
            "-105.36733760246982 -78.93830493622784 8177.0387167800145\n",
            "-101.63391349531202 -15.398516373467828 2907.796153400326\n",
            "-116.93357678052797 -203.95419058576226 10741.304549786462\n",
            "-146.0631981704247 -238.05904414213728 9999.243754964198\n",
            "-109.62423945386846 -42.78640912098308 3553.3786504796517\n",
            "-178.14593895057794 -268.63001354075095 4664.772062346707\n",
            "-120.80726342211788 -189.91410362751958 4506.865197400635\n",
            "-91.23568943259053 -102.56890596290577 4900.040498097984\n",
            "-136.8264925395974 -83.9863861942822 5761.685892873456\n",
            "-138.50916947699392 -261.1176035996468 9491.709615115778\n",
            "-136.7229684302389 -171.1418882850121 8363.885002600728\n",
            "-108.20479224109951 -166.18595179426484 8259.591817785898\n",
            "-88.59510025863905 -75.0847714575375 6086.323588674295\n",
            "-87.59548673504031 -38.129443091136636 11143.522013794533\n",
            "106.48611236207068 -182.6049643748199 16170.656165929588\n",
            "-108.18579126856437 -69.03932251327751 5084.489069439446\n",
            "-96.83058492573605 -63.567450845508574 9156.366285520286\n",
            "-88.55991440643395 -73.3474182006903 10276.83008359556\n",
            "-103.28434670711746 -127.85861086996738 10904.092450235807\n",
            "-178.93308124593446 -229.07540068111211 3085.829202372276\n",
            "-143.76684547799442 -39.99626527653281 4940.789717103169\n",
            "-89.89077232153933 -81.85551257987066 7544.101420853709\n",
            "-107.73071424822598 -61.82976054106348 3128.5901322355867\n",
            "-113.49374216564392 -37.77220642150496 2028.261118442042\n",
            "-166.0126503801863 -130.58192316492637 5741.030599073869\n",
            "-118.40166534667966 -13.776022866086805 6169.077570162104\n",
            "-108.76911993421 -23.394901826104615 6940.55533353216\n",
            "-172.2035606486122 -179.05394835150219 2242.9854971807727\n",
            "-145.50584025758883 -8.69515722546248 6305.989605152883\n",
            "-110.6468078371422 -59.775488172410405 5801.335303538523\n",
            "-81.55539401982283 -56.67408895504195 9567.663928111207\n",
            "-155.20312617130634 -145.03716189748923 4337.818515267746\n",
            "-112.18762339443462 -77.77235059112809 11108.570215886397\n",
            "-105.21301195623661 -103.57959624379873 8216.043534900848\n",
            "-122.50587423384478 -83.65822524702105 2044.1956768450327\n",
            "-132.2298504996502 -88.12540327082388 4601.699064526154\n",
            "-222.22093209843422 -108.45817481040649 4699.460340315927\n",
            "-121.75848403720897 -139.1867681118565 8541.891724806894\n",
            "500 -39.28314600559428 123.70447629609252\n",
            "-94.47921811354797 -52.28667627694085 2581.8913186782393\n",
            "-74.46618809087207 -20.04657125597805 3308.9785669986013\n",
            "-151.30053596093381 -27.300145804327258 6536.605406955329\n",
            "-85.88750608219173 -184.2273169751279 10984.052566865394\n",
            "-109.68166112158268 -59.32069446472451 3298.733071582371\n",
            "-161.78856931514278 -143.03832425623114 11373.959255094791\n",
            "-113.15291230769645 -53.53608506836463 1873.4046627598036\n",
            "136.06469716371797 -120.16564129694162 21256.555440173484\n",
            "-235.1844043234256 -111.66569317995891 10401.912072070269\n",
            "-95.08961808415586 -87.11333019490121 10452.930950457929\n",
            "-161.70120228418608 -184.70583863480715 6242.465914078345\n",
            "-123.55661958320434 -146.8614026725263 4206.21111309782\n",
            "-212.74322048749704 -227.30917547389072 10966.767328307194\n",
            "-129.73447386475652 -54.76974993472686 7608.029018376052\n",
            "-115.09347051838898 -182.76893116394058 11273.804542672638\n",
            "-106.17612249717854 -96.27953419738424 1416.221333386093\n",
            "-84.13446102880539 -288.2504264715244 10591.530562909844\n",
            "-173.09869829287706 -91.57524843035208 4914.182460848599\n",
            "-144.90007829950284 -59.086750528274706 3902.4028060478313\n",
            "-104.63522896325776 -96.63574751246779 10097.83838615224\n",
            "-140.0575285592804 -40.37787717557512 6389.441131469519\n",
            "-105.97028851014748 -88.13674710318446 10437.68919574871\n",
            "-154.81631260974893 -282.105436460115 8927.772355895373\n",
            "-134.74428443796737 -115.45374669715238 1812.7344466046166\n",
            "-114.50341248556052 -49.1595589867793 10814.773949693767\n",
            "-101.55656260672697 -75.2841194444336 10778.548578689493\n",
            "-95.99607775545465 -196.01060598692857 9846.741975210272\n",
            "-132.03866334498125 -173.0513306442408 10222.21214284867\n",
            "-118.44501762136998 -206.70807032167795 10451.788161860008\n",
            "-82.79546479643071 -146.7233226613389 7590.93139269622\n",
            "-178.04040619592428 -159.84547939745244 9938.576047909599\n",
            "-134.72685563662338 -159.64950556101394 10104.912191981379\n",
            "-156.19694423858147 -75.66358095267788 7076.810911041059\n",
            "-166.27146033052338 -163.31366703354252 4337.609156030136\n",
            "-190.37516320523628 -123.35242734313942 7570.765708952589\n",
            "-133.6634526311643 -147.480122084713 4935.055160908225\n",
            "-100.53130761129239 -63.95057784467144 5565.467490179224\n",
            "-74.11047293744282 -66.51853188248788 877.0734849988366\n",
            "-117.2855670723138 -35.47227197443135 2694.66900516898\n",
            "-149.2063249413319 -80.12399862334132 7241.435623319361\n",
            "-142.5455520538383 -97.68171802156263 6656.311656231592\n",
            "-177.50688223083213 -162.86777773266658 7603.46703919815\n",
            "-71.81837144860609 -43.224259143928066 1135.5411851744384\n",
            "-262.38418309031414 -134.21587289034335 4764.56455067686\n",
            "-134.05075092975437 -59.82058123243041 10428.271405617263\n",
            "-129.27674881209902 -124.2599842781201 9597.478039426176\n",
            "-155.03778065333654 -104.92431459599175 4281.126210829302\n",
            "-105.8290068638328 -117.02295639377553 10209.40912084511\n",
            "-175.41860417452864 -152.80239512454136 6814.893959832346\n",
            "-148.44912416274187 -320.3726935146842 10398.95803836\n",
            "-134.2757212128728 -108.40958404775665 1487.8687925964914\n",
            "-71.24840595583817 -71.99379891982437 628.0648236291872\n",
            "-118.83413708425059 -47.26542691333452 4770.5913732994595\n",
            "-111.94399037325302 -87.11349418168902 3978.340652656899\n",
            "-127.06376344733684 -120.59103448664246 6201.153970069526\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-696112ab3a4b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_with_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-cb0fd7ef934a>\u001b[0m in \u001b[0;36mrollout_with_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtruncated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-cb0fd7ef934a>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrollout_with_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベースラインの有無による学習曲線の差を可視化してみましょう．\n",
        "比較のため，Actorの学習率をそろえていますが，ベースラインの導入により，方策勾配の分散が削減されるので，少し大きめの学習率を設定することも可能になり，その結果として高速化することも可能になると期待されます．\n",
        "ただ，一方で，ActorとCriticの学習率の両方を調整することが必要となるため，パラメータ調整が実用上は面倒になりえることは述べておきます．"
      ],
      "metadata": {
        "id": "j5j-ip9ZcDxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = np.arange(1, 1+returns.size, returns.shape[1])\n",
        "avg = np.mean(returns, axis=1)\n",
        "std = np.std(returns, axis=1)\n",
        "plt.errorbar(episodes, avg, std, linestyle=':', marker='^')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "SOXTLc4oxCXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "経験分布関数についても確認しておきましょう．"
      ],
      "metadata": {
        "id": "pPZYoMrmdNjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=agent2, render=False)\n",
        "    return_array[i] = cumulative_reward(history)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "iVWlj9sJSXlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習結果の確認は以下のコードで行います．"
      ],
      "metadata": {
        "id": "O5QmAygcdr9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=agent2, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "TW0fJ1TswjLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自習課題\n",
        "\n",
        "* 方策を変えてみましょう．特に，中間層のノード数を変更した場合に，学習効率がどの程度変わるのか，グラフを作成するなどして確認しましょう．\n",
        "\n",
        "* 学習率を調整してみましょう．特に，ベースラインを導入したREINFORCEでは，Actorの学習率とCriticの学習率について，効率的なパラメータの関係を確認してみましょう．\n",
        "\n",
        "* タスクを変えてみましょう．タスクが異なれば，適切な方策（ノード数など）や適切な学習率も変化する可能性があります．これを確認してみましょう．"
      ],
      "metadata": {
        "id": "uwEbG_9Bd26g"
      }
    }
  ]
}