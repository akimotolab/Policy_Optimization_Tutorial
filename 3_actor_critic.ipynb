{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akimotolab/Policy_Optimization_Tutorial/blob/main/3_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備\n",
        "\n",
        "まず，必要なパッケージのインストールとインポート，および仮想displayを設定します．"
      ],
      "metadata": {
        "id": "qWe6P9xqH8be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXoBp0fgHyhe"
      },
      "outputs": [],
      "source": [
        "# 必要なパッケージのインストール\n",
        "!apt update\n",
        "!pip install swig\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import torch\n",
        "\n",
        "# 仮想ディスプレイの設定\n",
        "_display = Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "metadata": {
        "id": "iJ1PYM90IHa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "d5Oklvxh5nAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "続いて，第１回の資料で定義した`rollout`などの基本的な関数をここでも定義しておきます．"
      ],
      "metadata": {
        "id": "ci2EEsRpD1oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout(envname, policy=None, render=False, seed=None):\n",
        "    if render:\n",
        "        env = gym.make(envname, render_mode=\"rgb_array\")\n",
        "    else:\n",
        "        env = gym.make(envname)\n",
        "    history = []\n",
        "    img = []\n",
        "\n",
        "    # 乱数の設定\n",
        "    if seed is not None:\n",
        "        random.seed(int(seed))\n",
        "    envseed = random.randint(0, 1000)\n",
        "    actseed = random.randint(0, 1000)\n",
        "    observation, info = env.reset(seed=envseed)\n",
        "    env.action_space.seed(actseed)\n",
        "\n",
        "    # 可視化用の設定\n",
        "    if render:\n",
        "        d = Display()\n",
        "        d.start()\n",
        "        img.append(env.render())\n",
        "\n",
        "    # メインループ（環境とのインタラクション）\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # 行動を選択\n",
        "        if policy is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy(observation)\n",
        "\n",
        "        # 行動を実行\n",
        "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "        history.append([observation, action, next_observation, reward, terminated, truncated, info])\n",
        "        observation = next_observation\n",
        "        if render:\n",
        "            display.clear_output(wait=True)\n",
        "            img.append(env.render())\n",
        "    env.close()\n",
        "    return history, img\n",
        "\n",
        "\n",
        "def visualize(img):\n",
        "    dpi = 72\n",
        "    interval = 50\n",
        "    plt.figure(figsize=(img[0].shape[1]/dpi, img[0].shape[0]/dpi), dpi=dpi)\n",
        "    patch = plt.imshow(img[0])\n",
        "    plt.axis=('off')\n",
        "    animate = lambda i: patch.set_data(img[i])\n",
        "    ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(img), interval=interval)\n",
        "    display.display(display.HTML(ani.to_jshtml()))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def cumulative_reward(history):\n",
        "    return sum(hist[3] for hist in history)"
      ],
      "metadata": {
        "id": "9ELUIKjb5vY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actor-Critic法による方策最適化（online & on-policy）\n",
        "\n",
        "今回は「Actor-Critic法」を見ていきます．\n",
        "第２回は，価値関数をモンテカルロ近似する方策勾配法であるREINFORCEアルゴリズムを紹介しました．\n",
        "今回紹介するActor-Critic法も方策勾配法の一種ですが，価値関数の推定にTD誤差（temporal-difference誤差）を活用する点が大きく異なります．\n",
        "\n",
        "もう一点，前回扱ったREINFORCEとの違いがあります．REINFORCEでは，エピソード毎に方策勾配を計算し，方策や状態価値を更新していました．今回扱うActor-Critic法もそのように扱うことができますが，ここではこれに加えて，各エピソードの中の各ステップ（状態遷移）毎に方策を更新していく方向を見ていきます．\n",
        "このようなアプローチ（エピソード内に方策を学習していくアプローチ）をオンラインアプローチと言います．\n",
        "\n",
        "加えて，Actor-Critic法は，REINFORCE（モンテカルロ法）では必ずしも適切ではない非エピソディックタスク（連続タスク）に対しても適用可能です．\n",
        "これについても見ていきます．"
      ],
      "metadata": {
        "id": "H_gG0F5uKTYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 状態価値の再帰表現\n",
        "\n",
        "まず，状態価値のおさらいです．\n",
        "状態$s$の価値を，「$s_0 = s$からインタラクションを始めて，方策$\\pi$に従って行動選択した際に得られる割引累積報酬の期待値$\\mathrm{E}[G_0 \\mid s_0 = s]$」と定義します．\n",
        "これを$V^{\\pi}(s)$と書きます．\n",
        "定義からわかるように，状態価値は方策$\\pi$に依存しています．\n",
        "割引累積報酬が\n",
        "$$\n",
        "G_{t} = r_{t+1} + \\gamma G_{t+1}\n",
        "$$\n",
        "という再帰的な関係式を満たすことを考えると，状態価値は\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathrm{E}[r_{t+1} + \\gamma V^{\\pi}(s_{t+1}) \\mid s_t = s]\n",
        "$$\n",
        "という関係式を満たすことがわかります．"
      ],
      "metadata": {
        "id": "9PHLZ4LlvJNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD誤差を用いた価値関数の推定\n",
        "\n",
        "方策勾配を計算するには価値を推定することが必要になります．\n",
        "REINFORCEアルゴリズムでは，価値をモンテカルロ近似していました．\n",
        "すなわち，1エピソード分，現在の方策を用いて環境とインタラクションし，その結果から計算される累積報酬を用いて，価値を推定していたことになります．\n",
        "（補足：ステップtでの状態の価値を推定するために，ステップt+1以降に得られる報酬が必要になります．そのため，REINFORCEでは，エピソード毎にしか方策を更新できません．）\n",
        "\n",
        "TD誤差を用いた価値推定方法は，次のようなアイディアに基づいています．\n",
        "まず，状態価値の再帰式に着目しましょう．\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathrm{E}[r_{t+1} + \\gamma V^{\\pi}(s_{t+1}) \\mid s_t = s]\n",
        "$$\n",
        "価値関数の推定値 $v_{\\phi}(s)$ の目標値は$V^{\\pi}(s)$となります．\n",
        "すなわち，目標は$(V^{\\pi}(s) - v_{\\phi}(s))^2$を最小化することなどと解釈できます．\n",
        "しかし，$V^{\\pi}(s)$は未知なので，これを直接最適化することはできません．\n",
        "上の再帰式における右辺の$V^{\\pi}(s_{t+1})$も当然未知ですから，この右辺を直接使うこともできません．\n",
        "しかし，$V^{\\pi}(s_{t+1})$を現在の推定値$v_{\\phi}(s_{t+1})$で近似することを許せば，\n",
        "$$\n",
        "V^{\\pi}(s_t) \\approx r_{t+1} + \\gamma v_{\\phi}(s_{t+1}) =: y_t\n",
        "$$\n",
        "と近似することができます．\n",
        "そこで，上の近似式の右辺を$y_t$とおき，\n",
        "$(y_{t} - v_{\\phi}(s_t))^2$を最小化するように$\\phi$を学習する方針を考えます．なお，$y_t$も$\\phi$に依存していますが，こちらは定数と見なします．\n",
        "このように，目標値を計算する際に推定値自身を利用する方法をブートストラップといい，この目標値との差$y_{t} - v_{\\phi}(s_t)$をTD誤差と言います．\n",
        "Actor-Critic法では，TD誤差を用いて状態価値関数を推定していきます．\n",
        "\n",
        "価値推定の際，一点だけ注意が必要です．\n",
        "状態$s_{t+1}$が終端状態である場合，すなわち，`terminated`フラグがTrueになっている場合，その状態の価値は$0$と解釈する必要があります．しかし，推定している価値関数は，終端状態について正しく学習されていません．\n",
        "そこで，終端状態である場合には，\n",
        "$$\n",
        "V^{\\pi}(s_t) \\approx r_{t+1}\n",
        "$$\n",
        "とします．"
      ],
      "metadata": {
        "id": "4tek1u07PR5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 方策勾配のオンライン推定\n",
        "\n",
        "TD誤差を用いることで，各タイムステップで方策を更新することが可能になります．\n",
        "\n",
        "第２回に紹介した，ベースラインとして状態価値を採用したREINFORCEアルゴリズムでは，各ステップでの方策勾配を以下のように推定していました．\n",
        "$$\n",
        "\\left( G_{t} - v_{\\phi}(s_{t}) \\right) \\nabla_{\\theta} \\ln \\pi_\\theta(a_{t} \\mid s_{t})\n",
        "$$\n",
        "ここで，$G_t$は現状態より先のステップにおいて得られる報酬和ですから，ステップ$t$では計算できません．\n",
        "$G_t$は状態$s_t$で行動$a_t$をとったときの行動価値の推定値として採用されており，価値関数の推定値を用いれば$y_t = r_{t+1} + \\gamma v_{\\phi}(s_{t+1})$で推定することが可能です．\n",
        "すなわち，各ステップでの方策勾配を以下のように推定することが可能です．\n",
        "$$\n",
        "\\left( y_t - v_{\\phi}(s_{t}) \\right) \\nabla_{\\theta} \\ln \\pi_\\theta(a_{t} \\mid s_{t})\n",
        "$$\n"
      ],
      "metadata": {
        "id": "9dNYBinvUmRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## バッチ更新とオンライン更新\n",
        "TD誤差を用いる場合，現在よりも先の状態で得られる累積報酬を計算する必要が無いので，モンテカルロ法を用いる場合と異なり，各ステップで方策を更新していくことが可能です．\n",
        "当然，一エピソード毎にパラメータ更新することも可能です．この場合をバッチ更新と呼ぶことにします．\n",
        "\n",
        "バッチ更新の場合，Actor-Critic法とベースラインを推定するREINFORCEとでは，行動価値の推定方法のみが異なります．前者ではブートストラップを用いて行動価値を推定していますが，後者では累積報酬のモンテカルロ近似によって行動価値を推定しています．\n",
        "モンテカルロ近似の良い点は，不偏推定になることですが，一般に分散が大きくなります．\n",
        "他方，ブートストラップ推定する場合には，分散を抑えることができますが，一般に不偏となりません．\n"
      ],
      "metadata": {
        "id": "9W3U67JheX_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor-Criticの実装\n",
        "\n",
        "ここでは，まずバッチ学習（エピソード単位で学習）をするActor-Critic法を実装しています．ActorとCriticのアーキテクチャは第２回と同じものを採用しています．"
      ],
      "metadata": {
        "id": "IK-B4miwa4IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "# gpuが使用される場合の設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
      ],
      "metadata": {
        "id": "hDnqAPt1jSJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, dim_state, num_action, dim_hidden=128):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_state, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, num_action)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.softmax(self.fc2(x), dim=0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3d4NDkjPjSHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, dim_state, dim_hidden=128):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_state, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eQFnydEz8ovf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchActorCriticAgent:\n",
        "    def __init__(self, env, actor, critic, device, lr_a, lr_c):\n",
        "        self.device = device\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.env = env\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_a, betas=(0.9, 0.999))\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_c, betas=(0.9, 0.999))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        return self.select_action(observation)[0]\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # 行動選択\n",
        "        observation_ = Variable(torch.Tensor(observation)).to(self.device)\n",
        "        action_probs = self.actor(observation_)\n",
        "        log_probs = action_probs.log()\n",
        "        action = Categorical(action_probs).sample()\n",
        "        return action.data.cpu().numpy(), log_probs[action]\n",
        "\n",
        "    def rollout_with_update(self):\n",
        "        # 1 エピソード実行\n",
        "        observation, info = self.env.reset()\n",
        "        steps = 0\n",
        "        l_observation = []\n",
        "        l_next_observation = []\n",
        "        l_reward = []\n",
        "        l_terminated = []\n",
        "        l_log_prob = []\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        # エピソード\n",
        "        while not (terminated or truncated):\n",
        "            action, log_prob = self.select_action(observation)\n",
        "            next_observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "            l_observation.append(observation)\n",
        "            l_next_observation.append(next_observation)\n",
        "            l_reward.append(reward)\n",
        "            l_terminated.append(terminated)\n",
        "            l_log_prob.append(log_prob)\n",
        "            observation = next_observation\n",
        "            steps +=1\n",
        "        # 更新\n",
        "        self.update(l_observation, l_next_observation, l_reward, l_terminated, l_log_prob)\n",
        "        return l_reward\n",
        "\n",
        "    def update(self, observation, next_observation, reward, terminated, log_prob):\n",
        "        obs_tensor = torch.FloatTensor(np.array(observation)).to(self.device)\n",
        "        next_obs_tensor = torch.FloatTensor(np.array(next_observation)).to(self.device)\n",
        "        reward_tensor = torch.FloatTensor(np.array(reward)).reshape((-1, 1)).to(self.device)\n",
        "        flg_tensor = torch.FloatTensor(np.array(terminated)).reshape((-1, 1)).to(self.device)\n",
        "\n",
        "        vtt = (reward_tensor + (1 - flg_tensor) * self.critic(next_obs_tensor)).detach()\n",
        "        vt = self.critic(obs_tensor)\n",
        "\n",
        "        # Actor の更新\n",
        "        loss_a = - sum([delta * lp for delta, lp in zip(vtt - vt.detach(), log_prob)]) / len(reward)\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        loss_a.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Critic の更新\n",
        "        loss_c = torch.sum((vtt - vt)**2) / len(reward)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        loss_c.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        return loss_a, loss_c"
      ],
      "metadata": {
        "id": "AZzL_KsP9l-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "env = gym.make(envname)\n",
        "\n",
        "actor = Actor(dim_state = 8, num_action = 4).to(device)\n",
        "critic = Critic(dim_state = 8).to(device)\n",
        "agent = BatchActorCriticAgent(env, actor, critic, device, lr_a=2e-4, lr_c=2e-3)"
      ],
      "metadata": {
        "id": "4cJI3P6pAT9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interval = 100\n",
        "returns = np.zeros((100, interval))\n",
        "\n",
        "for i in range(returns.shape[0]):\n",
        "    for j in range(returns.shape[1]):\n",
        "        rewards = agent.rollout_with_update()\n",
        "        returns[i, j] = np.sum(rewards)\n",
        "    print(interval * (i+1), np.mean(returns[i]), np.std(returns[i]))"
      ],
      "metadata": {
        "id": "lHdMuxUQAife"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = np.arange(1, 1+returns.size, returns.shape[1])\n",
        "avg = np.mean(returns, axis=1)\n",
        "std = np.std(returns, axis=1)\n",
        "plt.errorbar(episodes, avg, std, linestyle=':', marker='^')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "SOXTLc4oxCXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "経験分布関数についても確認しておきましょう．"
      ],
      "metadata": {
        "id": "pPZYoMrmdNjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=agent, render=False)\n",
        "    return_array[i] = cumulative_reward(history)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "iVWlj9sJSXlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習結果の確認は以下のコードで行います．"
      ],
      "metadata": {
        "id": "O5QmAygcdr9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=agent, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "TW0fJ1TswjLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 連続タスクへの適用\n",
        "\n",
        "REINFORCEを用いた場合，エピソディックタスクであることを仮定する必要がありました．\n",
        "これは，価値をモンテカルロ推定しているため，エピソードが定義されていない場合これをうまく推定することができないことに起因しています．\n",
        "TD誤差を用いる場合にはこの限りではありません．次状態の推定価値が得られれば，現状態についての行動価値が推定でき，方策勾配を計算できるためです．"
      ],
      "metadata": {
        "id": "YN6n3Z4TrTAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 割引累積報酬\n",
        "\n",
        "これまでの議論ではエピソディックタスクを仮定していたため，エピソードが必ず有限のステップ$T$で終了し，累積報酬\n",
        "$$\n",
        "G_1 = \\sum_{t=1}^{T} r_{t}\n",
        "$$\n",
        "が有限の値を取ることが仮定されてきました．そのため，減衰率$\\gamma = 1$としてきました．\n",
        "連続タスクを考える場合，即時報酬が有限の値であっても，累積報酬が発散してしまう可能性があります．\n",
        "そこで，これに対する一つのアプローチとして，減衰率を$\\gamma < 1$とした割引累積報酬\n",
        "$$\n",
        "G_1 = \\sum_{t=1}^{T} \\gamma^{t-1} r_{t}\n",
        "$$\n",
        "を考えることにします．こうすれば，即時報酬が有限である限り，割引累積報酬も有限となります．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zGPEQ0NJzTUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OnlineActorCriticAgent:\n",
        "    def __init__(self, env, actor, critic, device, lr_a, lr_c):\n",
        "        self.device = device\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.env = env\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_a, betas=(0.9, 0.999))\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_c, betas=(0.9, 0.999))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        return self.select_action(observation)[0]\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # 行動選択\n",
        "        observation_ = Variable(torch.Tensor(observation)).to(self.device)\n",
        "        action_probs = self.actor(observation_)\n",
        "        log_probs = action_probs.log()\n",
        "        action = Categorical(action_probs).sample()\n",
        "        return action.data.cpu().numpy(), log_probs[action]\n",
        "\n",
        "    def rollout_with_update(self):\n",
        "        # 1 エピソード実行\n",
        "        observation, info = self.env.reset()\n",
        "        steps = 0\n",
        "        rewards = []\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        while not (terminated or truncated):\n",
        "            action, log_prob = self.select_action(observation)\n",
        "            next_observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "            loss_a, loss_c = self.update(observation, next_observation, reward, terminated, log_prob)\n",
        "            rewards.append(reward)\n",
        "            observation = next_observation\n",
        "            steps +=1\n",
        "        return rewards\n",
        "\n",
        "    def update(self, observation, next_observation, reward, terminated, log_prob):\n",
        "        if terminated:\n",
        "            vtt = reward\n",
        "        else:\n",
        "            vtt = reward + self.critic(torch.Tensor(next_observation).to(self.device)).detach()\n",
        "        vt = self.critic(torch.Tensor(observation).to(self.device))\n",
        "\n",
        "        # Actor の更新\n",
        "        loss_a = - sum((vtt - vt.detach()) * log_prob)\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        loss_a.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Critic の更新\n",
        "        loss_c = sum((vtt - vt)**2)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        loss_c.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        return loss_a, loss_c"
      ],
      "metadata": {
        "id": "GeCzR9UQjR0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "env = gym.make(envname)\n",
        "\n",
        "actor = Actor(dim_state = 8, num_action = 4).to(device)\n",
        "critic = Critic(dim_state = 8).to(device)\n",
        "agent = OnlineActorCriticAgent(env, actor, critic, device, lr_a=2e-6, lr_c=2e-5)"
      ],
      "metadata": {
        "id": "jtebYrBrgVMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interval = 100\n",
        "returns = np.zeros((100, interval))\n",
        "\n",
        "for i in range(returns.shape[0]):\n",
        "    for j in range(returns.shape[1]):\n",
        "        rewards = agent.rollout_with_update()\n",
        "        returns[i, j] = np.sum(rewards)\n",
        "    print(interval * (i+1), np.mean(returns[i]), np.std(returns[i]))"
      ],
      "metadata": {
        "id": "jBPmSkFXgVIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "実際に実行してみるとすぐに気が付きますが，オンライン学習する場合，実行時間が長くかかります．\n",
        "これは，ステップ毎にパラメータ更新を計算することが必要となるため，オーバーヘッドが多くかかるからです．\n",
        "また，各ステップ方策勾配を計算する場合，計算される方策勾配の分散が大きくなるため，バッチ更新時に用いた学習率よりも小さめの学習率が必要になります．"
      ],
      "metadata": {
        "id": "pnO-Lv-9qXoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自習課題\n",
        "\n",
        "* 方策を変えてみましょう．特に，中間層のノード数を変更した場合に，学習効率がどの程度変わるのか，グラフを作成するなどして確認しましょう．\n",
        "\n",
        "* 学習率を調整してみましょう．特に，ベースラインを導入したREINFORCEでは，Actorの学習率とCriticの学習率について，効率的なパラメータの関係を確認してみましょう．\n",
        "\n",
        "* タスクを変えてみましょう．タスクが異なれば，適切な方策（ノード数など）や適切な学習率も変化する可能性があります．これを確認してみましょう．"
      ],
      "metadata": {
        "id": "uwEbG_9Bd26g"
      }
    }
  ]
}