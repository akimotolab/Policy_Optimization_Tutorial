{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akimotolab/Policy_Optimization_Tutorial/blob/main/3_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備\n",
        "\n",
        "まず，必要なパッケージのインストールとインポート，および仮想displayを設定します．"
      ],
      "metadata": {
        "id": "qWe6P9xqH8be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kXoBp0fgHyhe",
        "outputId": "b94f828f-dca0-432a-d4db-38355ae5d4fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,242 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:12 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,244 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,521 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,016 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,498 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,284 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,152 kB]\n",
            "Fetched 10.2 MB in 7s (1,565 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "32 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1.post0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 7,814 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n",
            "Fetched 7,814 kB in 3s (2,690 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120880 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1.post0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373071 sha256=f9a8f5a43624194d32cdc43d63bfa704cc55aca7572943ca31aa6dac31bec140\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: farama-notifications, box2d-py, gymnasium\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "# 必要なパッケージのインストール\n",
        "!apt update\n",
        "!pip install swig\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import torch\n",
        "\n",
        "# 仮想ディスプレイの設定\n",
        "_display = Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "metadata": {
        "id": "iJ1PYM90IHa9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "d5Oklvxh5nAC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "続いて，第１回の資料で定義した`rollout`などの基本的な関数をここでも定義しておきます．"
      ],
      "metadata": {
        "id": "ci2EEsRpD1oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout(envname, policy=None, render=False, seed=None):\n",
        "    if render:\n",
        "        env = gym.make(envname, render_mode=\"rgb_array\")\n",
        "    else:\n",
        "        env = gym.make(envname)\n",
        "    history = []\n",
        "    img = []\n",
        "\n",
        "    # 乱数の設定\n",
        "    if seed is not None:\n",
        "        random.seed(int(seed))\n",
        "    envseed = random.randint(0, 1000)\n",
        "    actseed = random.randint(0, 1000)\n",
        "    observation, info = env.reset(seed=envseed)\n",
        "    env.action_space.seed(actseed)\n",
        "\n",
        "    # 可視化用の設定\n",
        "    if render:\n",
        "        d = Display()\n",
        "        d.start()\n",
        "        img.append(env.render())\n",
        "\n",
        "    # メインループ（環境とのインタラクション）\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # 行動を選択\n",
        "        if policy is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy(observation)\n",
        "\n",
        "        # 行動を実行\n",
        "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "        history.append([observation, action, next_observation, reward, terminated, truncated, info])\n",
        "        observation = next_observation\n",
        "        if render:\n",
        "            display.clear_output(wait=True)\n",
        "            img.append(env.render())\n",
        "    env.close()\n",
        "    return history, img\n",
        "\n",
        "\n",
        "def visualize(img):\n",
        "    dpi = 72\n",
        "    interval = 50\n",
        "    plt.figure(figsize=(img[0].shape[1]/dpi, img[0].shape[0]/dpi), dpi=dpi)\n",
        "    patch = plt.imshow(img[0])\n",
        "    plt.axis=('off')\n",
        "    animate = lambda i: patch.set_data(img[i])\n",
        "    ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(img), interval=interval)\n",
        "    display.display(display.HTML(ani.to_jshtml()))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def cumulative_reward(history):\n",
        "    return sum(hist[3] for hist in history)"
      ],
      "metadata": {
        "id": "9ELUIKjb5vY2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actor-Critic法による方策最適化（online & on-policy）\n",
        "\n",
        "今回は「Actor-Critic法」を見ていきます．\n",
        "第２回は，価値関数をモンテカルロ近似する方策勾配法であるREINFORCEアルゴリズムを紹介しました．\n",
        "今回紹介するActor-Critic法も方策勾配法の一種ですが，価値関数の推定にTD誤差（temporal-difference誤差）を活用する点が大きく異なります．\n",
        "\n",
        "もう一点，前回扱ったREINFORCEとの違いがあります．REINFORCEでは，エピソード毎に方策勾配を計算し，方策や状態価値を更新していました．今回扱うActor-Critic法もそのように扱うことができますが，ここではこれに加えて，各エピソードの中の各ステップ（状態遷移）毎に方策を更新していく方向を見ていきます．\n",
        "このようなアプローチ（エピソード内に方策を学習していくアプローチ）をオンラインアプローチと言います．\n",
        "\n",
        "加えて，Actor-Critic法は，REINFORCE（モンテカルロ法）では必ずしも適切ではない非エピソディックタスク（連続タスク）に対しても適用可能です．\n",
        "これについても見ていきます．"
      ],
      "metadata": {
        "id": "H_gG0F5uKTYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 状態価値の再帰表現\n",
        "\n",
        "まず，状態価値のおさらいです．\n",
        "状態$s$の価値を，「$s_0 = s$からインタラクションを始めて，方策$\\pi$に従って行動選択した際に得られる割引累積報酬の期待値$\\mathrm{E}[G_0 \\mid s_0 = s]$」と定義します．\n",
        "これを$V^{\\pi}(s)$と書きます．\n",
        "定義からわかるように，状態価値は方策$\\pi$に依存しています．\n",
        "割引累積報酬が\n",
        "$$\n",
        "G_{t} = r_{t+1} + \\gamma G_{t+1}\n",
        "$$\n",
        "という再帰的な関係式を満たすことを考えると，状態価値は\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathrm{E}[r_{t+1} + \\gamma V^{\\pi}(s_{t+1}) \\mid s_t = s]\n",
        "$$\n",
        "という関係式を満たすことがわかります．"
      ],
      "metadata": {
        "id": "9PHLZ4LlvJNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD誤差を用いた価値関数の推定\n",
        "\n",
        "方策勾配を計算するには価値を推定することが必要になります．\n",
        "REINFORCEアルゴリズムでは，価値をモンテカルロ近似していました．\n",
        "すなわち，1エピソード分，現在の方策を用いて環境とインタラクションし，その結果から計算される累積報酬を用いて，価値を推定していたことになります．\n",
        "（補足：ステップtでの状態の価値を推定するために，ステップt+1以降に得られる報酬が必要になります．そのため，REINFORCEでは，エピソード毎にしか方策を更新できません．）\n",
        "\n",
        "TD誤差を用いた価値推定方法は，次のようなアイディアに基づいています．\n",
        "まず，状態価値の再帰式に着目しましょう．\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathrm{E}[r_{t+1} + \\gamma V^{\\pi}(s_{t+1}) \\mid s_t = s]\n",
        "$$\n",
        "価値関数の推定値 $v_{\\phi}(s)$ の目標値は$V^{\\pi}(s)$となります．\n",
        "すなわち，目標は$(V^{\\pi}(s) - v_{\\phi}(s))^2$を最小化することなどと解釈できます．\n",
        "しかし，$V^{\\pi}(s)$は未知なので，これを直接最適化することはできません．\n",
        "上の再帰式における右辺の$V^{\\pi}(s_{t+1})$も当然未知ですから，この右辺を直接使うこともできません．\n",
        "しかし，$V^{\\pi}(s_{t+1})$を現在の推定値$v_{\\phi}(s_{t+1})$で近似することを許せば，\n",
        "$$\n",
        "V^{\\pi}(s_t) \\approx r_{t+1} + \\gamma v_{\\phi}(s_{t+1}) =: y_t\n",
        "$$\n",
        "と近似することができます．\n",
        "そこで，上の近似式の右辺を$y_t$とおき，\n",
        "$(y_{t} - v_{\\phi}(s_t))^2$を最小化するように$\\phi$を学習する方針を考えます．なお，$y_t$も$\\phi$に依存していますが，こちらは定数と見なします．\n",
        "このように，目標値を計算する際に推定値自身を利用する方法をブートストラップといい，この目標値との差$y_{t} - v_{\\phi}(s_t)$をTD誤差と言います．\n",
        "Actor-Critic法では，TD誤差を用いて状態価値関数を推定していきます．\n",
        "\n",
        "価値推定の際，一点だけ注意が必要です．\n",
        "状態$s_{t+1}$が終端状態である場合，すなわち，`terminated`フラグがTrueになっている場合，その状態の価値は$0$と解釈する必要があります．しかし，推定している価値関数は，終端状態について正しく学習されていません．\n",
        "そこで，終端状態である場合には，\n",
        "$$\n",
        "V^{\\pi}(s_t) \\approx r_{t+1}\n",
        "$$\n",
        "とします．"
      ],
      "metadata": {
        "id": "4tek1u07PR5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 方策勾配のオンライン推定\n",
        "\n",
        "TD誤差を用いることで，各タイムステップで方策を更新することが可能になります．\n",
        "\n",
        "第２回に紹介した，ベースラインとして状態価値を採用したREINFORCEアルゴリズムでは，各ステップでの方策勾配を以下のように推定していました．\n",
        "$$\n",
        "\\left( G_{t} - v_{\\phi}(s_{t}) \\right) \\nabla_{\\theta} \\ln \\pi_\\theta(a_{t} \\mid s_{t})\n",
        "$$\n",
        "ここで，$G_t$は現状態より先のステップにおいて得られる報酬和ですから，ステップ$t$では計算できません．\n",
        "$G_t$は状態$s_t$で行動$a_t$をとったときの行動価値の推定値として採用されており，価値関数の推定値を用いれば$y_t = r_{t+1} + \\gamma v_{\\phi}(s_{t+1})$で推定することが可能です．\n",
        "すなわち，各ステップでの方策勾配を以下のように推定することが可能です．\n",
        "$$\n",
        "\\left( y_t - v_{\\phi}(s_{t}) \\right) \\nabla_{\\theta} \\ln \\pi_\\theta(a_{t} \\mid s_{t})\n",
        "$$\n"
      ],
      "metadata": {
        "id": "9dNYBinvUmRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## バッチ更新とオンライン更新\n",
        "TD誤差を用いる場合，現在よりも先の状態で得られる累積報酬を計算する必要が無いので，モンテカルロ法を用いる場合と異なり，各ステップで方策を更新していくことが可能です．\n",
        "当然，一エピソード毎にパラメータ更新することも可能です．この場合をバッチ更新と呼ぶことにします．\n",
        "\n",
        "バッチ更新の場合，Actor-Critic法とベースラインを推定するREINFORCEとでは，行動価値の推定方法のみが異なります．前者ではブートストラップを用いて行動価値を推定していますが，後者では累積報酬のモンテカルロ近似によって行動価値を推定しています．\n",
        "モンテカルロ近似の良い点は，不偏推定になることですが，一般に分散が大きくなります．\n",
        "他方，ブートストラップ推定する場合には，分散を抑えることができますが，一般に不偏となりません．\n"
      ],
      "metadata": {
        "id": "9W3U67JheX_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor-Criticの実装\n",
        "\n",
        "ここでは，まずバッチ学習（エピソード単位で学習）をするActor-Critic法を実装しています．ActorとCriticのアーキテクチャは第２回と同じものを採用しています．"
      ],
      "metadata": {
        "id": "IK-B4miwa4IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "# gpuが使用される場合の設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
      ],
      "metadata": {
        "id": "hDnqAPt1jSJh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, dim_state, num_action, dim_hidden=128):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_state, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, num_action)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.softmax(self.fc2(x), dim=0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3d4NDkjPjSHG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, dim_state, dim_hidden=128):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_state, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eQFnydEz8ovf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchActorCriticAgent:\n",
        "    def __init__(self, env, actor, critic, device, lr_a, lr_c):\n",
        "        self.device = device\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.env = env\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_a, betas=(0.9, 0.999))\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_c, betas=(0.9, 0.999))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        return self.select_action(observation)[0]\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # 行動選択\n",
        "        observation_ = Variable(torch.Tensor(observation)).to(self.device)\n",
        "        action_probs = self.actor(observation_)\n",
        "        log_probs = action_probs.log()\n",
        "        action = Categorical(action_probs).sample()\n",
        "        return action.data.cpu().numpy(), log_probs[action]\n",
        "\n",
        "    def rollout_with_update(self):\n",
        "        # 1 エピソード実行\n",
        "        observation, info = self.env.reset()\n",
        "        steps = 0\n",
        "        l_observation = []\n",
        "        l_next_observation = []\n",
        "        l_reward = []\n",
        "        l_terminated = []\n",
        "        l_log_prob = []\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        # エピソード\n",
        "        while not (terminated or truncated):\n",
        "            action, log_prob = self.select_action(observation)\n",
        "            next_observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "            l_observation.append(observation)\n",
        "            l_next_observation.append(next_observation)\n",
        "            l_reward.append(reward)\n",
        "            l_terminated.append(terminated)\n",
        "            l_log_prob.append(log_prob)\n",
        "            observation = next_observation\n",
        "            steps +=1\n",
        "        # 更新\n",
        "        self.update(l_observation, l_next_observation, l_reward, l_terminated, l_log_prob)\n",
        "        return l_reward\n",
        "\n",
        "    def update(self, observation, next_observation, reward, terminated, log_prob):\n",
        "        obs_tensor = torch.FloatTensor(np.array(observation)).to(self.device)\n",
        "        next_obs_tensor = torch.FloatTensor(np.array(next_observation)).to(self.device)\n",
        "        reward_tensor = torch.FloatTensor(np.array(reward)).reshape((-1, 1)).to(self.device)\n",
        "        flg_tensor = torch.FloatTensor(np.array(terminated)).reshape((-1, 1)).to(self.device)\n",
        "\n",
        "        vtt = (reward_tensor + (1 - flg_tensor) * self.critic(next_obs_tensor)).detach()\n",
        "        vt = self.critic(obs_tensor)\n",
        "\n",
        "        # Actor の更新\n",
        "        loss_a = - sum([delta * lp for delta, lp in zip(vtt - vt.detach(), log_prob)]) / len(reward)\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        loss_a.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Critic の更新\n",
        "        loss_c = torch.sum((vtt - vt)**2) / len(reward)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        loss_c.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        return loss_a, loss_c"
      ],
      "metadata": {
        "id": "AZzL_KsP9l-3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "env = gym.make(envname)\n",
        "\n",
        "actor = Actor(dim_state = 8, num_action = 4).to(device)\n",
        "critic = Critic(dim_state = 8).to(device)\n",
        "agent = BatchActorCriticAgent(env, actor, critic, device, lr_a=2e-4, lr_c=2e-3)"
      ],
      "metadata": {
        "id": "4cJI3P6pAT9n"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interval = 100\n",
        "returns = np.zeros((100, interval))\n",
        "\n",
        "for i in range(returns.shape[0]):\n",
        "    for j in range(returns.shape[1]):\n",
        "        rewards = agent.rollout_with_update()\n",
        "        returns[i, j] = np.sum(rewards)\n",
        "    print(interval * (i+1), np.mean(returns[i]), np.std(returns[i]))"
      ],
      "metadata": {
        "id": "lHdMuxUQAife",
        "outputId": "46a260b0-b511-43f0-ec88-ea0355b6a707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 -206.72226816409108 127.67933144686761\n",
            "200 -205.96241824425258 124.87223609715862\n",
            "300 -213.06150953564105 127.43467492024337\n",
            "400 -218.41733966373718 117.50842423462697\n",
            "500 -268.98001157012516 162.43335674038389\n",
            "600 -320.7105501212443 163.46326389224447\n",
            "700 -286.85237903762896 150.47643598153584\n",
            "800 -259.00883065239907 152.3478124305795\n",
            "900 -227.72801756605054 128.01051892429743\n",
            "1000 -192.24724545251857 113.49403416483617\n",
            "1100 -168.86957584447322 107.51260418750971\n",
            "1200 -150.33448305103806 106.72933037542533\n",
            "1300 -148.12955373497206 110.32235219734953\n",
            "1400 -146.05963672688017 95.80358465478132\n",
            "1500 -136.40848976902706 86.79811229964129\n",
            "1600 -138.32834751306055 95.6412481034117\n",
            "1700 -141.02363146923253 92.62871410511903\n",
            "1800 -102.23493721118803 78.28872389058175\n",
            "1900 -105.84855892760822 91.13380988752857\n",
            "2000 -104.47276601834699 84.6075949579003\n",
            "2100 -98.38869862304811 89.76247670721541\n",
            "2200 -99.87370111624979 96.96749185060983\n",
            "2300 -94.46933205958175 89.70095374969394\n",
            "2400 -81.22244047820581 71.61414109740535\n",
            "2500 -72.94464880830273 84.39989130670139\n",
            "2600 -60.18608579607724 82.97377714559696\n",
            "2700 -46.75295286846521 84.62909985994045\n",
            "2800 -52.08457972712689 79.90702632619492\n",
            "2900 -50.22799775834285 76.87606454709137\n",
            "3000 -44.39666461624913 73.74864004772016\n",
            "3100 -31.341288131181894 71.37517486957448\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-0f2df1eb685d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_with_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-e5f33290c32d>\u001b[0m in \u001b[0;36mrollout_with_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# 更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_next_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_terminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_log_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ml_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-e5f33290c32d>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, observation, next_observation, reward, terminated, log_prob)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Actor の更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mloss_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-e5f33290c32d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Actor の更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mloss_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = np.arange(1, 1+returns.size, returns.shape[1])\n",
        "avg = np.mean(returns, axis=1)\n",
        "std = np.std(returns, axis=1)\n",
        "plt.errorbar(episodes, avg, std, linestyle=':', marker='^')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "SOXTLc4oxCXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "経験分布関数についても確認しておきましょう．"
      ],
      "metadata": {
        "id": "pPZYoMrmdNjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "return_array = np.zeros(50)\n",
        "for i in range(len(return_array)):\n",
        "    history, img = rollout(envname, policy=agent, render=False)\n",
        "    return_array[i] = cumulative_reward(history)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.ecdfplot(data=-return_array, ax=ax)\n",
        "ax.set_xlim(-400, 400)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "iVWlj9sJSXlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習結果の確認は以下のコードで行います．"
      ],
      "metadata": {
        "id": "O5QmAygcdr9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history, img = rollout(envname, policy=agent, render=True)\n",
        "print(cumulative_reward(history))\n",
        "visualize(img)"
      ],
      "metadata": {
        "id": "TW0fJ1TswjLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 連続タスクへの適用\n",
        "\n",
        "REINFORCEを用いた場合，エピソディックタスクであることを仮定する必要がありました．\n",
        "これは，価値をモンテカルロ推定しているため，エピソードが定義されていない場合これをうまく推定することができないことに起因しています．\n",
        "TD誤差を用いる場合にはこの限りではありません．次状態の推定価値が得られれば，現状態についての行動価値が推定でき，方策勾配を計算できるためです．"
      ],
      "metadata": {
        "id": "YN6n3Z4TrTAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 割引累積報酬\n",
        "\n",
        "これまでの議論ではエピソディックタスクを仮定していたため，エピソードが必ず有限のステップ$T$で終了し，累積報酬\n",
        "$$\n",
        "G_1 = \\sum_{t=1}^{T} r_{t}\n",
        "$$\n",
        "が有限の値を取ることが仮定されてきました．そのため，減衰率$\\gamma = 1$としてきました．\n",
        "連続タスクを考える場合，即時報酬が有限の値であっても，累積報酬が発散してしまう可能性があります．\n",
        "そこで，これに対する一つのアプローチとして，減衰率を$\\gamma < 1$とした割引累積報酬\n",
        "$$\n",
        "G_1 = \\sum_{t=1}^{T} \\gamma^{t-1} r_{t}\n",
        "$$\n",
        "を考えることにします．こうすれば，即時報酬が有限である限り，割引累積報酬も有限となります．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zGPEQ0NJzTUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OnlineActorCriticAgent:\n",
        "    def __init__(self, env, actor, critic, device, lr_a, lr_c):\n",
        "        self.device = device\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.env = env\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_a, betas=(0.9, 0.999))\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_c, betas=(0.9, 0.999))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        return self.select_action(observation)[0]\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # 行動選択\n",
        "        observation_ = Variable(torch.Tensor(observation)).to(self.device)\n",
        "        action_probs = self.actor(observation_)\n",
        "        log_probs = action_probs.log()\n",
        "        action = Categorical(action_probs).sample()\n",
        "        return action.data.cpu().numpy(), log_probs[action]\n",
        "\n",
        "    def rollout_with_update(self):\n",
        "        # 1 エピソード実行\n",
        "        observation, info = self.env.reset()\n",
        "        steps = 0\n",
        "        rewards = []\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        while not (terminated or truncated):\n",
        "            action, log_prob = self.select_action(observation)\n",
        "            next_observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "            loss_a, loss_c = self.update(observation, next_observation, reward, terminated, log_prob)\n",
        "            rewards.append(reward)\n",
        "            observation = next_observation\n",
        "            steps +=1\n",
        "        return rewards\n",
        "\n",
        "    def update(self, observation, next_observation, reward, terminated, log_prob):\n",
        "        if terminated:\n",
        "            vtt = reward\n",
        "        else:\n",
        "            vtt = reward + self.critic(torch.Tensor(next_observation).to(self.device)).detach()\n",
        "        vt = self.critic(torch.Tensor(observation).to(self.device))\n",
        "\n",
        "        # Actor の更新\n",
        "        loss_a = - sum((vtt - vt.detach()) * log_prob)\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        loss_a.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Critic の更新\n",
        "        loss_c = sum((vtt - vt)**2)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        loss_c.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        return loss_a, loss_c"
      ],
      "metadata": {
        "id": "GeCzR9UQjR0b"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envname = \"LunarLander-v2\"\n",
        "dim_state = 8\n",
        "num_action = 4\n",
        "env = gym.make(envname)\n",
        "\n",
        "actor = Actor(dim_state = 8, num_action = 4).to(device)\n",
        "critic = Critic(dim_state = 8).to(device)\n",
        "agent = OnlineActorCriticAgent(env, actor, critic, device, lr_a=2e-6, lr_c=2e-5)"
      ],
      "metadata": {
        "id": "jtebYrBrgVMr"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interval = 100\n",
        "returns = np.zeros((100, interval))\n",
        "\n",
        "for i in range(returns.shape[0]):\n",
        "    for j in range(returns.shape[1]):\n",
        "        rewards = agent.rollout_with_update()\n",
        "        returns[i, j] = np.sum(rewards)\n",
        "    print(interval * (i+1), np.mean(returns[i]), np.std(returns[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "jBPmSkFXgVIy",
        "outputId": "2198ef4e-7c64-4a91-a6b2-b369900317dc"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 -181.99175727453715 107.48923339347144\n",
            "200 -207.41193447615248 131.59417374315046\n",
            "300 -199.43683054483935 116.43834616185057\n",
            "400 -184.1240167311555 107.16973285918472\n",
            "500 -181.53275873818248 115.64225152678098\n",
            "600 -197.6734166747809 118.16222983846815\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-0f2df1eb685d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_with_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-9fd0561e54ad>\u001b[0m in \u001b[0;36mrollout_with_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-9fd0561e54ad>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, observation, next_observation, reward, terminated, log_prob)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mloss_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "実際に実行してみるとすぐに気が付きますが，オンライン学習する場合，実行時間が長くかかります．\n",
        "これは，ステップ毎にパラメータ更新を計算することが必要となるため，オーバーヘッドが多くかかるからです．\n",
        "また，各ステップ方策勾配を計算する場合，計算される方策勾配の分散が大きくなるため，バッチ更新時に用いた学習率よりも小さめの学習率が必要になります．"
      ],
      "metadata": {
        "id": "pnO-Lv-9qXoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自習課題\n",
        "\n",
        "* 方策を変えてみましょう．特に，中間層のノード数を変更した場合に，学習効率がどの程度変わるのか，グラフを作成するなどして確認しましょう．\n",
        "\n",
        "* 学習率を調整してみましょう．特に，ベースラインを導入したREINFORCEでは，Actorの学習率とCriticの学習率について，効率的なパラメータの関係を確認してみましょう．\n",
        "\n",
        "* タスクを変えてみましょう．タスクが異なれば，適切な方策（ノード数など）や適切な学習率も変化する可能性があります．これを確認してみましょう．"
      ],
      "metadata": {
        "id": "uwEbG_9Bd26g"
      }
    }
  ]
}